{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH2HZGeRgd38"
      },
      "source": [
        "# Generalised Linear Models\n",
        "\n",
        "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
        "\n",
        "Cambridge\n",
        "\n",
        "### 2025-09-10"
      ],
      "id": "NH2HZGeRgd38"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCIvwp_vgd4B"
      },
      "source": [],
      "id": "PCIvwp_vgd4B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThcFL4VWgd4B"
      },
      "source": [
        "$$\n",
        "$$"
      ],
      "id": "ThcFL4VWgd4B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cZc-OnVgd4C"
      },
      "source": [
        "<!-- Do not edit this file locally. -->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!---->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
        "<!--\n",
        "\n",
        "-->"
      ],
      "id": "5cZc-OnVgd4C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toTG28uJgd4D"
      },
      "source": [
        "## ML Foundations Course Notebook Setup\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_mlfc/includes/mlfc-notebook-setup.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_mlfc/includes/mlfc-notebook-setup.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We install some bespoke codes for creating and saving plots as well as\n",
        "loading data sets."
      ],
      "id": "toTG28uJgd4D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1JUYa7Egd4D"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install notutils\n",
        "%pip install pods\n",
        "%pip install mlai"
      ],
      "id": "u1JUYa7Egd4D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCPxmkSAgd4F"
      },
      "outputs": [],
      "source": [
        "import notutils\n",
        "import pods\n",
        "import mlai\n",
        "import mlai.plot as plot"
      ],
      "id": "wCPxmkSAgd4F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYzCV7H3gd4F"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 22})"
      ],
      "id": "AYzCV7H3gd4F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE97vyK-gd4H"
      },
      "source": [
        "<!--setupplotcode{import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_context('paper')\n",
        "sns.set_palette('colorblind')}-->"
      ],
      "id": "qE97vyK-gd4H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxOQClQgd4H"
      },
      "source": [
        "## Review\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/generalised-linear-models.gpp.markdown\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/generalised-linear-models.gpp.markdown', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We introduced machine learning as a way to extract knowledge from data\n",
        "to make predictions through a prediction function and an objective\n",
        "function. We looked at a simple example of predicting whether someone\n",
        "would buy a jumper based on their age and latitude, *using logistic\n",
        "regression* to model the log-odds of purchase. This highlighted how\n",
        "machine learning can codify predictions through mathematical functions.\n",
        "This is an example of a broader approach known as *generalised linear\n",
        "models*.\n",
        "\n",
        "When taking a probabilistic approach to supervised learning we’re\n",
        "interested in predicting a class label, $y_i$, given an input,\n",
        "$\\mathbf{ x}_i$. That’s represented probabilisticially as\n",
        "$p(y_i|\\mathbf{ x}_i)$. We can derive this conditional distribution\n",
        "through either (1) modelling the joint distribution,\n",
        "$p(\\mathbf{ y}, \\mathbf{X})$ and then dividing by the marginal\n",
        "distribution of the inputs, $p(\\mathbf{X})$ , or (2) focusing\n",
        "specifically on modeling the conditional density,\n",
        "$p(\\mathbf{ y}|\\mathbf{X})$, that directly answers our prediction\n",
        "question. In the *generalised linear model* we choose the second\n",
        "approach.\n",
        "\n",
        "As we move to generalised linear models like logistic regression, we’ll\n",
        "see how directly modeling the conditional density\n",
        "$p(\\mathbf{ y}|\\mathbf{X})$ can provide more flexibility in our modeling\n",
        "assumptions, while still allowing us to make the specific predictions we\n",
        "need."
      ],
      "id": "rfxOQClQgd4H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbsm434pgd4I"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression-intro.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression-intro.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "A logistic regression is an approach to classification which extends the\n",
        "linear regression models we’ve already explored. Rather than modeling\n",
        "the output of the function directly the assumption is that we model the\n",
        "*log-odds* with the basis functions.\n",
        "\n",
        "The [odds](http://en.wikipedia.org/wiki/Odds) are defined as the ratio\n",
        "of the probability of a positive outcome, to the probability of a\n",
        "negative outcome. Just as we saw in our jumper (sweater) example where\n",
        "$$\n",
        "\\log \\frac{p(\\text{bought})}{p(\\text{not bought})} = w_0 + w_1 \\text{age} + w_2 \\text{latitude}\n",
        "$$ If the probability of a positive outcome is denoted by $\\pi$, then\n",
        "the odds are computed as $\\frac{\\pi}{1-\\pi}$.\n",
        "\n",
        "Odds are widely used by\n",
        "[bookmakers](http://en.wikipedia.org/wiki/Bookmaker) in gambling,\n",
        "although a bookmakers odds won’t normalise: i.e. if you look at the\n",
        "equivalent probabilities, and sum over the probability of all outcomes\n",
        "the bookmakers are considering, then you won’t get one. This is how the\n",
        "bookmaker makes a profit. Because a probability is always between zero\n",
        "and one, the odds are always between $0$ and $\\infty$. If the positive\n",
        "outcome is unlikely the odds are close to zero, if it is very likely\n",
        "then the odds become close to infinite. Taking the logarithm of the odds\n",
        "maps the odds from the positive half space to being across the entire\n",
        "real line. Odds that were between 0 and 1 (where the negative outcome\n",
        "was more likely) are mapped to the range between $-\\infty$ and $0$. Odds\n",
        "that are greater than 1 are mapped to the range between $0$ and\n",
        "$\\infty$. Considering the log odds therefore takes a number between 0\n",
        "and 1 (the probability of positive outcome) and maps it to the entire\n",
        "real line. The function that does this is known as the [logit\n",
        "function](http://en.wikipedia.org/wiki/Logit),\n",
        "$g(p_i) = \\log\\frac{p_i}{1-p_i}$. This function is known as a *link\n",
        "function*.\n",
        "\n",
        "For a standard regression we take, $$\n",
        "f(\\mathbf{ x}) = \\mathbf{ w}^\\top\n",
        "\\boldsymbol{ \\phi}(\\mathbf{ x}),\n",
        "$$ if we want to perform classification we perform a logistic\n",
        "regression. $$\n",
        "\\log \\frac{\\pi}{(1-\\pi)} = \\mathbf{ w}^\\top\n",
        "\\boldsymbol{ \\phi}(\\mathbf{ x})\n",
        "$$ where the odds ratio between the positive class and the negative\n",
        "class is given by $$\n",
        "\\frac{\\pi}{(1-\\pi)}\n",
        "$$ The odds can never be negative, but can take any value from 0 to\n",
        "$\\infty$. We have defined the link function as taking the form\n",
        "$g^{-1}(\\cdot)$ implying that the inverse link function is given by\n",
        "$g(\\cdot)$. Since we have defined, $$\n",
        "g(\\pi) =\n",
        "\\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x})\n",
        "$$ we can write $\\pi$ in terms of the *inverse link* function,\n",
        "$h(\\cdot)$ as $$\n",
        "\\pi = h(\\mathbf{ w}^\\top\n",
        "\\boldsymbol{ \\phi}(\\mathbf{ x})).\n",
        "$$"
      ],
      "id": "Bbsm434pgd4I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXeo5qASgd4I"
      },
      "outputs": [],
      "source": [
        "import mlai.plot as plot"
      ],
      "id": "cXeo5qASgd4I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u302svu3gd4J"
      },
      "outputs": [],
      "source": [
        "plot.logistic('./ml')"
      ],
      "id": "u302svu3gd4J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltLKL0bUgd4J"
      },
      "source": [
        "We’ll define our prediction, objective and gradient functions below. But\n",
        "before we start, we need to define a basis function for our model. Let’s\n",
        "start with the linear basis."
      ],
      "id": "ltLKL0bUgd4J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG_Sf0Fpgd4J"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "kG_Sf0Fpgd4J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h06CQzegd4J"
      },
      "outputs": [],
      "source": [
        "import mlai\n",
        "import inspect\n",
        "file_path = inspect.getfile(mlai.linear)"
      ],
      "id": "2h06CQzegd4J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8KYNhI8gd4K"
      },
      "outputs": [],
      "source": [
        "%load -s linear {file_path}"
      ],
      "id": "z8KYNhI8gd4K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzcAEfzlgd4K"
      },
      "source": [
        "### Sigmoid Function\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/sigmoid-function.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/sigmoid-function.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
      ],
      "id": "mzcAEfzlgd4K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVK4MNwCgd4K"
      },
      "outputs": [],
      "source": [
        "import mlai.plot as plot"
      ],
      "id": "WVK4MNwCgd4K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xlxuHIcgd4K"
      },
      "outputs": [],
      "source": [
        "plot.logistic('./ml')"
      ],
      "id": "0xlxuHIcgd4K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdtqqMNggd4K"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/logistic.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The logistic function.</i>\n",
        "\n",
        "The function has this characeristic ‘s’-shape (from where the term\n",
        "sigmoid, as in sigma, comes from). It also takes the input from the\n",
        "entire real line and ‘squashes’ it into an output that is between zero\n",
        "and one. For this reason it is sometimes also called a ‘squashing\n",
        "function.’\n",
        "\n",
        "The sigmoid comes from the inverting the odds ratio, $$\n",
        "\\frac{\\pi}{(1-\\pi)}\n",
        "$$ where $\\pi$ is the probability of a positive outcome and $1-\\pi$ is\n",
        "the probability of a negative outcome"
      ],
      "id": "cdtqqMNggd4K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0pFb29-gd4L"
      },
      "source": [
        "## Prediction Function\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression-prediction-function.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression-prediction-function.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Now we have the basis function let’s define the prediction function."
      ],
      "id": "K0pFb29-gd4L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGNn0qI4gd4L"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "cGNn0qI4gd4L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "110436xBgd4L"
      },
      "outputs": [],
      "source": [
        "def predict(w, x, basis=mlai.linear, **kwargs):\n",
        "    \"Generates the prediction function and the basis matrix.\"\n",
        "    Phi = basis(x, **kwargs)\n",
        "    f = np.dot(Phi, w)\n",
        "    return 1./(1+np.exp(-f)), Phi"
      ],
      "id": "110436xBgd4L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVXqE-8xgd4L"
      },
      "source": [
        "This inverse of the link function is known as the\n",
        "[logistic](http://en.wikipedia.org/wiki/Logistic_function) (thus the\n",
        "name logistic regression) or sometimes it is called the sigmoid\n",
        "function. For a particular value of the input to the link function,\n",
        "$f_i = \\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x}_i)$ we can plot\n",
        "the value of the inverse link function as below.\n",
        "\n",
        "By replacing the inverse link with the sigmoid we can write $\\pi$ as a\n",
        "function of the input and the parameter vector as, $$\n",
        "\\pi(\\mathbf{ x},\\mathbf{ w}) = \\frac{1}{1+\\exp\\left(-\\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x})\\right)}.\n",
        "$$ The process for logistic regression is as follows. Compute the output\n",
        "of a standard linear basis function composition\n",
        "($\\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x})$, as we did for\n",
        "linear regression) and then apply the inverse link function,\n",
        "$g(\\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x}))$. In logistic\n",
        "regression this involves *squashing* it with the logistic (or sigmoid)\n",
        "function. Use this value, which now has an interpretation as a\n",
        "*probability* in a Bernoulli distribution to form the likelihood. Then\n",
        "we can assume conditional independence of each data point given the\n",
        "parameters and develop a likelihod for the entire data set.\n",
        "\n",
        "The Bernoulli likelihood is of the form, $$\n",
        "P(y_i|\\mathbf{ w}, \\mathbf{ x}) =\n",
        "\\pi_i^{y_i} (1-\\pi_i)^{1-y_i}\n",
        "$$ which we can think of as clever trick for mathematically switching\n",
        "between two probabilities if we were to write it as code it would be\n",
        "better described as\n",
        "\n",
        "``` python\n",
        "def bernoulli(x, y, pi):\n",
        "    if y == 1:\n",
        "        return pi(x)\n",
        "    else:\n",
        "        return 1-pi(x)\n",
        "```\n",
        "\n",
        "but writing it mathematically makes it easier to write our objective\n",
        "function within a single mathematical equation."
      ],
      "id": "bVXqE-8xgd4L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS-YaQ9Rgd4M"
      },
      "source": [
        "## Objective Function\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression-maximum-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression-maximum-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "As normal, we would like to minimise this objective. This can be done by\n",
        "differentiating with respect to the parameters of our prediction\n",
        "function, $\\pi(\\mathbf{ x};\\mathbf{ w})$, for optimisation. The gradient\n",
        "of the likelihood with respect to $\\pi(\\mathbf{ x};\\mathbf{ w})$ is of\n",
        "the form, $$\n",
        "\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}\\mathbf{ w}} = -\\sum_{i=1}^n\n",
        "\\frac{y_i}{h\\left(\\mathbf{ w}^\\top\n",
        "\\boldsymbol{ \\phi}(\\mathbf{ x})\\right)}\\frac{\\text{d}h(f_i)}{\\text{d}f_i}\n",
        "\\boldsymbol{ \\phi}(\\mathbf{ x}_i) +  \\sum_{i=1}^n\n",
        "\\frac{1-y_i}{1-h\\left(\\mathbf{ w}^\\top\n",
        "\\boldsymbol{ \\phi}(\\mathbf{ x})\\right)}\\frac{\\text{d}h(f_i)}{\\text{d}f_i}\n",
        "\\boldsymbol{ \\phi}(\\mathbf{ x}_i)\n",
        "$$ where we used the chain rule to develop the derivative in terms of\n",
        "$\\frac{\\text{d}h(f_i)}{\\text{d}f_i}$, which is the gradient of the\n",
        "inverse link function (in our case the gradient of the sigmoid\n",
        "function).\n",
        "\n",
        "So the objective function now depends on the gradient of the inverse\n",
        "link function, as well as the likelihood depends on the gradient of the\n",
        "inverse link function, as well as the gradient of the log likelihood,\n",
        "and naturally the gradient of the argument of the inverse link function\n",
        "with respect to the parameters, which is simply\n",
        "$\\boldsymbol{ \\phi}(\\mathbf{ x}_i)$.\n",
        "\n",
        "The only missing term is the gradient of the inverse link function. For\n",
        "the sigmoid squashing function we have, $$\\begin{align*}\n",
        "h(f_i) &= \\frac{1}{1+\\exp(-f_i)}\\\\\n",
        "&=(1+\\exp(-f_i))^{-1}\n",
        "\\end{align*}$$ and the gradient can be computed as $$\\begin{align*}\n",
        "\\frac{\\text{d}h(f_i)}{\\text{d} f_i} & =\n",
        "\\exp(-f_i)(1+\\exp(-f_i))^{-2}\\\\\n",
        "& = \\frac{1}{1+\\exp(-f_i)}\n",
        "\\frac{\\exp(-f_i)}{1+\\exp(-f_i)} \\\\\n",
        "& = h(f_i) (1-g(f_i))\n",
        "\\end{align*}$$ so the full gradient can be written down as $$\n",
        "\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}\\mathbf{ w}} = -\\sum_{i=1}^n\n",
        "y_i\\left(1-h\\left(\\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x})\\right)\\right)\n",
        "\\boldsymbol{ \\phi}(\\mathbf{ x}_i) +  \\sum_{i=1}^n\n",
        "(1-y_i)\\left(h\\left(\\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x})\\right)\\right)\n",
        "\\boldsymbol{ \\phi}(\\mathbf{ x}_i).\n",
        "$$"
      ],
      "id": "PS-YaQ9Rgd4M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrmL2EJlgd4M"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "PrmL2EJlgd4M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClpFNMR2gd4M"
      },
      "outputs": [],
      "source": [
        "def gradient(h, Phi, y):\n",
        "    \"Generates the gradient of the parameter vector.\"\n",
        "    labs = np.asarray(y, dtype=float).flatten()\n",
        "    posind = np.where(labs==1)\n",
        "    dw = -(Phi[posind]*(1-g[posind])).sum(0)\n",
        "    negind = np.where(labs==0 )\n",
        "    dw += (Phi[negind]*h[negind]).sum(0)\n",
        "    return dw[:, None]"
      ],
      "id": "ClpFNMR2gd4M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujsw8P7Agd4M"
      },
      "source": [
        "## Optimisation of the Function\n",
        "\n",
        "Reorganising the gradient to find a stationary point of the function\n",
        "with respect to the parameters $\\mathbf{ w}$ turns out to be impossible.\n",
        "Optimisation has to proceed by *numerical methods*. Options include the\n",
        "multidimensional variant of [Newton’s\n",
        "method](http://en.wikipedia.org/wiki/Newton%27s_method) or [gradient\n",
        "based optimisation\n",
        "methods](http://en.wikipedia.org/wiki/Gradient_method) like we used for\n",
        "optimising matrix factorisation for the movie recommender system. We\n",
        "recall from matrix factorisation that, for large data, *stochastic\n",
        "gradient descent* or the Robbins Munro (Robbins and Monro, 1951)\n",
        "optimisation procedure worked best for function minimisation.\n",
        "\n",
        "-   Can’t find a stationary point of the objective function\n",
        "    analytically.\n",
        "-   Optimisation has to proceed by *numerical methods*.\n",
        "    -   [Newton’s\n",
        "        method](http://en.wikipedia.org/wiki/Newton%27s_method) or\n",
        "    -   [gradient based optimisation\n",
        "        methods](http://en.wikipedia.org/wiki/Gradient_method)\n",
        "    -   *Iterative Reweighted Least Squares*\n",
        "-   Similarly to matrix factorisation, for large data *stochastic\n",
        "    gradient descent* (Robbins Munro (Robbins and Monro, 1951)\n",
        "    optimisation procedure) works well.\n",
        "\n",
        "<!--include{_datasets/includes/classification-toy-data.md}\n",
        "\n",
        "Now from the toy data we create design matrices with a leading column of ones (an Eins column)\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.cell .code}\n",
        "\n",
        "```{.python}\n",
        "import numpy as np"
      ],
      "id": "Ujsw8P7Agd4M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu4-cP4jgd4N"
      },
      "outputs": [],
      "source": [
        "phi_plus = np.hstack([np.ones((x_plus.shape[0], 1)), x_plus])\n",
        "phi_minus = np.hstack([np.ones((x_minus.shape[0], 1)), x_minus])"
      ],
      "id": "uu4-cP4jgd4N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_aS5Unlgd4N"
      },
      "source": [
        "## Logistic Regression vs Perceptron\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression-perceptron.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression-perceptron.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We already looked at the Perceptron, where the prediction function was\n",
        "given by $$\n",
        "\\mappingFunction(\\mappingVector, \\inputVector_i) = \\text{sign}(\\designVector_i^\\top \\mappingVector)\n",
        "$$ and the sign of the output gave us the class, where we had a positive\n",
        "and negative class.\n",
        "\n",
        "In logistic regression, the prediction function gives us the probability\n",
        "of positive class, $$\n",
        "\\mappingFunction(\\mappingVector, \\inputVector) = \\pi_i = \\transformationFunction(\\designVector_i^\\top \\mappingVector)\n",
        "$$ so the two are closely related. We can think of the sigmoid as\n",
        "providing a “soft decision” whereas the sign provides a hard decision.\n",
        "\n",
        "Let’s make a more detailed study by considering a *stochastic gradient\n",
        "descent* algorithm for the logistic regression of the negative log\n",
        "likelihood. We’ll compare that update to the Perceptron update.\n",
        "\n",
        "We can think of the Perceptron update as randomly sampling a data point,\n",
        "$i$, checking its prediction. If the predicted class doesn’t match the\n",
        "true class, $y_i$, then we update the weights by subtracting\n",
        "$\\designVector_i$ if the prediction is positive, and adding\n",
        "$\\designVector_i$ if the prediction is negative. Using\n",
        "$\\dataScalar_i\\in\\{0,1\\}$ to match logistic regression, a compact update\n",
        "is $$\n",
        " \\mappingVector_\\text{new} \\leftarrow \\mappingVector_\\text{old} - \\eta(\\heaviside(\\designVector_i^\\top \\mappingVector) (1-\\dataScalar_i) \\designVector_i - (1-\\heaviside(\\designVector_i^\\top \\mappingVector)) \\dataScalar_i \\designVector_i\n",
        "$$\n",
        "\n",
        "The gradient of the negative log-likelihood of logistic regression is $$\n",
        "\\frac{\\text{d}\\errorFunction(\\mappingVector)}{\\text{d}\\mappingVector} = -\\sum_{i=1}^\\numData\n",
        "\\dataScalar_i\\left(1-\\transformationFunction\\left(\\mappingVector^\\top \\designVector_i\\right)\\right)\n",
        "\\designVector_i +  \\sum_{i=1}^\\numData\n",
        "(1-\\dataScalar_i)\\left(\\transformationFunction\\left(\\mappingVector^\\top \\designVector_i\\right)\\right)\n",
        "\\designVector_i.\n",
        "$$ so the gradient with respect to one point is $$\n",
        "\\frac{\\text{d}\\errorFunction_i(\\mappingVector)}{\\text{d}\\mappingVector}=\\dataScalar_i\\left(1-\\transformationFunction\\left(\\mappingVector^\\top \\designVector_i\\right)\\right)\n",
        "\\designVector_i + (1-\\dataScalar_i)\\left(\\transformationFunction\\left(\\mappingVector^\\top \\designVector_i\\right)\\right)\n",
        "\\designVector_i.\n",
        "$$ and the stochastic gradient update for logistic regression (with\n",
        "mini-batch size set to 1) is $$\n",
        "\\mappingVector_\\text{new} \\leftarrow \\mappingVector_\\text{old} - \\eta ((1-\\dataScalar_i)\\left(\\transformationFunction\\left(\\mappingVector^\\top \\designVector_i\\right)\\right) - \\dataScalar_i\\left(1-\\transformationFunction\\left(\\mappingVector^\\top \\designVector_i\\right)\\right)\n",
        "\\designVector_i)\n",
        "$$ The difference between the two is that for the perceptron we are\n",
        "using the Heaviside function, $\\heaviside(\\cdot)$, whereas for logistic\n",
        "regression we’re using the sigmoid, $\\transformationFunction(\\cdot)$,\n",
        "which is like a soft version of the Heaviside function."
      ],
      "id": "6_aS5Unlgd4N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLTrTaBLgd4N"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai\n",
        "import mlai.plot"
      ],
      "id": "iLTrTaBLgd4N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9tmq_Sugd4N"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=plot.two_figsize)\n",
        "f = np.linspace(-8, 8, 100)\n",
        "h = 1/(1+np.exp(-f))\n",
        "H = np.zeros(100)\n",
        "H[51:] = 1.0\n",
        "ax.plot(f, h, 'r-', lw=3)\n",
        "ax.plot(f, H, 'b-', lw=3)\n",
        "\n",
        "ax.set_title('Sigmoid vs Heaviside Function', fontsize=20)\n",
        "ax.set_xlabel('$f_i$', fontsize=20)\n",
        "ax.set_ylabel('$h_i$', fontsize=20)\n",
        "mlai.write_figure('sigmoid-heaviside.svg', directory='\\writeDiagrams/ml', transparent=True)\n"
      ],
      "id": "t9tmq_Sugd4N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0pLJmqsgd4N"
      },
      "source": [
        "Because $\\pi(\\designVector_i)=\\transformationFunction(\\designVector_i)$,\n",
        "when $\\transformationFunction(\\designVector_i)>0.5$ we classify as the\n",
        "positive class, otherwise we classify as the negative class. So the\n",
        "decision boundary is just like the peceptron. The vector\n",
        "$\\mappingVector$ gives the normal vector for the decision boundary.\n",
        "\n",
        "In two dimensions, in the linear basis, the prediction function is $$\n",
        "\\transformationFunction\\left(w_1 \\inputScalar_{i, 1} + w_2 \\inputScalar_{i, 2} + b\\right)\n",
        "$$ and the equation of a plane is $$\n",
        "w_1 \\inputScalar_{i, 1} + w_2 \\inputScalar_{i, 2} + b = 0\n",
        "$$ or $$\n",
        "w_1 \\inputScalar_{i, 1} + w_2 \\inputScalar_{i, 2} = -b.\n",
        "$$\n",
        "\n",
        "We can mirror the way we implemented the Perceptron by selecting a\n",
        "point, $i$, at random and setting the logistic regression weights to\n",
        "that point, $\\mappingVector = \\designVector_i$, if it is in the positive\n",
        "class, and setting to $\\mappingVector = -\\designVector_i$ if it is in\n",
        "the negatvie class. That guarantees that the selected point will be\n",
        "correctly classified."
      ],
      "id": "D0pLJmqsgd4N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBggtyY7gd4O"
      },
      "outputs": [],
      "source": [
        "def init_logistic_regression(phi_plus, phi_minus, seed=1000001):\n",
        "    \"\"\"\n",
        "    Initialise the perceptron algorithm with random weights and bias.\n",
        "\n",
        "    The perceptron is a simple binary classifier that learns a linear decision boundary.\n",
        "    This function initialises the weight vector w and bias b by randomly selecting\n",
        "    a point from either the positive or negative class and setting the normal vector\n",
        "    accordingly.\n",
        "\n",
        "    Mathematical formulation:\n",
        "    The perceptron decision function is f(x) = w^T φ, where:\n",
        "    - w is the weight vector (normal to the decision boundary)\n",
        "    - φ is the design matrix vector\n",
        "\n",
        "    :param phi_plus: Positive class data points, shape (n_plus, n_features)\n",
        "    :type phi_plus: numpy.ndarray\n",
        "    :param phi_minus: Negative class data points, shape (n_minus, n_features)\n",
        "    :type phi_minus: numpy.ndarray\n",
        "    :param seed: Random seed for reproducible initialisation\n",
        "    :type seed: int, optional\n",
        "    :returns: Initial weight vector, shape (n_features,)\n",
        "    :rtype: numpy.ndarray\n",
        "    :returns: Initial bias term\n",
        "    :rtype: float\n",
        "    :returns: The randomly selected point used for initialisation\n",
        "    :rtype: numpy.ndarray\n",
        "\n",
        "    Examples:\n",
        "        >>> phi_plus = np.array([[1, 1, 2], [1, 2, 3], [1, 3, 4]])\n",
        "        >>> phi_minus = np.array([[1, 0, 0], [1, 1, 0], [1, 0, 1]])\n",
        "        >>> w, phi_select = init_logistic_regression(phi_plus, phi_minus)\n",
        "        >>> print(f\"Weight vector: {w}\")\n",
        "    \"\"\"\n",
        "    np.random.seed(seed=seed)\n",
        "    # flip a coin (i.e. generate a random number and check if it is greater than 0.5)\n",
        "    plus_portion = phi_plus.shape[0]/(phi_plus.shape[0] + phi_minus.shape[0])\n",
        "    choose_plus = np.random.rand(1)<plus_portion\n",
        "    if choose_plus:\n",
        "        # generate a random point from the positives\n",
        "        index = np.random.randint(0, phi_plus.shape[0])\n",
        "        phi_select = phi_plus[index, :]\n",
        "        w = phi_plus[index, :].astype(float)  # set the normal vector to plus that point\n",
        "    else:\n",
        "        # generate a random point from the negatives\n",
        "        index = np.random.randint(0, phi_minus.shape[0])\n",
        "        phi_select = phi_minus[index, :]\n",
        "        w = -phi_minus[index, :].astype(float)  # set the normal vector to minus that point.\n",
        "    return w, phi_select"
      ],
      "id": "wBggtyY7gd4O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGsWBQ1qgd4O"
      },
      "outputs": [],
      "source": [
        "def init_logistic_regression_plot(f, ax, phi_plus, phi_minus, w, fontsize=18):\n",
        "    \"\"\"\n",
        "    Initialize a plot for showing the logistic decision boundary.\n",
        "\n",
        "    :param f: Matplotlib figure object.\n",
        "    :param ax: Array of matplotlib axes (should have 2 axes).\n",
        "    :param phi_plus: Positive class design points (numpy array).\n",
        "    :param phi_minus: Negative class design  points (numpy array).\n",
        "    :param w: Weight vector for the logistic regression.\n",
        "    :param fontsize: Font size for labels and titles (default: 18).\n",
        "    :returns: Dictionary containing plot handles for updating.\n",
        "    \"\"\"\n",
        "    h = {}\n",
        "\n",
        "    ax[0].set_aspect('equal')\n",
        "    # Plot the data again\n",
        "    ax[0].plot(phi_plus[:, 1], phi_plus[:, 2], 'rx')\n",
        "    ax[0].plot(phi_minus[:, 1], phi_minus[:, 2], 'go')\n",
        "    plot_limits = {}\n",
        "    plot_limits['x'] = np.asarray(ax[0].get_xlim())\n",
        "    plot_limits['y'] = np.asarray(ax[0].get_ylim())\n",
        "    x0, x1 = plot.hyperplane_coordinates(w[1:], w[0], plot_limits)\n",
        "    strt = -w[0]/w[2]\n",
        "\n",
        "    norm = w[1]*w[1] + w[2]*w[2]\n",
        "    offset0 = -w[1]/norm*w[0]\n",
        "    offset1 = -w[2]/norm*w[0]\n",
        "    h['arrow'] = ax[0].arrow(offset0, offset1, offset0+w[1], offset1+w[2], head_width=0.2)\n",
        "    # plot a line to represent the separating 'hyperplane'\n",
        "    h['plane'], = ax[0].plot(x0, x1, 'b-')\n",
        "    ax[0].set_xlim(plot_limits['x'])\n",
        "    ax[0].set_ylim(plot_limits['y'])\n",
        "    ax[0].set_xlabel('$x_0$', fontsize=fontsize)\n",
        "    ax[0].set_ylabel('$x_1$', fontsize=fontsize)\n",
        "    h['iter'] = ax[0].set_title('Update 0')\n",
        "\n",
        "    bins = 15\n",
        "    f_minus = np.dot(phi_minus, w)\n",
        "    f_plus = np.dot(phi_plus, w)\n",
        "    ax[1].hist(f_plus, bins, alpha=0.5, label='+1', color='r')\n",
        "    ax[1].hist(f_minus, bins, alpha=0.5, label='-1', color='g')\n",
        "    ax[1].legend(loc='upper right')\n",
        "    return h\n"
      ],
      "id": "CGsWBQ1qgd4O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b2q4kl8gd4P"
      },
      "outputs": [],
      "source": [
        "import mlai.plot as plot"
      ],
      "id": "4b2q4kl8gd4P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqu0XeOUgd4P"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(1, 2, figsize=(14,7))\n",
        "w, phi_select = init_logistic_regression(phi_plus, phi_minus)\n",
        "handle = init_logistic_regression_plot(f, ax, phi_plus, phi_minus, w)\n",
        "mlai.write_figure(\"logistic-regression_init.svg\", directory=\"./ml\")"
      ],
      "id": "Lqu0XeOUgd4P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s_fl0JTgd4P"
      },
      "source": [
        "Learning then proceeds"
      ],
      "id": "_s_fl0JTgd4P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di0UbOUPgd4U"
      },
      "outputs": [],
      "source": [
        "def logistic(x):\n",
        "    \"\"\"\n",
        "    Compute the logistic function.\n",
        "    \"\"\"\n",
        "    return 1/(1+np.exp(x))\n",
        "\n",
        "def update_logistic_regression(w, phi_plus, phi_minus, learn_rate):\n",
        "    \"\"\"\n",
        "    Update the logistic regression weights and bias using stochastic gradient descent.\n",
        "\n",
        "    This function implements one step of the perceptron learning algorithm.\n",
        "    It randomly selects a training point and updates the weights if the point\n",
        "    is misclassified.\n",
        "\n",
        "    Mathematical formulation:\n",
        "    The perceptron update rule is:\n",
        "    - If y_i = +1 and w^T φ_i ≤ 0: w ← w + η    h(w^Tφ_i) φ_i\n",
        "    - If y_i = -1 and w^T φ_i > 0: w ← w - η (1-h(w^Tφ_i))φ_i\n",
        "\n",
        "    where η is the learning rate.\n",
        "\n",
        "    :param w: Current weight vector, shape (n_features,)\n",
        "    :type w: numpy.ndarray\n",
        "    :param phi_plus: Positive class design matrix, shape (n_plus, n_features)\n",
        "    :type phi_plus: numpy.ndarray\n",
        "    :param phi_minus: Negative class design matrix, shape (n_minus, n_features)\n",
        "    :type phi_minus: numpy.ndarray\n",
        "    :param learn_rate: Learning rate (step size) for weight updates\n",
        "    :type learn_rate: float\n",
        "    :returns: Updated weight vector\n",
        "    :rtype: numpy.ndarray\n",
        "    :returns: Updated bias term\n",
        "    :rtype: float\n",
        "    :returns: The randomly selected point used for the update\n",
        "    :rtype: numpy.ndarray\n",
        "    :returns: True if weights were updated, False otherwise\n",
        "    :rtype: bool\n",
        "\n",
        "    Examples:\n",
        "        >>> w = np.array([0.1 0.5, -0.3])\n",
        "        >>> phi_plus = np.array([[1, 1, 2], [1, 2, 3]])\n",
        "        >>> phi_minus = np.array([[1, 0, 0], [1, 1, 0]])\n",
        "        >>> w_new, phi_select, updated = update_perceptron(w, phi_plus, phi_minus, 0.1)\n",
        "    \"\"\"\n",
        "    # select a point at random from the data\n",
        "    plus_portion = phi_plus.shape[0]/(phi_plus.shape[0] + phi_minus.shape[0])\n",
        "    choose_plus = np.random.rand(1)<plus_portion\n",
        "    updated=False\n",
        "    if choose_plus:\n",
        "        # choose a point from the positive data\n",
        "        index = np.random.randint(phi_plus.shape[0])\n",
        "        phi_select = phi_plus[index, :]\n",
        "        a = np.dot(w, phi_select)\n",
        "        if np.dot(w, phi_select) <= 0.:\n",
        "            # point is currently incorrectly classified\n",
        "            w += learn_rate*logistic(a)*phi_select\n",
        "            updated=True\n",
        "    else:\n",
        "        # choose a point from the negative data\n",
        "        index = np.random.randint(phi_minus.shape[0])\n",
        "        phi_select = phi_minus[index, :]\n",
        "        a = np.dot(w, phi_select)\n",
        "        if a > 0.:\n",
        "            # point is currently incorrectly classified\n",
        "            w -= learn_rate*(1-logistic(a))*phi_select\n",
        "    return w, phi_select"
      ],
      "id": "di0UbOUPgd4U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk7e58Bcgd4U"
      },
      "outputs": [],
      "source": [
        "def update_logistic_plot(h, f, ax, phi_plus, phi_minus, i, w):\n",
        "    \"\"\"\n",
        "    Update plots after decision boundary has changed.\n",
        "\n",
        "    :param h: Dictionary containing plot handles from init_logistic_regression.\n",
        "    :param f: Matplotlib figure object.\n",
        "    :param ax: Array of matplotlib axes.\n",
        "    :param phi_plus: Positive class data points.\n",
        "    :param phi_minus: Negative class data points.\n",
        "    :param i: Current iteration number.\n",
        "    :param w: Updated weight vector.\n",
        "    \"\"\"\n",
        "    # Re-plot the hyper plane\n",
        "    plot_limits = {}\n",
        "    plot_limits['x'] = np.asarray(ax[0].get_xlim())\n",
        "    plot_limits['y'] = np.asarray(ax[0].get_ylim())\n",
        "    x0, x1 = plot.hyperplane_coordinates(w[1:], w[0], plot_limits)\n",
        "\n",
        "    # Add arrow to represent hyperplane.\n",
        "    h['arrow'].remove()\n",
        "    del(h['arrow'])\n",
        "    norm = (w[1]*w[1] + w[2]*w[2])\n",
        "    offset0 = -w[1]/norm*w[0]\n",
        "    offset1 = -w[2]/norm*w[0]\n",
        "    h['arrow'] = ax[0].arrow(offset0, offset1, offset0+w[1],\n",
        "                             offset1+w[2], head_width=0.2)\n",
        "\n",
        "    h['plane'].set_xdata(x0)\n",
        "    h['plane'].set_ydata(x1)\n",
        "\n",
        "    h['iter'].set_text('Iteration ' + str(i))\n",
        "    ax[1].cla()\n",
        "    bins = 15\n",
        "    f_minus = np.dot(phi_minus, w)\n",
        "    f_plus = np.dot(phi_plus, w)\n",
        "    ax[1].hist(f_plus, bins, alpha=0.5, label='+1', color='r')\n",
        "    ax[1].hist(f_minus, bins, alpha=0.5, label='-1', color='g')\n",
        "    ax[1].legend(loc='upper right')\n",
        "\n",
        "    IPython.display.display(f)\n",
        "    IPython.display.clear_output(wait=True)\n",
        "    return h\n",
        "\n"
      ],
      "id": "mk7e58Bcgd4U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dQh0aKEgd4V"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import mlai\n",
        "import mlai.plot\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "2dQh0aKEgd4V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTSR-TgRgd4V"
      },
      "outputs": [],
      "source": [
        "diagrams = './ml'\n",
        "seed = 42\n",
        "max_iters = 30\n",
        "learn_rate = 0.01\n",
        "w, phi_select = init_logistic_regression(phi_plus, phi_minus, seed=seed)\n",
        "count = 0\n",
        "iterations = 0\n",
        "setup=True\n",
        "f2, ax2 = plt.subplots(1, 2, figsize=plot.two_figsize)\n",
        "handle = init_logistic_regression_plot(f2, ax2, phi_plus, phi_minus, w)\n",
        "handle['plane'].set_visible(False)\n",
        "handle['arrow'].set_visible(False)\n",
        "handle['circle'] = plt.Circle((phi_select[0], phi_select[1]), 0.25, color='b', fill=False)\n",
        "ax2[0].add_artist(handle['circle'])\n",
        "mlai.write_figure(figure=f2, filename='logistic-regression{samp:0>3}.svg'.format(samp=count), directory=diagrams, transparent=True)\n",
        "extent = ax2[0].get_window_extent().transformed(f2.dpi_scale_trans.inverted())\n",
        "mlai.write_figure(figure=f2, filename='logistic-regression{samp:0>3}.png'.format(samp=count), directory=diagrams, bbox_inches=extent, transparent=True)\n",
        "count += 1\n",
        "handle['plane'].set_visible(True)\n",
        "handle['arrow'].set_visible(True)\n",
        "mlai.write_figure(figure=f2, filename='logistic-regression{samp:0>3}.svg'.format(samp=count), directory=diagrams, transparent=True)\n",
        "mlai.write_figure(figure=f2, filename='logistic-regression{samp:0>3}.png'.format(samp=count), directory=diagrams, bbox_inches=extent, transparent=True)\n",
        "\n",
        "while iterations<max_iters:\n",
        "    iterations += 1\n",
        "    w, phi_select = update_logistic_regression(w, phi_plus, phi_minus, learn_rate)\n",
        "    count+=1\n",
        "    handle['circle'].center = phi_select[1], phi_select[2]\n",
        "    mlai.write_figure(figure=f2, filename='logistic-regression{samp:0>3}.svg'.format(samp=count), directory=diagrams, transparent=True)\n",
        "    mlai.write_figure(figure=f2, filename='logistic-regression{samp:0>3}.png'.format(samp=count), bbox_inches=extent, directory=diagrams, transparent=True)\n",
        "    count+=1\n",
        "    handle = update_logistic_plot(handle, f2, ax2, phi_plus, phi_minus, iterations, w)\n",
        "    mlai.write_figure(filename='logistic-regression{samp:0>3}.svg'.format(samp=count),\n",
        "                      figure=f2,\n",
        "                      directory=diagrams,\n",
        "                      transparent=True)\n",
        "    mlai.write_figure(filename='logistic-regression{samp:0>3}.png'.format(samp=count),\n",
        "                      figure=f2,\n",
        "                      directory=diagrams,\n",
        "                      bbox_inches=extent,\n",
        "                      transparent=True)\n",
        "print('Data passes:', iterations)\n"
      ],
      "id": "RTSR-TgRgd4V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gEIOmt3gd4V"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "1gEIOmt3gd4V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlQBVEbCgd4V"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('logistic-regression{samp:0>3}.svg', directory='./ml', samp=(0, count))"
      ],
      "id": "PlQBVEbCgd4V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acI8SHHzgd4W"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/logistic-regression014.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The logistic-regression decision boundary.</i>\n",
        "\n",
        "–\\>\n",
        "\n",
        "    ::: {.cell .markdown}\n",
        "    # Nigeria NMIS Data\n",
        "\n",
        "    ::: {style=\"text-align:right\"}\n",
        "    [\\[`</span>`{=html}[`<a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/nigeria-nmis-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/nigeria-nmis-data.md', 13);\">`{=html}edit`</a>`{=html}]{.editsection\n",
        "    style=\"\"}`<span class=\"editsection-bracket\" style=\"\">`{=html}\\]]{.editsection-bracket\n",
        "    style=\"\"}\n",
        "    :::\n",
        "\n",
        "    As an example data set we will use Nigerian Millennium Development Goals\n",
        "    Information System Health Facility [@Nigeria-nmis14]. It can be found\n",
        "    here\n",
        "    <https://energydata.info/dataset/nigeria-nmis-education-facility-data-2014>.\n",
        "\n",
        "    Taking from the information on the site,\n",
        "\n",
        "    > The Nigeria MDG (Millennium Development Goals) Information System --\n",
        "    > NMIS health facility data is collected by the Office of the Senior\n",
        "    > Special Assistant to the President on the Millennium Development Goals\n",
        "    > (OSSAP-MDGs) in partner with the Sustainable Engineering Lab at\n",
        "    > Columbia University. A rigorous, geo-referenced baseline facility\n",
        "    > inventory across Nigeria is created spanning from 2009 to 2011 with an\n",
        "    > additional survey effort to increase coverage in 2014, to build\n",
        "    > Nigeria's first nation-wide inventory of health facility. The database\n",
        "    > includes 34,139 health facilities info in Nigeria.\n",
        "    >\n",
        "    > The goal of this database is to make the data collected available to\n",
        "    > planners, government officials, and the public, to be used to make\n",
        "    > strategic decisions for planning relevant interventions.\n",
        "    >\n",
        "    > For data inquiry, please contact Ms. Funlola Osinupebi, Performance\n",
        "    > Monitoring & Communications, Advisory Power Team, Office of the Vice\n",
        "    > President at funlola.osinupebi\\@aptovp.org\n",
        "    >\n",
        "    > To learn more, please visit\n",
        "    > <http://csd.columbia.edu/2014/03/10/the-nigeria-mdg-information-system-nmis-takes-open-data-further/>\n",
        "    >\n",
        "    > Suggested citation: Nigeria NMIS facility database (2014), the Office\n",
        "    > of the Senior Special Assistant to the President on the Millennium\n",
        "    > Development Goals (OSSAP-MDGs) & Columbia University\n",
        "\n",
        "    For ease of use we've packaged this data set in the `pods` library\n",
        "    :::\n",
        "\n",
        "    ::: {.cell .code}\n",
        "    ``` {.python}\n",
        "    data = pods.datasets.nigeria_nmis()['Y']\n",
        "    data.head()\n",
        "\n",
        ":::\n",
        "\n",
        "Alternatively, you can access the data directly with the following\n",
        "commands.\n",
        "\n",
        "``` python\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve('https://energydata.info/dataset/f85d1796-e7f2-4630-be84-79420174e3bd/resource/6e640a13-cab4-457b-b9e6-0336051bac27/download/healthmopupandbaselinenmisfacility.csv', 'healthmopupandbaselinenmisfacility.csv')\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('healthmopupandbaselinenmisfacility.csv')\n",
        "```\n",
        "\n",
        "Once it is loaded in the data can be summarized using the `describe`\n",
        "method in pandas."
      ],
      "id": "acI8SHHzgd4W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOhKyB-2gd4W"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ],
      "id": "kOhKyB-2gd4W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCxpoZTbgd4W"
      },
      "source": [
        "We can also find out the dimensions of the dataset using the `shape`\n",
        "property."
      ],
      "id": "RCxpoZTbgd4W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vya98r_Bgd4X"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ],
      "id": "vya98r_Bgd4X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFBJGjIugd4X"
      },
      "source": [
        "Dataframes have different functions that you can use to explore and\n",
        "understand your data. In python and the Jupyter notebook it is possible\n",
        "to see a list of all possible functions and attributes by typing the\n",
        "name of the object followed by `.<Tab>` for example in the above case if\n",
        "we type `data.<Tab>` it show the columns available (these are attributes\n",
        "in pandas dataframes) such as `num_nurses_fulltime`, and also functions,\n",
        "such as `.describe()`.\n",
        "\n",
        "For functions we can also see the documentation about the function by\n",
        "following the name with a question mark. This will open a box with\n",
        "documentation at the bottom which can be closed with the x button."
      ],
      "id": "eFBJGjIugd4X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k6y9007gd4Y"
      },
      "outputs": [],
      "source": [
        "data.describe?"
      ],
      "id": "1k6y9007gd4Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6qp13fbgd4Z"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai\n",
        "import mlai.plot as plot"
      ],
      "id": "_6qp13fbgd4Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aJrmpfggd4Z"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
        "ax.plot(data.longitude, data.latitude, 'ro', alpha=0.01)\n",
        "ax.set_xlabel('longitude')\n",
        "ax.set_ylabel('latitude')\n",
        "\n",
        "mlai.write_figure('nigerian-health-facilities.png', directory='./ml')"
      ],
      "id": "5aJrmpfggd4Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBaJ3u10gd4Z"
      },
      "source": [
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/nigerian-health-facilities.png\" style=\"width:60%\">\n",
        "\n",
        "Figure: <i>Location of the over thirty-four thousand health facilities\n",
        "registered in the NMIS data across Nigeria. Each facility plotted\n",
        "according to its latitude and longitude.</i>"
      ],
      "id": "EBaJ3u10gd4Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xly8qozbgd4Z"
      },
      "source": [
        "## Nigeria NMIS Data Classification\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/nigeria-nmis-data-classification.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/nigeria-nmis-data-classification.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Our aim will be to predict whether a center has maternal health delivery\n",
        "services given the attributes in the data. We will predict of the number\n",
        "of nurses, the number of doctors, location etc.\n",
        "\n",
        "Now we will convert this data into a form which we can use as inputs\n",
        "`X`, and labels `y`."
      ],
      "id": "xly8qozbgd4Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxtRS0_Fgd4Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "id": "MxtRS0_Fgd4Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvMIYvS3gd4Z"
      },
      "outputs": [],
      "source": [
        "data = data[~pd.isnull(data['maternal_health_delivery_services'])]\n",
        "data = data.dropna() # Remove entries with missing values\n",
        "X = data[['emergency_transport',\n",
        "          'num_chews_fulltime',\n",
        "          'phcn_electricity',\n",
        "          'child_health_measles_immun_calc',\n",
        "          'num_nurses_fulltime',\n",
        "          'num_doctors_fulltime',\n",
        "          'improved_water_supply',\n",
        "          'improved_sanitation',\n",
        "          'antenatal_care_yn',\n",
        "          'family_planning_yn',\n",
        "          'malaria_treatment_artemisinin',\n",
        "          'latitude',\n",
        "          'longitude']].copy()\n",
        "y = data['maternal_health_delivery_services']==True  # set label to be whether there's a maternal health delivery service\n",
        "\n",
        "# Create series of health center types with the relevant index\n",
        "s = data['facility_type_display'].apply(pd.Series).stack()\n",
        "s.index = s.index.droplevel(-1) # to line up with df's index\n",
        "\n",
        "# Extract from the series the unique list of types.\n",
        "types = s.unique()\n",
        "\n",
        "# For each type extract the indices where it is present and add a column to X\n",
        "type_names = []\n",
        "for htype in types:\n",
        "    index = s[s==htype].index.tolist()\n",
        "    type_col=htype.replace(' ', '_').replace('/','-').lower()\n",
        "    type_names.append(type_col)\n",
        "    X.loc[:, type_col] = 0.0\n",
        "    X.loc[index, type_col] = 1.0"
      ],
      "id": "dvMIYvS3gd4Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb7is8t1gd4a"
      },
      "source": [
        "This has given us a new data frame `X` which contains the different\n",
        "facility types in different columns."
      ],
      "id": "Nb7is8t1gd4a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcCPfcGVgd4a"
      },
      "outputs": [],
      "source": [
        "X.describe()"
      ],
      "id": "NcCPfcGVgd4a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbrV9hrwgd4a"
      },
      "source": [
        "## Batch Gradient Descent\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/nigeria-nmis-data-logistic.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/nigeria-nmis-data-logistic.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We will need to define some initial random values for our vector and\n",
        "then minimize the objective by descending the gradient."
      ],
      "id": "TbrV9hrwgd4a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXsPaY_Agd4a"
      },
      "outputs": [],
      "source": [
        "# Separate train and test\n",
        "indices = np.random.permutation(X.shape[0])\n",
        "num_train = int(np.ceil(X.shape[0]/2))\n",
        "train_indices = indices[:num_train]\n",
        "test_indices = indices[num_train:]\n",
        "X_train = X.iloc[train_indices]\n",
        "y_train = y.iloc[train_indices]==True\n",
        "X_test = X.iloc[test_indices]\n",
        "y_test = y.iloc[test_indices]==True"
      ],
      "id": "QXsPaY_Agd4a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1aY6abEgd4a"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "D1aY6abEgd4a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v81V0v3agd4a"
      },
      "outputs": [],
      "source": [
        "# gradient descent algorithm\n",
        "w_vals = np.random.normal(size=(X.shape[1]+1, 1), scale = 0.001)\n",
        "# Convert weights to a pandas DataFrame with column names\n",
        "if type(X_train) is pd.DataFrame:\n",
        "    w = pd.DataFrame(data=w_vals,\n",
        "                     index=['Eins'] + list(X_train.columns),\n",
        "                     columns=['weight'])\n",
        "\n",
        "eta = 1e-9\n",
        "iters = 10000\n",
        "for i in range(iters):\n",
        "    g, Phi = predict(w, X_train, mlai.linear)\n",
        "    w -= eta*gradient(g, Phi, y_train) + 0.001*w\n",
        "    if not i % 100:\n",
        "        print(\"Iter\", i, \"Objective\", objective(g, y_train))"
      ],
      "id": "v81V0v3agd4a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqKtzwPVgd4b"
      },
      "source": [
        "Let’s look at the weights and how they relate to the inputs."
      ],
      "id": "cqKtzwPVgd4b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQk5Os_Qgd4b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "id": "FQk5Os_Qgd4b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhL0mWeagd4b"
      },
      "outputs": [],
      "source": [
        "# Plot a histogram of the weights\n",
        "plt.figure(figsize=(10, 6))\n",
        "if type(w) is pd.DataFrame:\n",
        "    plt.bar(w.index, w['weight'])\n",
        "else:\n",
        "    plt.bar(w)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Weight Value')\n",
        "plt.title('Histogram of Model Weights')\n",
        "plt.tight_layout()\n",
        "mlai.write_figure('nmis-logistic-regression-weights.svg', directory='./ml')"
      ],
      "id": "uhL0mWeagd4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_tKrHl_gd4b"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "What does the magnitude of the weight vectors tell you about the\n",
        "different parameters and their influence on outcome? Are the weights of\n",
        "roughly the same size, if not, how might you fix this?"
      ],
      "id": "d_tKrHl_gd4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRUcLEqFgd4b"
      },
      "source": [
        "### Exercise 1 Answer\n",
        "\n",
        "Write your answer to Exercise 1 here"
      ],
      "id": "PRUcLEqFgd4b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEXozS79gd4b"
      },
      "outputs": [],
      "source": [
        "g_test, Phi_test = predict(w, X_test, mlai.linear)\n",
        "np.sum(g_test[y_test]>0.5)"
      ],
      "id": "EEXozS79gd4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDjCuiLegd4b"
      },
      "source": [
        "## Stochastic Gradient Descent"
      ],
      "id": "VDjCuiLegd4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oCYrp-Ygd4c"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Now construct a stochastic gradient descent algorithm and run it on the\n",
        "data. Is it faster or slower than batch gradient descent? What can you\n",
        "do to improve convergence speed?"
      ],
      "id": "5oCYrp-Ygd4c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjp0R3Akgd4c"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Exercise 2 here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "cjp0R3Akgd4c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thtd47Txgd4c"
      },
      "outputs": [],
      "source": [
        "%pip install statsmodels"
      ],
      "id": "Thtd47Txgd4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXy9i9Higd4c"
      },
      "source": [
        "## Linear Regression with `statsmodels`\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_statistics/includes/linear-regression-statsmodels.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_statistics/includes/linear-regression-statsmodels.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "In linear regression, we model the relationship between a continuous\n",
        "response variable $y_i$ and input variables $\\mathbf{ x}_i$ through a\n",
        "linear function with Gaussian noise:\n",
        "\n",
        "$$y_i = f(\\mathbf{ x}_i) + \\epsilon_i$$\n",
        "\n",
        "where\n",
        "$f(\\mathbf{ x}_i) = \\mathbf{ w}^\\top\\mathbf{ x}_i = \\sum_{j=1}^D w_jx_{i,j}$\n",
        "and $\\epsilon_i \\sim \\mathscr{N}\\left(0,\\sigma^2\\right)$\n",
        "\n",
        "This gives us a probabilistic model:\n",
        "\n",
        "$$p(y_i|\\mathbf{ x}_i) = \\mathscr{N}\\left(y_i|\\mathbf{ w}^\\top\\mathbf{ x}_i,\\sigma^2\\right)$$\n",
        "\n",
        "The key components are:\n",
        "\n",
        "-   $y_i$ is the target/response variable we want to predict\n",
        "-   $\\mathbf{ x}_i$ contains the input features/explanatory variables  \n",
        "-   $\\mathbf{ w}$ contains the parameters/coefficients we learn\n",
        "-   $\\epsilon_i$ represents random Gaussian noise with variance\n",
        "    $\\sigma^2$\n",
        "\n",
        "For the full dataset, we can write this in matrix form:\n",
        "\n",
        "$$\\mathbf{ y}= \\mathbf{X}\\mathbf{ w}+ \\boldsymbol{ \\epsilon}$$\n",
        "\n",
        "where $\\mathbf{ y}= [y_1,\\ldots,y_N]^\\top$, $\\mathbf{X}$ contains the\n",
        "input vectors as rows, and\n",
        "$\\boldsymbol{ \\epsilon}\\sim \\mathscr{N}\\left(\\mathbf{0},\\sigma^2\\mathbf{I}\\right)$.\n",
        "\n",
        "The expected value of our prediction is:\n",
        "\n",
        "$$\\mathbb{E}[y_i|\\mathbf{ x}_i] = \\mathbf{ w}^\\top\\mathbf{ x}_i$$\n",
        "\n",
        "This linear model forms the foundation for generalized linear models\n",
        "like logistic regression, where we’ll adapt the model for classification\n",
        "by transforming the output through a non-linear function."
      ],
      "id": "aXy9i9Higd4c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdRxovL9gd4c"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import pods"
      ],
      "id": "vdRxovL9gd4c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Huskd7BNgd4d"
      },
      "outputs": [],
      "source": [
        "# Demo of linear regression using python statsmodels.\n",
        "data = pods.datasets.olympic_marathon_men()\n",
        "x = data['X']\n",
        "y = data['Y']\n",
        "# Add constant term to design matrix\n",
        "x = sm.add_constant(x)\n",
        "model = sm.OLS(y, x)\n",
        "results = model.fit()\n",
        "results.summary()"
      ],
      "id": "Huskd7BNgd4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umrC47Xxgd4d"
      },
      "source": [
        "The statsmodels summary provides several key diagnostic measures that\n",
        "help us evaluate our model fit and identify potential areas for\n",
        "improvement. Since we’re working with one-dimensional data (year vs\n",
        "time), we can visualize everything easily to complement these\n",
        "statistical measures.\n",
        "\n",
        "The model fit statistics show a moderately strong fit, with an R-squared\n",
        "of 0.730 indicating that our model explains 73.0% of the variance in the\n",
        "data. The adjusted R-squared of 0.721 confirms this isn’t just due to\n",
        "overfitting. The very low F-statistic p-value (1.85e-09) confirms the\n",
        "model’s overall significance. The AIC (11.33) and BIC (14.13) values\n",
        "will be useful when we compare this model against alternative\n",
        "specifications we might try.\n",
        "\n",
        "Looking at the model parameters, we see a coefficient of -0.0116 for our\n",
        "predictor, with a small standard error (0.001). The t-statistic of\n",
        "-8.710 and p-value of 0.000 indicate this effect is highly significant.\n",
        "The 95% confidence interval \\[-0.014, -0.009\\] gives us good confidence\n",
        "in our estimate. The negative coefficient confirms the expected downward\n",
        "trend in marathon times over the years.\n",
        "\n",
        "However, the residual diagnostics suggest several potential issues we\n",
        "should investigate:\n",
        "\n",
        "1.  The Durbin-Watson statistic (0.981) indicates positive\n",
        "    autocorrelation in the residuals. This suggests we might want to:\n",
        "\n",
        "    -   Consider time series modeling approaches\n",
        "    -   Add polynomial terms to capture non-linear trends\n",
        "    -   Investigate if there are distinct “eras” in marathon times\n",
        "\n",
        "2.  The highly significant Jarque-Bera test (p-value 8.32e-14) tells us\n",
        "    our residuals aren’t normally distributed. The skew (1.947) and\n",
        "    kurtosis (8.746) values show the distribution is strongly\n",
        "    right-skewed with very heavy tails. We might want to:\n",
        "\n",
        "    -   Look for outliers or influential points\n",
        "    -   Consider robust regression techniques\n",
        "    -   Try transforming our response variable\n",
        "\n",
        "3.  The large condition number (9.94e+04) suggests potential numerical\n",
        "    instability or multicollinearity issues. While less concerning with\n",
        "    single-predictor models, we should:\n",
        "\n",
        "    -   Consider centering and scaling our predictor\n",
        "    -   Watch for numerical precision issues\n",
        "    -   Be cautious when extending to multiple predictors\n",
        "\n",
        "The beauty of having one-dimensional data is that we can plot everything\n",
        "to visually confirm these statistical findings. A scatter plot with our\n",
        "fitted line will help us:\n",
        "\n",
        "-   Visually assess the linearity assumption\n",
        "-   Identify potential outliers\n",
        "-   Spot any systematic patterns in the residuals\n",
        "-   See if the relationship makes practical sense in terms of marathon\n",
        "    performance over time\n",
        "\n",
        "This visual inspection, combined with our statistical diagnostics, will\n",
        "guide our next steps in improving the model."
      ],
      "id": "umrC47Xxgd4d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfrQqwukgd4d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai\n",
        "from mlai import plot"
      ],
      "id": "vfrQqwukgd4d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUmzekQcgd4d"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=mlai.plot.big_wide_figsize)\n",
        "ax.plot(x[:, 1], y, '.')\n",
        "\n",
        "# Plot the fitted line\n",
        "ax.plot(x[:, 1], results.predict(x), '-')\n",
        "\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_ylabel('Time')\n",
        "ax.set_xlim(1890, 2030)\n",
        "plt.show()\n",
        "mlai.write_figure(\"linear-regression-olympic-marathon-men-statsmodels.svg\", directory=\"./data-science\")"
      ],
      "id": "kUmzekQcgd4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBUBec48gd4d"
      },
      "source": [
        "Looking at our plot and model diagnostics, we can now better understand\n",
        "the large condition number (1.08e+05) in our results. This high value\n",
        "likely stems from using raw year values (e.g., 1896, 1900, etc.) as our\n",
        "predictor variable. Such large numbers can lead to numerical instability\n",
        "in the computations.\n",
        "\n",
        "To address this, we could consider:\n",
        "\n",
        "-   Centering the years around their mean\n",
        "-   Scaling the years to a smaller range (e.g., 0-1)\n",
        "-   Using years since the first Olympics (e.g., 0, 4, 8, etc.)\n",
        "\n",
        "Any of these transformations would help reduce the condition number\n",
        "while preserving the underlying relationship in our data. The\n",
        "coefficients would change, but the fitted values and overall model\n",
        "quality would remain the same.\n",
        "\n",
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//data-science/linear-regression-olympic-marathon-men-statsmodels.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Linear regression fit to Olympic marathon men’s times using\n",
        "`statsmodels`.</i>\n",
        "\n",
        "The plot reveals features that help explain our diagnostic statistics:\n",
        "\n",
        "-   The 1904 St. Louis Olympics appears as a clear outlier, contributing\n",
        "    to the non-normal residuals (Jarque-Bera p=0.00432) and right-skewed\n",
        "    distribution (skew=1.385)\n",
        "-   We can observe distinct regimes in the data:\n",
        "    -   Rapid improvement in times pre-WWI\n",
        "    -   Disruption and variation during the war years\n",
        "    -   More steady, consistent progress post-WWII\n",
        "-   These regime changes help explain the strong positive\n",
        "    autocorrelation (Durbin-Watson=0.242) in our residuals\n",
        "-   While our high R-squared (0.972) captures the overall downward\n",
        "    trend, these features suggest we could improve the model by adding\n",
        "    additional features:\n",
        "    -   Polynomial terms to capture non-linear trends\n",
        "    -   Indicator variables for different time periods\n",
        "    -   Interaction terms between features\n",
        "    -   Variables accounting for external factors like temperature or\n",
        "        course conditions\n",
        "\n",
        "To incorporate multiple features into our model, we need a systematic\n",
        "way to organize this additional information. This brings us to the\n",
        "concept of the design matrix."
      ],
      "id": "HBUBec48gd4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOlNa3gagd4d"
      },
      "source": [
        "### Design Matrix\n",
        "\n",
        "The design matrix, often denoted as $\\boldsymbol{ \\Phi}$, is a key\n",
        "component of a statistical model. It organizes our feature data in a\n",
        "structured way that facilitates model fitting and analysis. Each row of\n",
        "the design matrix represents a single observation or data point, while\n",
        "each column represents a different feature or predictor variable.\n",
        "\n",
        "For $n$ observations and $p$ features, the design matrix takes the form:\n",
        "\n",
        "$$\\boldsymbol{ \\Phi}= \\begin{bmatrix}\n",
        "x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n",
        "x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{n1} & x_{n2} & \\cdots & x_{np}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "For example, if we’re predicting house prices, each row might represent\n",
        "a different house, with columns for features like:\n",
        "\n",
        "-   Square footage\n",
        "-   Number of bedrooms  \n",
        "-   Year built\n",
        "-   Lot size\n",
        "\n",
        "The design matrix provides a compact way to represent all our feature\n",
        "data and is used directly in model fitting. When we write our linear\n",
        "model as\n",
        "$\\mathbf{ y}= \\boldsymbol{ \\Phi}\\mathbf{ w}+ \\boldsymbol{ \\epsilon}$,\n",
        "the design matrix $\\boldsymbol{ \\Phi}$ is multiplied by our parameter\n",
        "vector $\\mathbf{ w}$ to generate predictions.\n",
        "\n",
        "The design matrix often includes a column of 1s to account for the\n",
        "intercept term in our model. This allows us to write the model in matrix\n",
        "form without explicitly separating out the intercept term."
      ],
      "id": "fOlNa3gagd4d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YhUEdG8gd4e"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import pods\n",
        "import numpy as np"
      ],
      "id": "_YhUEdG8gd4e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoZuADzhgd4e"
      },
      "outputs": [],
      "source": [
        "# Demo of additional features with interactions regression usying python statsmodels.\n",
        "data = pods.datasets.olympic_marathon_men()\n",
        "x = data['X']\n",
        "y = data['Y']\n",
        "\n",
        "# Scale the year to avoid numerical issues\n",
        "x_scaled = (x - 1900) / 100  # Center around 1900 and scale to century units\n",
        "\n",
        "# Add to design matrix indicator variable for pre-1914\n",
        "x_aug = np.hstack([x_scaled, (x[:, 0] < 1914).astype(np.float64)[:, np.newaxis]])\n",
        "\n",
        "# Add to design matrix indicator variable for 1914-1945\n",
        "x_aug = np.hstack([x_aug, ((x[:, 0] >= 1914) & (x[:, 0] <= 1945)).astype(np.float64)[:, np.newaxis]])\n",
        "\n",
        "# Add to design matrix indicator variable for post-1945\n",
        "x_aug = np.hstack([x_aug, (x[:, 0] > 1945).astype(np.float64)[:, np.newaxis]])\n",
        "\n",
        "# Add product terms that multiply the scaled year and the indicator variables.\n",
        "x_aug = np.hstack([x_aug, x_scaled[:, 0:1] * x_aug[:, 1:2], x_scaled[:, 0:1] * x_aug[:, 2:3]])\n",
        "\n",
        "# Add constant term to design matrix\n",
        "x_aug = sm.add_constant(x_aug)\n",
        "\n",
        "# Do the linear fit\n",
        "model = sm.OLS(y, x_aug)\n",
        "results = model.fit()\n",
        "results.summary()"
      ],
      "id": "EoZuADzhgd4e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CicGI93agd4e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai\n",
        "from mlai import plot"
      ],
      "id": "CicGI93agd4e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxnu-GKqgd4e"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=mlai.plot.big_wide_figsize)\n",
        "ax.plot(x[:, 0], y, '.')\n",
        "\n",
        "# Plot the fitted line\n",
        "ax.plot(x[:, 0], results.predict(x_aug), '-')\n",
        "\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_ylabel('Time')\n",
        "ax.set_xlim(1890, 2030)\n",
        "plt.show()\n",
        "mlai.write_figure(\"linear-regression-olympic-marathon-men-augmented-statsmodels.svg\", directory=\"./data-science\")"
      ],
      "id": "jxnu-GKqgd4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdwArCPwgd4e"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//data-science/linear-regression-olympic-marathon-men-augmented-statsmodels.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Polynomial regression fit to Olympic marathon men’s times\n",
        "using `statsmodels`.</i>\n",
        "\n",
        "The augmented model with interactions shows a significant improvement in\n",
        "fit compared to the simpler linear model, with an R-squared value of\n",
        "0.877 (adjusted R-squared of 0.851). This indicates that about 87% of\n",
        "the variance in marathon times is explained by our model.\n",
        "\n",
        "The model includes several key components:\n",
        "\n",
        "-   A base time trend (x1 coefficient: -0.5634)\n",
        "-   Indicator variables for different historical periods (pre-1914,\n",
        "    1914-1945, post-1945)\n",
        "-   Interaction terms between the time trend and these periods\n",
        "\n",
        "The coefficients reveal interesting patterns:\n",
        "\n",
        "-   The pre-1914 period shows a significant positive effect (x2: 1.5700,\n",
        "    p\\<0.001)\n",
        "-   The wartime period 1914-1945 also shows a positive effect (x3:\n",
        "    0.8176, p=0.030)\n",
        "-   The post-1945 period has a positive effect (x4: 0.6300, p=0.002)\n",
        "-   The interaction terms (x5, x6) suggest different rates of\n",
        "    improvement in different periods, though these are less\n",
        "    statistically significant (x5: -3.0493, p=0.076; x6: -0.1694,\n",
        "    p=0.919)\n",
        "\n",
        "However, there are some concerns:\n",
        "\n",
        "1.  The very high condition number (2.17e+16) suggests serious\n",
        "    multicollinearity issues\n",
        "2.  The highly significant Jarque-Bera test (p-value 1.34e-24) indicates\n",
        "    non-normal residuals\n",
        "3.  There’s significant skewness (2.393) and kurtosis (11.065) in the\n",
        "    residuals\n",
        "\n",
        "Despite these statistical issues, the model captures the major trends in\n",
        "marathon times across different historical periods better than a simple\n",
        "linear regression would."
      ],
      "id": "JdwArCPwgd4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSnWj4MPgd4e"
      },
      "source": [
        "## Fitting with `statsmodels`\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_statistics/includes/logistic-regression-statsmodels.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_statistics/includes/logistic-regression-statsmodels.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
      ],
      "id": "tSnWj4MPgd4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0Nu4c9Kgd4f"
      },
      "source": [
        "## Fitting the Model\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_statistics/includes/logistic-regression-train.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_statistics/includes/logistic-regression-train.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
      ],
      "id": "h0Nu4c9Kgd4f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23nOh21Lgd4f"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification"
      ],
      "id": "23nOh21Lgd4f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5smz2wHgd4f"
      },
      "outputs": [],
      "source": [
        "# Demo of logistic regression using python statsmodels.\n",
        "# Create a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
        "                         n_informative=2, n_clusters_per_class=1,\n",
        "                         random_state=42)\n",
        "\n",
        "# Convert to DataFrame for easier handling\n",
        "df = pd.DataFrame(X, columns=['feature1', 'feature2'])\n",
        "df['target'] = y\n",
        "\n",
        "# Split into train and test sets\n",
        "indices = np.random.permutation(df.shape[0])\n",
        "num_train = int(np.ceil(df.shape[0]/2))\n",
        "train_indices = indices[:num_train]\n",
        "test_indices = indices[num_train:]\n",
        "\n",
        "X_train = df[['feature1', 'feature2']].iloc[train_indices]\n",
        "y_train = df['target'].iloc[train_indices]\n",
        "X_test = df[['feature1', 'feature2']].iloc[test_indices]\n",
        "y_test = df['target'].iloc[test_indices]\n",
        "\n",
        "# Add constant term to design matrix\n",
        "X_train_sm = sm.add_constant(X_train)\n",
        "X_test_sm = sm.add_constant(X_test)\n",
        "\n",
        "# Fit logistic regression model\n",
        "model = sm.Logit(y_train, X_train_sm)\n",
        "results = model.fit()\n",
        "results.summary()"
      ],
      "id": "t5smz2wHgd4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d60WOwQcgd4f"
      },
      "source": [
        "## Model Fit Statistics\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_statistics/includes/logistic-regression-fit.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_statistics/includes/logistic-regression-fit.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "The statsmodels summary for logistic regression provides several\n",
        "diagnostic measures that help us evaluate our classification model’s\n",
        "performance and identify potential areas for improvement.\n",
        "\n",
        "**Model Fit Statistics:** The logistic regression model doesn’t have a\n",
        "traditional R-squared since we’re dealing with binary outcomes rather\n",
        "than continuous responses. Instead, we use pseudo R-squared measures:\n",
        "\n",
        "-   **McFadden’s R-squared**: Compares the log-likelihood of our model\n",
        "    to a null model with only an intercept. Values between 0.2-0.4\n",
        "    indicate excellent fit.\n",
        "-   **Log-Likelihood Ratio (LLR) test**: Tests whether our model is\n",
        "    significantly better than the null model. A low p-value indicates\n",
        "    our predictors significantly improve the model.\n",
        "-   **AIC/BIC**: Help compare different model specifications. Lower\n",
        "    values indicate better models when comparing alternatives.\n",
        "\n",
        "**Parameter Interpretation:** The coefficients in logistic regression\n",
        "represent changes in log-odds: - A positive coefficient means the\n",
        "feature increases the odds of the positive class - A negative\n",
        "coefficient means the feature decreases the odds of the positive class  \n",
        "- The magnitude indicates the strength of the effect - To get odds\n",
        "ratios, we exponentiate the coefficients: $\\exp(\\beta_j)$\n",
        "\n",
        "For example, if a coefficient is 0.693, then $\\exp(0.693) = 2.0$,\n",
        "meaning a one-unit increase in that feature doubles the odds of the\n",
        "positive outcome.\n",
        "\n",
        "**Diagnostic Considerations:** Unlike linear regression, logistic\n",
        "regression has different diagnostic concerns:\n",
        "\n",
        "1.  **Multicollinearity**: Check condition numbers and correlation\n",
        "    matrices, just like in linear regression\n",
        "2.  **Outliers and Influential Points**: Use deviance residuals and\n",
        "    leverage measures to identify problematic observations\n",
        "3.  **Model Adequacy**: Hosmer-Lemeshow test checks if predicted\n",
        "    probabilities match observed frequencies\n",
        "4.  **Separation**: Perfect or quasi-perfect separation can cause\n",
        "    convergence issues and inflated standard errors\n",
        "\n",
        "**Classification Performance:** Beyond the statistical diagnostics, we\n",
        "should evaluate practical classification performance: - **Confusion\n",
        "Matrix**: Shows true vs predicted classifications - **Accuracy**:\n",
        "Overall percentage of correct predictions  \n",
        "- **Precision/Recall**: Important when classes are imbalanced -\n",
        "**ROC/AUC**: Measures discrimination ability across different thresholds"
      ],
      "id": "d60WOwQcgd4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28l_swnagd4f"
      },
      "source": [
        "## Predictions of the Model\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_statistics/includes/logistic-regression-test.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_statistics/includes/logistic-regression-test.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
      ],
      "id": "28l_swnagd4f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrOy1xhZgd4f"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "y_pred_proba = results.predict(X_test_sm)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "# Calculate classification metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"\\nClassification Report:\\n{classification_report(y_test, y_pred)}\")"
      ],
      "id": "DrOy1xhZgd4f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPNb8UlKgd4g"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai\n",
        "from mlai import plot"
      ],
      "id": "fPNb8UlKgd4g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfIhevrkgd4g"
      },
      "outputs": [],
      "source": [
        "# Create visualization of the logistic regression results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot 1: Data points colored by true class\n",
        "ax1 = axes[0]\n",
        "scatter = ax1.scatter(X_train['feature1'], X_train['feature2'],\n",
        "                     c=y_train, cmap='RdYlBu', alpha=0.7, s=50)\n",
        "ax1.set_xlabel('Feature 1')\n",
        "ax1.set_ylabel('Feature 2')\n",
        "ax1.set_title('Training Data (True Classes)')\n",
        "plt.colorbar(scatter, ax=ax1)\n",
        "\n",
        "# Plot 2: Decision boundary and predicted probabilities\n",
        "ax2 = axes[1]\n",
        "\n",
        "# Create a mesh to plot the decision boundary\n",
        "h = 0.1\n",
        "x_min, x_max = X_train['feature1'].min() - 1, X_train['feature1'].max() + 1\n",
        "y_min, y_max = X_train['feature2'].min() - 1, X_train['feature2'].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# Predict probabilities on the mesh\n",
        "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "mesh_points_sm = sm.add_constant(mesh_points)\n",
        "Z = results.predict(mesh_points_sm)\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary and probability contours\n",
        "contour = ax2.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
        "ax2.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2, linestyles='--')\n",
        "\n",
        "# Plot training points\n",
        "ax2.scatter(X_train['feature1'], X_train['feature2'],\n",
        "           c=y_train, cmap='RdYlBu', edgecolors='black', s=50)\n",
        "ax2.set_xlabel('Feature 1')\n",
        "ax2.set_ylabel('Feature 2')\n",
        "ax2.set_title('Decision Boundary and Probability Contours')\n",
        "plt.colorbar(contour, ax=ax2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "mlai.write_figure(\"logistic-regression-classification-statsmodels.svg\", directory=\"./statistics\")"
      ],
      "id": "AfIhevrkgd4g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST34hHIvgd4g"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//statistics/logistic-regression-classification-statsmodels.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Plot of the logistic regression fit made by\n",
        "`statsmodels`.</i>\n",
        "\n",
        "Looking at our classification results, we can evaluate several aspects\n",
        "of model performance:\n",
        "\n",
        "**Decision Boundary Analysis:** The visualization shows how our logistic\n",
        "regression model creates a linear decision boundary in the feature\n",
        "space. The dashed black line represents the 0.5 probability threshold\n",
        "where the model switches between predicting class 0 and class 1. The\n",
        "colored contours show the predicted probability landscape - areas closer\n",
        "to red have higher probability of being class 1, while areas closer to\n",
        "blue have higher probability of being class 0.\n",
        "\n",
        "**Model Performance:** From the classification metrics, we can assess: -\n",
        "**Accuracy**: Overall percentage of correct predictions on the test set\n",
        "- **Confusion Matrix**: Shows the breakdown of true positives, false\n",
        "positives, true negatives, and false negatives - **Precision and\n",
        "Recall**: Important when we care about specific types of errors (e.g.,\n",
        "medical diagnosis)\n",
        "\n",
        "**Potential Issues to Monitor:** 1. **Feature Scaling**: If features\n",
        "have very different scales, consider standardization 2. **Linear\n",
        "Separability**: Our model assumes a linear decision boundary - if\n",
        "classes aren’t linearly separable, consider polynomial features or\n",
        "non-linear methods 3. **Class Imbalance**: If one class dominates,\n",
        "consider resampling techniques or adjusting the decision threshold 4.\n",
        "**Overfitting**: Monitor performance on validation data, especially with\n",
        "many features\n",
        "\n",
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/logistic-regression-classification-statsmodels.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Logistic regression classification results showing training\n",
        "data and decision boundary with probability contours using\n",
        "`statsmodels`.</i>\n",
        "\n",
        "The classification visualization reveals several important aspects of\n",
        "our logistic regression model:\n",
        "\n",
        "**Linear Decision Boundary:** The model creates a straight-line decision\n",
        "boundary (shown as the dashed line at 0.5 probability). This linear\n",
        "separator works well when classes are roughly linearly separable, but\n",
        "may struggle with more complex class distributions.\n",
        "\n",
        "**Probability Gradients:** The colored contours show how predicted\n",
        "probabilities change smoothly across the feature space. Points far from\n",
        "the decision boundary have probabilities close to 0 or 1 (high\n",
        "confidence), while points near the boundary have probabilities around\n",
        "0.5 (uncertain predictions).\n",
        "\n",
        "**Model Extensions:** For more complex classification problems, we can\n",
        "enhance the basic logistic regression model: - **Polynomial Features**:\n",
        "Add $x_1^2$, $x_2^2$, $x_1 x_2$ terms for non-linear decision boundaries\n",
        "- **Feature Interactions**: Include products of features to capture\n",
        "synergistic effects - **Regularization**: Add L1 (Lasso) or L2 (Ridge)\n",
        "penalties to prevent overfitting - **Feature Engineering**: Transform or\n",
        "combine features to better capture relationships\n",
        "\n",
        "To incorporate multiple features and transformations into our model, we\n",
        "need a systematic way to organize this information through the design\n",
        "matrix."
      ],
      "id": "ST34hHIvgd4g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtEb8pvtgd4g"
      },
      "source": [
        "## Categorical (Multinomial) Regression\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/categorical-regression-intro.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/categorical-regression-intro.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "This is the multi-class analogue of logistic regression. Instead of a\n",
        "single Bernoulli outcome, we model a $K$-way categorical outcome using a\n",
        "softmax link over class-specific linear scores."
      ],
      "id": "jtEb8pvtgd4g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfN4U6Bcgd4g"
      },
      "source": [
        "## Soft Arg Max (Softmax) Function\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/soft-arg-max-function.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/soft-arg-max-function.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Given class scores\n",
        "$f_k(\\mathbf{ x})=\\mathbf{ w}_k^\\top\\boldsymbol{ \\phi}(\\mathbf{ x})$,\n",
        "the softmax maps scores to probabilities:\n",
        "$\\pi_k = \\frac{\\exp(f_k)}{\\sum_{j=1}^K \\exp(f_j)}$."
      ],
      "id": "rfN4U6Bcgd4g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4jdCwwjgd4g"
      },
      "source": [
        "## Prediction Function\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/categorical-regression-prediction-function.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/categorical-regression-prediction-function.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "For class $k\\in\\{1,\\dots,K\\}$, define a linear score\n",
        "$f_k(\\mathbf{ x})=\\mathbf{ w}_k^\\top\\boldsymbol{ \\phi}(\\mathbf{ x})$.\n",
        "The predictive probabilities are given by the softmax: $$\n",
        "\\pi_k(\\mathbf{ x}) = \\frac{\\exp\\left(f_k(\\mathbf{ x})\\right)}{\\sum_{j=1}^K \\exp\\left(f_j(\\mathbf{ x})\\right)}.\n",
        "$$"
      ],
      "id": "i4jdCwwjgd4g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoGGiuL7gd4g"
      },
      "source": [
        "## Maximum Likelihood\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/categorical-regression-maximum-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/categorical-regression-maximum-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "With one-hot targets $\\mathbf{ y}_i\\in\\{0,1\\}^K$ and softmax\n",
        "probabilities $\\pi_{ik}$, the (negative) log-likelihood is $$\n",
        "E(\\{\\mathbf{ w}_k\\}) = -\\sum_{i=1}^n\\sum_{k=1}^K y_{ik}\\log \\pi_{ik}.\n",
        "$$"
      ],
      "id": "HoGGiuL7gd4g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgSrrNMtgd4h"
      },
      "source": [
        "## Other GLMs\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_statistics/includes/other-glms-statsmodels.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_statistics/includes/other-glms-statsmodels.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We’ve introduced the formalism for generalised linear models. Have a\n",
        "think about how you might model count data using the [Poisson\n",
        "distribution](http://en.wikipedia.org/wiki/Poisson_distribution) and a\n",
        "log link function for the rate, $\\lambda(\\mathbf{ x})$. If you want a\n",
        "data set you can try the `pods.datasets.google_trends()` for some count\n",
        "data."
      ],
      "id": "kgSrrNMtgd4h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLaJ5kTagd4h"
      },
      "source": [
        "## Other GLMs\n",
        "\n",
        "We’ve introduced the formalism for generalised linear models. Have a\n",
        "think about how you might model count data using the [Poisson\n",
        "distribution](http://en.wikipedia.org/wiki/Poisson_distribution) and a\n",
        "log link function for the rate, $\\lambda(\\mathbf{ x})$. If you want a\n",
        "data set you can try the `pods.datasets.google_trends()` for some count\n",
        "data."
      ],
      "id": "SLaJ5kTagd4h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIG3_uu6gd4h"
      },
      "source": [
        "## Poisson Distribution\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/poisson-distribution.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/poisson-distribution.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
      ],
      "id": "JIG3_uu6gd4h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXthHpCMgd4h"
      },
      "outputs": [],
      "source": [
        "import mlai.plot as plot"
      ],
      "id": "CXthHpCMgd4h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWMLU54Vgd4h"
      },
      "outputs": [],
      "source": [
        "plot.poisson('./ml/')"
      ],
      "id": "KWMLU54Vgd4h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci5C3VBfgd4h"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/poisson.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The Poisson distribution.</i>"
      ],
      "id": "Ci5C3VBfgd4h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRrxq3-Tgd4h"
      },
      "source": [
        "## Poisson Regression\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/poisson-regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/poisson-regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Poisson regression is a type of generalized linear model (GLM) used when\n",
        "modeling count data. It assumes the response variable follows a Poisson\n",
        "distribution and uses a logarithmic link function to relate the mean of\n",
        "the response to the linear predictor.\n",
        "\n",
        "In this model, we make the rate parameter λ a function of covariates\n",
        "(like space or time). The logarithm of the rate is modeled as a linear\n",
        "combination of the input features:\n",
        "\n",
        "$$\\log \\lambda(\\mathbf{ x}, t) = \\mathbf{ w}_x^\\top \\boldsymbol{ \\phi}_x(\\mathbf{ x}) + \\mathbf{ w}_t^\\top \\boldsymbol{ \\phi}_t(t)$$\n",
        "\n",
        "where: - $\\mathbf{ w}_x$ and $\\mathbf{ w}_t$ are parameter vectors -\n",
        "$\\boldsymbol{ \\phi}_x(\\mathbf{ x})$ and $\\boldsymbol{ \\phi}_t(t)$ are\n",
        "basis functions for space and time respectively\n",
        "\n",
        "This formulation is known as a log-linear or log-additive model because\n",
        "we’re adding terms in the log space. The logarithm serves as our link\n",
        "function, connecting the linear predictor to the response variable’s\n",
        "mean.\n",
        "\n",
        "An important characteristic of this model that practitioners should be\n",
        "aware of is that while we add terms in the log space, the model becomes\n",
        "multiplicative when we transform back to the original space. This\n",
        "happens because:\n",
        "\n",
        "1.  We start with the log-additive form:\n",
        "    $\\log \\lambda(\\mathbf{ x}, t) = f_x(\\mathbf{ x}) + f_t(t)$\n",
        "\n",
        "2.  When we exponentiate both sides to get back to λ, the addition in\n",
        "    log space becomes multiplication:\n",
        "    $$\\lambda(\\mathbf{ x}, t) = \\exp(f_x(\\mathbf{ x}) + f_t(t)) = \\exp(f_x(\\mathbf{ x}))\\exp(f_t(t))$$\n",
        "\n",
        "This multiplicative nature has important implications for\n",
        "interpretation. For example, if we increase one input variable, it has a\n",
        "multiplicative effect on the rate, not an additive one. This can lead to\n",
        "rapid growth in the predicted counts as input values increase.\n",
        "\n",
        "Let’s look at another example using synthetic data to demonstrate\n",
        "Poisson regression without relying on external APIs."
      ],
      "id": "gRrxq3-Tgd4h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18oq0upegd4i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm"
      ],
      "id": "18oq0upegd4i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCdLKbtkgd4i"
      },
      "outputs": [],
      "source": [
        "# Generate some example count data\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "x1 = np.random.uniform(0, 10, n_samples)\n",
        "x2 = np.random.uniform(0, 5, n_samples)\n",
        "X = np.column_stack((x1, x2))\n",
        "\n",
        "# True relationship: y ~ Poisson(exp(1 + 0.3*x1 - 0.2*x2))\n",
        "lambda_true = np.exp(1 + 0.3*x1 - 0.2*x2)\n",
        "y = np.random.poisson(lambda_true)\n",
        "\n",
        "# Fit Poisson regression\n",
        "model_synthetic = sm.GLM(y, sm.add_constant(X), family=sm.families.Poisson())\n",
        "result_synthetic = model_synthetic.fit()"
      ],
      "id": "zCdLKbtkgd4i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIgKO15ygd4i"
      },
      "source": [
        "The `statsmodels` library in Python provides a convenient way to fit\n",
        "Poisson regression models. The `sm.GLM` function is used to fit\n",
        "generalized linear models, and we specify the Poisson family to indicate\n",
        "that we’re modeling count data. The `sm.add_constant(x)` function adds a\n",
        "column of ones to the design matrix to account for the intercept term.\n",
        "\n",
        "In this synthetic example, we generate count data that follows a Poisson\n",
        "distribution where the rate parameter $\\lambda$ depends on two predictor\n",
        "variables. This demonstrates how Poisson regression can model count data\n",
        "with multiple predictors."
      ],
      "id": "rIgKO15ygd4i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emwOWS1ogd4i"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot actual vs predicted counts\n",
        "y_pred = result_synthetic.predict(sm.add_constant(X))\n",
        "ax1.scatter(y, y_pred, alpha=0.5)\n",
        "ax1.plot([0, max(y)], [0, max(y)], 'r--')\n",
        "ax1.set_xlabel('Actual Counts')\n",
        "ax1.set_ylabel('Predicted Counts')\n",
        "ax1.set_title('Actual vs Predicted')\n",
        "\n",
        "# Plot residuals\n",
        "residuals = y - y_pred\n",
        "ax2.scatter(y_pred, residuals, alpha=0.5)\n",
        "ax2.axhline(y=0, color='r', linestyle='--')\n",
        "ax2.set_xlabel('Predicted Counts')\n",
        "ax2.set_ylabel('Residuals')\n",
        "ax2.set_title('Residual Plot')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "mlai.write_figure(\"poisson-regression-diagnostics.svg\", directory=\"./ml/\")"
      ],
      "id": "emwOWS1ogd4i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIparP2Ogd4i"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/poisson-regression-diagnostics.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Diagnostic plots for the Poisson regression model showing\n",
        "actual vs predicted counts and residual analysis.</i>"
      ],
      "id": "gIparP2Ogd4i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJFtamphgd4j"
      },
      "source": [
        "## Practical Tips\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_statistics/includes/glm-practical-tips.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_statistics/includes/glm-practical-tips.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "When working with generalised linear models in practice, there are\n",
        "several key considerations that can significantly impact model\n",
        "performance:\n",
        "\n",
        "Feature engineering is often the most critical factor in model success:\n",
        "\n",
        "-   Build modular data processing pipelines that allow you to easily\n",
        "    test different feature sets. For example, if modeling house prices,\n",
        "    you might want to test combinations of raw features (square footage,\n",
        "    bedrooms), derived features (price per square foot), and interaction\n",
        "    terms (bedrooms × bathrooms).\n",
        "-   Consider non-linear transformations of continuous variables. For\n",
        "    instance, taking the log of price data often helps normalize\n",
        "    distributions.\n",
        "-   Be thoughtful about encoding categorical variables - one-hot\n",
        "    encoding isn’t always optimal. For high-cardinality categories,\n",
        "    consider target encoding or feature hashing.\n",
        "-   Scale features appropriately - standardization or min-max scaling\n",
        "    depending on your model assumptions.\n",
        "-   Document your feature creation process thoroughly, including the\n",
        "    rationale for each transformation.\n",
        "\n",
        "Model validation requires careful consideration:\n",
        "\n",
        "-   Cross-validation should match your real-world use case. For time\n",
        "    series data, use time-based splits rather than random splits.\n",
        "-   Bootstrap sampling helps understand parameter uncertainty. For\n",
        "    example, bootstrapping can show if a coefficient’s sign might flip\n",
        "    under different samples.\n",
        "-   Hold-out test sets should be truly independent. In a customer churn\n",
        "    model, this might mean testing on future customers rather than a\n",
        "    random subset.\n",
        "-   Watch for data leakage, especially with time-dependent features. If\n",
        "    predicting customer churn, using future purchase data would create\n",
        "    leakage.\n",
        "\n",
        "Diagnostic checks are essential for model reliability:\n",
        "\n",
        "-   Create residual plots against fitted values and each predictor. Look\n",
        "    for systematic patterns - a U-shaped residual plot suggests missing\n",
        "    quadratic terms.\n",
        "-   For logistic regression, plot predicted probabilities against actual\n",
        "    outcomes in bins to check calibration.\n",
        "-   Calculate influence measures like Cook’s distance to identify\n",
        "    outliers. In a house price model, a mansion might have outsized\n",
        "    influence on coefficients.\n",
        "-   Check Variance Inflation Factors (VIF) for multicollinearity. High\n",
        "    VIF (\\>5-10) suggests problematic correlation between predictors.\n",
        "\n",
        "Visualization remains crucial throughout:\n",
        "\n",
        "-   Before modeling, create scatter plots, box plots, and histograms to\n",
        "    understand your data distribution and relationships.\n",
        "-   Use pairs plots to identify correlations and potential interactions\n",
        "    between features.\n",
        "-   Create residual diagnostic plots including Q-Q plots for normality\n",
        "    checking.\n",
        "-   When communicating results, focus on interpretable visualizations.\n",
        "    For instance, partial dependence plots can show how predictions\n",
        "    change with a single feature.\n",
        "\n",
        "Additional practical considerations:\n",
        "\n",
        "-   Start simple and add complexity incrementally. A basic linear model\n",
        "    often provides a good baseline.\n",
        "-   Keep track of model performance metrics across iterations to ensure\n",
        "    changes actually improve results.\n",
        "-   Consider the computational cost of feature engineering - some\n",
        "    transformations might not be feasible in production.\n",
        "-   Think about how features will be available in production. If a\n",
        "    feature requires complex processing or external data, it might not\n",
        "    be practical.\n",
        "-   For categorical variables with many levels, consider grouping rare\n",
        "    categories.\n",
        "-   When dealing with missing data, document your imputation strategy\n",
        "    and test its impact on model performance."
      ],
      "id": "WJFtamphgd4j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-AELalsgd4j"
      },
      "source": [
        "## Further Reading\n",
        "\n",
        "-   Section 5.2.2 up to pg 182 of Rogers and Girolami (2011)"
      ],
      "id": "N-AELalsgd4j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcbKSyN7gd4j"
      },
      "source": [
        "## Thanks!\n",
        "\n",
        "For more information on these subjects and more you might want to check\n",
        "the following resources.\n",
        "\n",
        "-   company: [Trent AI](https://trent.ai)\n",
        "-   book: [The Atomic\n",
        "    Human](https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248)\n",
        "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
        "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
        "-   newspaper: [Guardian Profile\n",
        "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
        "-   blog:\n",
        "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
      ],
      "id": "gcbKSyN7gd4j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea-EtbgAgd4k"
      },
      "source": [
        "::: {.cell .markdown}\n",
        "\n",
        "## References\n",
        "\n",
        "Robbins, H., Monro, S., 1951. A stochastic approximation method. Annals\n",
        "of Mathematical Statistics 22, 400–407.\n",
        "\n",
        "Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC\n",
        "Press."
      ],
      "id": "ea-EtbgAgd4k"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    }
  }
}