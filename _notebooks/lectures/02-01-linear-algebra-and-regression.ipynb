{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq6sOcWEGm4_"
      },
      "source": [
        "# Linear Algebra and Regression\n",
        "\n",
        "### Neil Lawrence\n",
        "\n",
        "### 2025-09-08"
      ],
      "id": "tq6sOcWEGm4_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8ipzzAzGm5D"
      },
      "source": [
        "**Abstract**: In this session we combine the objective function\n",
        "perspective and the probabilistic perspective on *linear regression*. We\n",
        "motivate the importance of *linear algebra* by showing how much faster\n",
        "we can complete a linear regression using linear algebra."
      ],
      "id": "J8ipzzAzGm5D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37sxAio_Gm5E"
      },
      "source": [
        "$$\n",
        "$$"
      ],
      "id": "37sxAio_Gm5E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URj9pKG8Gm5G"
      },
      "source": [
        "<!-- Do not edit this file locally. -->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!---->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
        "<!--\n",
        "\n",
        "-->"
      ],
      "id": "URj9pKG8Gm5G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCZJ1BIqGm5H"
      },
      "source": [
        "## Review\n",
        "\n",
        "-   Last time: Reviewed Objective Functions and gradient descent."
      ],
      "id": "UCZJ1BIqGm5H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAo2p01fGm5I"
      },
      "source": [
        "## ML Foundations Course Notebook Setup\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_mlfc/includes/mlfc-notebook-setup.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_mlfc/includes/mlfc-notebook-setup.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We install some bespoke codes for creating and saving plots as well as\n",
        "loading data sets."
      ],
      "id": "cAo2p01fGm5I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mY8vCkjGm5J"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install notutils\n",
        "%pip install pods\n",
        "%pip install mlai"
      ],
      "id": "2mY8vCkjGm5J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m06qLbptGm5L"
      },
      "outputs": [],
      "source": [
        "import notutils\n",
        "import pods\n",
        "import mlai\n",
        "import mlai.plot as plot"
      ],
      "id": "m06qLbptGm5L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NPiXOvtGm5L"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 22})"
      ],
      "id": "-NPiXOvtGm5L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYIKfsu9Gm5M"
      },
      "source": [
        "<!--setupplotcode{import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_context('paper')\n",
        "sns.set_palette('colorblind')}-->"
      ],
      "id": "jYIKfsu9Gm5M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JooMI3fZGm5N"
      },
      "source": [
        "## Regression Examples\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-examples.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-examples.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Regression involves predicting a real value, $y_i$, given an input\n",
        "vector, $\\mathbf{ x}_i$. For example, the Tecator data involves\n",
        "predicting the quality of meat given spectral measurements. Or in\n",
        "radiocarbon dating, the C14 calibration curve maps from radiocarbon age\n",
        "to age measured through a back-trace of tree rings. Regression has also\n",
        "been used to predict the quality of board game moves given expert rated\n",
        "training data.\n",
        "\n",
        "In the [lecture on\n",
        "probability](https://mlatcl.github.io/mlfc/lectures/01-01-probability.html)\n",
        "we explored how Laplace proposed that we should introduce a latent slack\n",
        "variable for dealing with model mismatch."
      ],
      "id": "JooMI3fZGm5N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vipHfI7-Gm5N"
      },
      "source": [
        "## Latent Variables\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/laplace-latent-variable-solution.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/laplace-latent-variable-solution.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Laplace’s concept was that the reason that the data doesn’t match up to\n",
        "the model is because of unconsidered factors, and that these might be\n",
        "well represented through probability densities. He tackles the challenge\n",
        "of the unknown factors by adding a variable, $\\epsilon$, that represents\n",
        "the unknown. In modern parlance we would call this a *latent* variable.\n",
        "But in the context Laplace uses it, the variable is so common that it\n",
        "has other names such as a “slack” variable or the *noise* in the system.\n",
        "\n",
        "point 1: $x= 1$, $y=3$ $$\n",
        "3 = m + c + \\epsilon_1\n",
        "$$ point 2: $x= 3$, $y=1$ $$\n",
        "1 = 3m + c + \\epsilon_2\n",
        "$$ point 3: $x= 2$, $y=2.5$ $$\n",
        "2.5 = 2m + c + \\epsilon_3\n",
        "$$\n",
        "\n",
        "Laplace’s trick has converted the *overdetermined* system into an\n",
        "*underdetermined* system. He has now added three variables,\n",
        "$\\{\\epsilon_i\\}_{i=1}^3$, which represent the unknown corruptions of the\n",
        "real world. Laplace’s idea is that we should represent that unknown\n",
        "corruption with a *probability distribution*."
      ],
      "id": "vipHfI7-Gm5N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so6Uzgr6Gm5O"
      },
      "source": [
        "## A Probabilistic Process\n",
        "\n",
        "However, it was left to an admirer of Laplace to develop a practical\n",
        "probability density for that purpose. It was Carl Friedrich Gauss who\n",
        "suggested that the *Gaussian* density (which at the time was unnamed!)\n",
        "should be used to represent this error.\n",
        "\n",
        "The result is a *noisy* function, a function which has a deterministic\n",
        "part, and a stochastic part. This type of function is sometimes known as\n",
        "a probabilistic or stochastic process, to distinguish it from a\n",
        "deterministic process.\n",
        "\n",
        "As we saw, Gauss used his understanding to predict where the dwarf\n",
        "planet Ceres could be recovered. We’ve already used the least squares\n",
        "algorithm to fit linear regressions. Today we’re going to motivate least\n",
        "squares through the probabilistic framework we introduced in Lecture 1.\n",
        "\n",
        "First though, we’ll introduce a data set. Since our presentation mirrors\n",
        "that of Rogers and Girolami, we’ll follow them in looking at Olympic\n",
        "sprinting data."
      ],
      "id": "so6Uzgr6Gm5O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iezeP5d_Gm5Q"
      },
      "source": [
        "## Olympic 100m Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-100m-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-100m-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td width=\"70%\">\n",
        "\n",
        "-   Gold medal times for Olympic 100 m runners since 1896.\n",
        "-   One of a number of Olypmic data sets collected by Rogers and\n",
        "    Girolami (2011). `{=html}     </td>` `{=html}     <td width=\"30%\">`\n",
        "    <img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/100m_final_start.jpg\" style=\"width:100%\"><small>Start\n",
        "    of the 2012 London 100m race. *Image by Darren Wilkinson from\n",
        "    [Wikimedia Commons](http://bit.ly/191adDC)*</small>\n",
        "    `{=html}     </td>` `{=html}     </tr>` `{=html}     </table>`\n",
        "\n",
        "The first thing we will do is load a standard data set for regression\n",
        "modelling. The data consists of the time of Olympic Gold Medal 100m\n",
        "winners for the Olympics from 1896 to 2012. First we load in the data\n",
        "and plot."
      ],
      "id": "iezeP5d_Gm5Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zCPHIOKGm5Q"
      },
      "outputs": [],
      "source": [
        "%pip install pods"
      ],
      "id": "6zCPHIOKGm5Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFD2rzsNGm5R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pods"
      ],
      "id": "kFD2rzsNGm5R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05AmOMPZGm5R"
      },
      "outputs": [],
      "source": [
        "data = pods.datasets.olympic_100m_men()\n",
        "x = data['X']\n",
        "y = data['Y']\n",
        "\n",
        "offset = y.mean()\n",
        "scale = np.sqrt(y.var())"
      ],
      "id": "05AmOMPZGm5R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBGRMcpnGm5S"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "dBGRMcpnGm5S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtF6vs7lGm5S"
      },
      "outputs": [],
      "source": [
        "import pods\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "MtF6vs7lGm5S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJXkwIbkGm5T"
      },
      "outputs": [],
      "source": [
        "xlim = (1875,2030)\n",
        "ylim = (9, 12)\n",
        "yhat = (y-offset)/scale\n",
        "\n",
        "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "_ = ax.plot(x, y, 'r.',markersize=10)\n",
        "ax.set_xlabel('year', fontsize=20)\n",
        "ax.set_ylabel('pace min/km', fontsize=20)\n",
        "ax.set_xlim(xlim)\n",
        "ax.set_ylim(ylim)\n",
        "\n",
        "mlai.write_figure(filename='olympic-100m.svg',\n",
        "                  directory='./datasets')"
      ],
      "id": "NJXkwIbkGm5T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7RsrY3RGm5T"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//datasets/olympic-100m.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Olympic 100m wining times since 1896.</i>"
      ],
      "id": "s7RsrY3RGm5T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBFgGsODGm5U"
      },
      "source": [
        "## Sum of Squares Error\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/linear-algebra-and-regression.gpp.markdown\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/linear-algebra-and-regression.gpp.markdown', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Last week we considered a cost function for minimization of the error.\n",
        "We minimised an objective that assumed used the quadratic error\n",
        "function, $$\n",
        "E(\\mathbf{ w}) = \\sum_{i=1}^n \\left(y_i - \\mathbf{ x}_i^\\top \\mathbf{ w}\\right)^2.\n",
        "$$\n",
        "\n",
        "This week we will reinterpret the error as a *probabilistic model*. As\n",
        "Laplace suggests, we will consider the difference between our data and\n",
        "our model to have come from unconsidered factors which exhibit as a\n",
        "probability density. This leads to a more principled definition of least\n",
        "squares error due to [Carl Friederich\n",
        "Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss), but inspired\n",
        "by [Pierre-Simon\n",
        "Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)."
      ],
      "id": "eBFgGsODGm5U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMcCUeRHGm5U"
      },
      "source": [
        "## The Gaussian Density\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/univariate-gaussian.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/univariate-gaussian.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "The Gaussian density is perhaps the most commonly used probability\n",
        "density. It is defined by a *mean*, $\\mu$, and a *variance*, $\\sigma^2$.\n",
        "The variance is taken to be the square of the *standard deviation*,\n",
        "$\\sigma$.\n",
        "\n",
        "$$\\begin{align}\n",
        "  p(y| \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y- \\mu)^2}{2\\sigma^2}\\right)\\\\& \\buildrel\\triangle\\over = \\mathscr{N}\\left(y|\\mu,\\sigma^2\\right)\n",
        "  \\end{align}$$"
      ],
      "id": "JMcCUeRHGm5U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTbaEsccGm5V"
      },
      "outputs": [],
      "source": [
        "import mlai.plot as plot"
      ],
      "id": "MTbaEsccGm5V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baQGvhzoGm5V"
      },
      "outputs": [],
      "source": [
        "plot.gaussian_of_height(diagrams='./ml')"
      ],
      "id": "baQGvhzoGm5V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uflKJ5nGGm5W"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/gaussian_of_height.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The Gaussian PDF with ${\\mu}=1.7$ and variance\n",
        "${\\sigma}^2=0.0225$. Mean shown as red line. It could represent the\n",
        "heights of a population of students.</i>"
      ],
      "id": "uflKJ5nGGm5W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJAaukyvGm5W"
      },
      "source": [
        "## Two Important Gaussian Properties\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/univariate-gaussian-properties.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/univariate-gaussian-properties.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "The Gaussian density has many important properties, but for the moment\n",
        "we’ll review two of them."
      ],
      "id": "XJAaukyvGm5W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-o8BBcgGm5W"
      },
      "source": [
        "## Sum of Gaussians\n",
        "\n",
        "If we assume that a variable, $y_i$, is sampled from a Gaussian density,\n",
        "\n",
        "$$y_i \\sim \\mathscr{N}\\left(\\mu_i,\\sigma_i^2\\right)$$\n",
        "\n",
        "Then we can show that the sum of a set of variables, each drawn\n",
        "independently from such a density is also distributed as Gaussian. The\n",
        "mean of the resulting density is the sum of the means, and the variance\n",
        "is the sum of the variances,\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{n} y_i \\sim \\mathscr{N}\\left(\\sum_{i=1}^n\\mu_i,\\sum_{i=1}^n\\sigma_i^2\\right)\n",
        "$$\n",
        "\n",
        "Since we are very familiar with the Gaussian density and its properties,\n",
        "it is not immediately apparent how unusual this is. Most random\n",
        "variables, when you add them together, change the family of density they\n",
        "are drawn from. For example, the Gaussian is exceptional in this regard.\n",
        "Indeed, other random variables, if they are independently drawn and\n",
        "summed together tend to a Gaussian density. That is the [*central limit\n",
        "theorem*](https://en.wikipedia.org/wiki/Central_limit_theorem) which is\n",
        "a major justification for the use of a Gaussian density."
      ],
      "id": "H-o8BBcgGm5W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JTnZ5SyGm5X"
      },
      "source": [
        "## Scaling a Gaussian\n",
        "\n",
        "Less unusual is the *scaling* property of a Gaussian density. If a\n",
        "variable, $y$, is sampled from a Gaussian density,\n",
        "\n",
        "$$y\\sim \\mathscr{N}\\left(\\mu,\\sigma^2\\right)$$ and we choose to scale\n",
        "that variable by a *deterministic* value, $w$, then the *scaled\n",
        "variable* is distributed as\n",
        "\n",
        "$$wy\\sim \\mathscr{N}\\left(w\\mu,w^2 \\sigma^2\\right).$$ Unlike the summing\n",
        "properties, where adding two or more random variables independently\n",
        "sampled from a family of densitites typically brings the summed variable\n",
        "*outside* that family, scaling many densities leaves the distribution of\n",
        "that variable in the same *family* of densities. Indeed, many densities\n",
        "include a *scale* parameter (e.g. the [Gamma\n",
        "density](https://en.wikipedia.org/wiki/Gamma_distribution)) which is\n",
        "purely for this purpose. In the Gaussian the standard deviation,\n",
        "$\\sigma$, is the scale parameter. To see why this makes sense, let’s\n",
        "consider, $$z \\sim \\mathscr{N}\\left(0,1\\right),$$ then if we scale by\n",
        "$\\sigma$ so we have, $y=\\sigma z$, we can write,\n",
        "$$y=\\sigma z \\sim \\mathscr{N}\\left(0,\\sigma^2\\right)$$"
      ],
      "id": "0JTnZ5SyGm5X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faXUEfkjGm5X"
      },
      "source": [
        "## Laplace’s Idea\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-log-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-log-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Laplace had the idea to augment the observations by noise, that is\n",
        "equivalent to considering a probability density whose mean is given by\n",
        "the *prediction function*\n",
        "$$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\left(y_i-f\\left(x_i\\right)\\right)^{2}}{2\\sigma^2}\\right).$$\n",
        "\n",
        "This is known as *stochastic process*. It is a function that is\n",
        "corrupted by noise. Laplace didn’t suggest the Gaussian density for that\n",
        "purpose, that was an innovation from Carl Friederich Gauss, which is\n",
        "what gives the Gaussian density its name."
      ],
      "id": "faXUEfkjGm5X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq8sQ1U0Gm5X"
      },
      "source": [
        "## Height as a Function of Weight\n",
        "\n",
        "In the standard Gaussian, parameterized by mean and variance, make the\n",
        "mean a linear function of an *input*.\n",
        "\n",
        "This leads to a regression model. $$\n",
        "\\begin{align*}\n",
        "  y_i=&f\\left(x_i\\right)+\\epsilon_i,\\\\\n",
        "         \\epsilon_i \\sim & \\mathscr{N}\\left(0,\\sigma^2\\right).\n",
        "  \\end{align*}\n",
        "$$\n",
        "\n",
        "Assume $y_i$ is height and $x_i$ is weight."
      ],
      "id": "Qq8sQ1U0Gm5X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCQ60KY5Gm5Y"
      },
      "source": [
        "# Sum of Squares Error\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/sum-of-squares-log-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/sum-of-squares-log-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
      ],
      "id": "fCQ60KY5Gm5Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXzSDKcKGm5Y"
      },
      "source": [
        "## Legendre\n",
        "\n",
        "Minimising the sum of squares error was first proposed by\n",
        "[Legendre](http://en.wikipedia.org/wiki/Adrien-Marie_Legendre) in 1805\n",
        "(Legendre, 1805). His book, which was on the orbit of comets, is\n",
        "available on Google books, we can take a look at the relevant page by\n",
        "calling the code below."
      ],
      "id": "AXzSDKcKGm5Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtdASIdHGm5Y"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "nu.display_google_book(id='spcAAAAAMAAJ', page='PA72')"
      ],
      "id": "NtdASIdHGm5Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwmUtx0fGm5Z"
      },
      "source": [
        "Figure: <i>Legendre’s book was on the determination of orbits of comets.\n",
        "This page describes the formulation of least squares</i>\n",
        "\n",
        "Of course, the main text is in French, but the key part we are\n",
        "interested in can be roughly translated as\n",
        "\n",
        "> In most matters where we take measures data through observation, the\n",
        "> most accurate results they can offer, it is almost always leads to a\n",
        "> system of equations of the form $$E = a + bx + cy + fz + etc .$$ where\n",
        "> $a$, $b$, $c$, $f$ etc are the known coefficients and $x$, $y$, $z$\n",
        "> etc are unknown and must be determined by the condition that the value\n",
        "> of E is reduced, for each equation, to an amount or zero or very\n",
        "> small.\n",
        "\n",
        "He continues\n",
        "\n",
        "> Of all the principles that we can offer for this item, I think it is\n",
        "> not broader, more accurate, nor easier than the one we have used in\n",
        "> previous research application, and that is to make the minimum sum of\n",
        "> the squares of the errors. By this means, it is between the errors a\n",
        "> kind of balance that prevents extreme to prevail, is very specific to\n",
        "> make known the state of the closest to the truth system. The sum of\n",
        "> the squares of the errors\n",
        "> $E^2 + \\left.E^\\prime\\right.^2 + \\left.E^{\\prime\\prime}\\right.^2 + etc$\n",
        "> being if we wanted a minimum, by varying x alone, we will have the\n",
        "> equation …\n",
        "\n",
        "This is the earliest know printed version of the problem of least\n",
        "squares. The notation, however, is a little awkward for mordern eyes. In\n",
        "particular Legendre doesn’t make use of the sum sign, $$\n",
        "\\sum_{i=1}^3 z_i = z_1 + z_2 + z_3\n",
        "$$ nor does he make use of the inner product.\n",
        "\n",
        "In our notation, if we were to do linear regression, we would need to\n",
        "subsititue: $$\\begin{align*}\n",
        "a &\\leftarrow y_1-c, \\\\ a^\\prime &\\leftarrow y_2-c,\\\\ a^{\\prime\\prime} &\\leftarrow\n",
        "y_3 -c,\\\\\n",
        "\\text{etc.}\n",
        "\\end{align*}$$ to introduce the data observations $\\{y_i\\}_{i=1}^{n}$\n",
        "alongside $c$, the offset. We would then introduce the input locations\n",
        "$$\\begin{align*}\n",
        "b & \\leftarrow x_1,\\\\\n",
        "b^\\prime & \\leftarrow x_2,\\\\\n",
        "b^{\\prime\\prime} & \\leftarrow x_3\\\\\n",
        "\\text{etc.}\n",
        "\\end{align*}$$ and finally the gradient of the function\n",
        "$$x \\leftarrow -m.$$ The remaining coefficients ($c$ and $f$) would then\n",
        "be zero. That would give us $$\\begin{align*}   &(y_1 -\n",
        "(mx_1+c))^2 \\\\ + &(y_2 -(mx_2 + c))^2\\\\ + &(y_3 -(mx_3 + c))^2 \\\\ + & \\text{etc.}\n",
        "\\end{align*}$$ which we would write in the modern notation for sums as\n",
        "$$\n",
        "\\sum_{i=1}^n(y_i-(mx_i + c))^2\n",
        "$$ which is recognised as the sum of squares error for a linear\n",
        "regression.\n",
        "\n",
        "This shows the advantage of modern [summation\n",
        "operator](http://en.wikipedia.org/wiki/Summation), $\\sum$, in keeping\n",
        "our mathematical notation compact. Whilst it may look more complicated\n",
        "the first time you see it, understanding the mathematical rules that go\n",
        "around it, allows us to go much further with the notation.\n",
        "\n",
        "Inner products (or [dot\n",
        "products](http://en.wikipedia.org/wiki/Dot_product)) are similar. They\n",
        "allow us to write $$\n",
        "\\sum_{i=1}^q u_i v_i\n",
        "$$ in vector notation, $\\mathbf{u}^\\top\\mathbf{v}.$\n",
        "\n",
        "Here we are using bold face to represent vectors, and we assume that the\n",
        "individual elements of a vector $\\mathbf{z}$ are given as a series of\n",
        "scalars $$\n",
        "\\mathbf{z} = \\begin{bmatrix} z_1\\\\ z_2\\\\ \\vdots\\\\ z_n\n",
        "\\end{bmatrix}\n",
        "$$ which are each indexed by their position in the vector.\n",
        "\n",
        "Unfortunately for Legendre, Gauss had already used the least squares\n",
        "method in his recoveroy of Ceres. This led to a priority dispute which\n",
        "Gauss’s unpublished use being established as predating Legendre’s."
      ],
      "id": "nwmUtx0fGm5Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fANRt8tPGm5Z"
      },
      "source": [
        "## Olympic Marathon Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/olympic-marathon-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td width=\"70%\">\n",
        "\n",
        "-   Gold medal times for Olympic Marathon since 1896.\n",
        "-   Marathons before 1924 didn’t have a standardized distance.\n",
        "-   Present results using pace per km.\n",
        "-   In 1904 Marathon was badly organized leading to very slow times.\n",
        "\n",
        "</td>\n",
        "<td width=\"30%\">\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//datasets/eliud-kipchoge_berlin_2015.jpg\" style=\"width:100%\">\n",
        "<small>Image from [Wikimedia\n",
        "Commons](https://commons.wikimedia.org/wiki/File:Eliud_Kipchoge_in_Berlin_-_2015_(cropped).jpg)</small>\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "The Olympic Marathon data is a standard dataset for regression\n",
        "modelling. The data consists of the pace of Olympic Gold Medal Marathon\n",
        "winners for the Olympics from 1896 to present. Let’s load in the data\n",
        "and plot."
      ],
      "id": "fANRt8tPGm5Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBOmhy6uGm5Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pods"
      ],
      "id": "WBOmhy6uGm5Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zBJpCPjGm5a"
      },
      "outputs": [],
      "source": [
        "data = pods.datasets.olympic_marathon_men()\n",
        "x = data['X']\n",
        "y = data['Y']\n",
        "\n",
        "offset = y.mean()\n",
        "scale = np.sqrt(y.var())\n",
        "yhat = (y - offset)/scale"
      ],
      "id": "1zBJpCPjGm5a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqBY8mIKGm5a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "jqBY8mIKGm5a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-vPheOkGm5x"
      },
      "outputs": [],
      "source": [
        "xlim = (1875,2030)\n",
        "ylim = (2.5, 6.5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "_ = ax.plot(x, y, 'r.',markersize=10)\n",
        "ax.set_xlabel('year', fontsize=20)\n",
        "ax.set_ylabel('pace min/km', fontsize=20)\n",
        "ax.set_xlim(xlim)\n",
        "ax.set_ylim(ylim)\n",
        "\n",
        "mlai.write_figure(filename='olympic-marathon.svg',\n",
        "                  directory='./datasets')"
      ],
      "id": "C-vPheOkGm5x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_ai7qViGm5x"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//datasets/olympic-marathon.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Olympic marathon pace times since 1896.</i>\n",
        "\n",
        "Things to notice about the data include the outlier in 1904, in that\n",
        "year the Olympics was in St Louis, USA. Organizational problems and\n",
        "challenges with dust kicked up by the cars following the race meant that\n",
        "participants got lost, and only very few participants completed. More\n",
        "recent years see more consistently quick marathons."
      ],
      "id": "O_ai7qViGm5x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agGGMskbGm5y"
      },
      "source": [
        "## Alan Turing\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/alan-turing-marathon.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/alan-turing-marathon.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td width=\"50%\">\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//turing-times.gif\" style=\"width:100%\">\n",
        "\n",
        "</td>\n",
        "<td width=\"50%\">\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//turing-run.jpg\" style=\"width:50%\">\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Figure: <i>Alan Turing, in 1946 he was only 11 minutes slower than the\n",
        "winner of the 1948 games. Would he have won a hypothetical games held in\n",
        "1946? Source:\n",
        "<a href=\"http://www.turing.org.uk/scrapbook/run.html\" target=\"_blank\">Alan\n",
        "Turing Internet Scrapbook</a>.</i>\n",
        "\n",
        "If we had to summarise the objectives of machine learning in one word, a\n",
        "very good candidate for that word would be *generalization*. What is\n",
        "generalization? From a human perspective it might be summarised as the\n",
        "ability to take lessons learned in one domain and apply them to another\n",
        "domain. If we accept the definition given in the first session for\n",
        "machine learning, $$\n",
        "\\text{data} + \\text{model} \\stackrel{\\text{compute}}{\\rightarrow} \\text{prediction}\n",
        "$$ then we see that without a model we can’t generalise: we only have\n",
        "data. Data is fine for answering very specific questions, like “Who won\n",
        "the Olympic Marathon in 2012?” because we have that answer stored,\n",
        "however, we are not given the answer to many other questions. For\n",
        "example, Alan Turing was a formidable marathon runner, in 1946 he ran a\n",
        "time 2 hours 46 minutes (just under four minutes per kilometer, faster\n",
        "than I and most of the other [Endcliffe Park\n",
        "Run](http://www.parkrun.org.uk/sheffieldhallam/) runners can do 5 km).\n",
        "What is the probability he would have won an Olympics if one had been\n",
        "held in 1946?\n",
        "\n",
        "To answer this question we need to generalize, but before we formalize\n",
        "the concept of generalization let’s introduce some formal representation\n",
        "of what it means to generalize in machine learning."
      ],
      "id": "agGGMskbGm5y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gWYEJGVGm5z"
      },
      "source": [
        "## Running Example: Olympic Marathons\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-linear-regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-linear-regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Note that `x` and `y` are not `pandas` data frames for this example,\n",
        "they are just arrays of dimensionality $n\\times 1$, where $n$ is the\n",
        "number of data.\n",
        "\n",
        "The aim of this lab is to have you coding linear regression in python.\n",
        "We will do it in two ways, once using iterative updates (coordinate\n",
        "ascent) and then using linear algebra. The linear algebra approach will\n",
        "not only work much better, it is also easy to extend to multiple input\n",
        "linear regression and *non-linear* regression using basis functions."
      ],
      "id": "3gWYEJGVGm5z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPODtXD-Gm5z"
      },
      "source": [
        "## Maximum Likelihood: Iterative Solution\n",
        "\n",
        "Now we will take the maximum likelihood approach we derived in the\n",
        "lecture to fit a line, $y_i=mx_i + c$, to the data you’ve plotted. We\n",
        "are trying to minimize the error function, $$\n",
        "E(m, c) =  \\sum_{i=1}^n(y_i-mx_i-c)^2,\n",
        "$$ with respect to $m$, $c$ and $\\sigma^2$. We can start with an initial\n",
        "guess for $m$,"
      ],
      "id": "vPODtXD-Gm5z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGERsjt6Gm5z"
      },
      "outputs": [],
      "source": [
        "m = -0.4\n",
        "c = 80"
      ],
      "id": "GGERsjt6Gm5z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoxY10FqGm50"
      },
      "source": [
        "Then we use the maximum likelihood update to find an estimate for the\n",
        "offset, $c$."
      ],
      "id": "GoxY10FqGm50"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCI5zI55Gm50"
      },
      "source": [
        "## Coordinate Descent\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-coordinate-ascent.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-coordinate-ascent.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "In the movie recommender system example, we minimised the objective\n",
        "function by steepest descent based gradient methods. Our updates\n",
        "required us to compute the gradient at the position we were located,\n",
        "then to update the gradient according to the direction of steepest\n",
        "descent. This time, we will take another approach. It is known as\n",
        "*coordinate descent*. In coordinate descent, we choose to move one\n",
        "parameter at a time. Ideally, we design an algorithm that at each step\n",
        "moves the parameter to its minimum value. At each step we choose to move\n",
        "the individual parameter to its minimum.\n",
        "\n",
        "To find the minimum, we look for the point in the curve where the\n",
        "gradient is zero. This can be found by taking the gradient of $E(m,c)$\n",
        "with respect to the parameter."
      ],
      "id": "UCI5zI55Gm50"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0727iTHrGm51"
      },
      "source": [
        "### Update for Offset\n",
        "\n",
        "Let’s consider the parameter $c$ first. The gradient goes nicely through\n",
        "the summation operator, and we obtain $$\n",
        "\\frac{\\text{d}E(m,c)}{\\text{d}c} = -\\sum_{i=1}^n2(y_i-mx_i-c).\n",
        "$$ Now we want the point that is a minimum. A minimum is an example of a\n",
        "[*stationary point*](http://en.wikipedia.org/wiki/Stationary_point), the\n",
        "stationary points are those points of the function where the gradient is\n",
        "zero. They are found by solving the equation for\n",
        "$\\frac{\\text{d}E(m,c)}{\\text{d}c} = 0$. Substituting in to our gradient,\n",
        "we can obtain the following equation, $$\n",
        "0 = -\\sum_{i=1}^n2(y_i-mx_i-c)\n",
        "$$ which can be reorganised as follows, $$\n",
        "c^* = \\frac{\\sum_{i=1}^n(y_i-m^*x_i)}{n}.\n",
        "$$ The fact that the stationary point is easily extracted in this manner\n",
        "implies that the solution is *unique*. There is only one stationary\n",
        "point for this system. Traditionally when trying to determine the type\n",
        "of stationary point we have encountered we now compute the *second\n",
        "derivative*, $$\n",
        "\\frac{\\text{d}^2E(m,c)}{\\text{d}c^2} = 2n.\n",
        "$$ The second derivative is positive, which implies that we have found a\n",
        "minimum of the function. This means that setting $c$ using this update\n",
        "will take us to the lowest point along that axes."
      ],
      "id": "0727iTHrGm51"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bFWC_bVGm51"
      },
      "outputs": [],
      "source": [
        "# set c to the minimum\n",
        "c = (y - m*x).mean()\n",
        "print(c)"
      ],
      "id": "4bFWC_bVGm51"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYaJ4oKrGm51"
      },
      "source": [
        "## Update for Slope\n",
        "\n",
        "Now we have the offset set to the minimum value, in coordinate descent,\n",
        "the next step is to optimise another parameter. Only one further\n",
        "parameter remains. That is the slope of the system.\n",
        "\n",
        "Now we can turn our attention to the slope. We once again peform the\n",
        "same set of computations to find the minima. We end up with an update\n",
        "equation of the following form.\n",
        "\n",
        "$$m^* = \\frac{\\sum_{i=1}^n(y_i - c)x_i}{\\sum_{i=1}^nx_i^2}$$\n",
        "\n",
        "Communication of mathematics in data science is an essential skill, in a\n",
        "moment, you will be asked to rederive the equation above. Before we do\n",
        "that, however, we will briefly review how to write mathematics in the\n",
        "notebook."
      ],
      "id": "iYaJ4oKrGm51"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tX4AZ3CGm52"
      },
      "source": [
        "## $\\LaTeX$ for Maths\n",
        "\n",
        "These cells use [Markdown\n",
        "format](http://en.wikipedia.org/wiki/Markdown). You can include maths in\n",
        "your markdown using [$\\LaTeX$\n",
        "syntax](http://en.wikipedia.org/wiki/LaTeX), all you have to do is write\n",
        "your answer inside dollar signs, as follows:\n",
        "\n",
        "To write a fraction, we write `$\\frac{a}{b}$`, and it will display like\n",
        "this $\\frac{a}{b}$. To write a subscript we write `$a_b$` which will\n",
        "appear as $a_b$. To write a superscript (for example in a polynomial) we\n",
        "write `$a^b$` which will appear as $a^b$. There are lots of other macros\n",
        "as well, for example we can do greek letters such as\n",
        "`$\\alpha, \\beta, \\gamma$` rendering as $\\alpha, \\beta, \\gamma$. And we\n",
        "can do sum and intergral signs as `$\\sum \\int \\int$`.\n",
        "\n",
        "You can combine many of these operations together for composing\n",
        "expressions."
      ],
      "id": "4tX4AZ3CGm52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYEwJ1YAGm52"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Convert the following python code expressions into $\\LaTeX$j, writing\n",
        "your answers below. In each case write your answer as a single equality\n",
        "(i.e. your maths should only contain one expression, not several lines\n",
        "of expressions). For the purposes of your $\\LaTeX$ please assume that\n",
        "`x` and `w` are $n$ dimensional vectors.\n",
        "\n",
        "`(a) f = x.sum()`\n",
        "\n",
        "`(b) m = x.mean()`\n",
        "\n",
        "`(c) g = (x*w).sum()`"
      ],
      "id": "qYEwJ1YAGm52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO0qjVnUGm52"
      },
      "source": [
        "### Exercise 1 Answer\n",
        "\n",
        "Write your answer to Exercise 1 here"
      ],
      "id": "gO0qjVnUGm52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D57GIFTDGm53"
      },
      "source": [
        "## Fixed Point Updates\n",
        "\n",
        "Worked example.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    c^{*}=&\\frac{\\sum\n",
        "_{i=1}^{n}\\left(y_i-m^{*}x_i\\right)}{n},\\\\\n",
        "    m^{*}=&\\frac{\\sum\n",
        "_{i=1}^{n}x_i\\left(y_i-c^{*}\\right)}{\\sum _{i=1}^{n}x_i^{2}},\\\\\n",
        "\\left.\\sigma^2\\right.^{*}=&\\frac{\\sum\n",
        "_{i=1}^{n}\\left(y_i-m^{*}x_i-c^{*}\\right)^{2}}{n}\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "id": "D57GIFTDGm53"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crarSkvIGm53"
      },
      "source": [
        "## Gradient With Respect to the Slope\n",
        "\n",
        "Now that you’ve had a little training in writing maths with $\\LaTeX$, we\n",
        "will be able to use it to answer questions. The next thing we are going\n",
        "to do is a little differentiation practice."
      ],
      "id": "crarSkvIGm53"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hdcO5YLGm54"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Derive the the gradient of the objective function with respect to the\n",
        "slope, $m$. Rearrange it to show that the update equation written above\n",
        "does find the stationary points of the objective function. By computing\n",
        "its derivative show that it’s a minimum."
      ],
      "id": "9hdcO5YLGm54"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gChz0I0pGm54"
      },
      "source": [
        "### Exercise 2 Answer\n",
        "\n",
        "Write your answer to Exercise 2 here"
      ],
      "id": "gChz0I0pGm54"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXHId230Gm54"
      },
      "outputs": [],
      "source": [
        "m = ((y - c)*x).sum()/(x**2).sum()\n",
        "print(m)"
      ],
      "id": "lXHId230Gm54"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw4Nf8iNGm55"
      },
      "source": [
        "We can have a look at how good our fit is by computing the prediction\n",
        "across the input space. First create a vector of ‘test points,’"
      ],
      "id": "aw4Nf8iNGm55"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wYaE0NyGm55"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "1wYaE0NyGm55"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRIi8Up7Gm55"
      },
      "outputs": [],
      "source": [
        "x_test = np.linspace(1890, 2020, 130)[:, None]"
      ],
      "id": "HRIi8Up7Gm55"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmHs01_BGm56"
      },
      "source": [
        "Now use this vector to compute some test predictions,"
      ],
      "id": "SmHs01_BGm56"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tus6tgd-Gm56"
      },
      "outputs": [],
      "source": [
        "f_test = m*x_test + c"
      ],
      "id": "Tus6tgd-Gm56"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndr9d-MvGm56"
      },
      "source": [
        "Now plot those test predictions with a blue line on the same plot as the\n",
        "data,"
      ],
      "id": "ndr9d-MvGm56"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARNfNOwVGm57"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "id": "ARNfNOwVGm57"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OStte3M2Gm57"
      },
      "outputs": [],
      "source": [
        "plt.plot(x_test, f_test, 'b-')\n",
        "plt.plot(x, y, 'rx')"
      ],
      "id": "OStte3M2Gm57"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbPuIwKYGm57"
      },
      "source": [
        "The fit isn’t very good, we need to iterate between these parameter\n",
        "updates in a loop to improve the fit, we have to do this several times,"
      ],
      "id": "GbPuIwKYGm57"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df1sBsEUGm57"
      },
      "outputs": [],
      "source": [
        "for i in np.arange(10):\n",
        "    m = ((y - c)*x).sum()/(x*x).sum()\n",
        "    c = (y-m*x).sum()/y.shape[0]\n",
        "print(m)\n",
        "print(c)"
      ],
      "id": "df1sBsEUGm57"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9_rrOJIGm58"
      },
      "source": [
        "And let’s try plotting the result again"
      ],
      "id": "D9_rrOJIGm58"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obvuvk8mGm58"
      },
      "outputs": [],
      "source": [
        "f_test = m*x_test + c\n",
        "plt.plot(x_test, f_test, 'b-')\n",
        "plt.plot(x, y, 'rx')"
      ],
      "id": "obvuvk8mGm58"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cojmIG9NGm58"
      },
      "source": [
        "Clearly we need more iterations than 10! In the next question you will\n",
        "add more iterations and report on the error as optimisation proceeds."
      ],
      "id": "cojmIG9NGm58"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMUareBvGm59"
      },
      "source": [
        "## Coordiate Descent Algorithm\n",
        "\n",
        "Let’s run the complete coordiante descent algorithm and visualise how\n",
        "the parameters evolve over multiple iterations. The animation will show\n",
        "the path taken through parameter space as the algorithm navigates toward\n",
        "the minimum of the error surface."
      ],
      "id": "sMUareBvGm59"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0XUOPWfGm59"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "There is a problem here, we seem to need many interations to get to a\n",
        "good solution. Let’s explore what’s going on. Write code which\n",
        "alternates between updates of `c` and `m`. Include the following\n",
        "features in your code.\n",
        "\n",
        "1.  Initialise with `m=-0.4` and `c=80`.\n",
        "2.  Every 10 iterations compute the value of the objective function for\n",
        "    the training data and print it to the screen.\n",
        "3.  Cause the code to stop running when the error changes over less than\n",
        "    10 iterations is smaller than $1\\times 10^{-4}$. This is known as a\n",
        "    stopping criterion.\n",
        "\n",
        "Why do we need so many iterations to get to the solution?"
      ],
      "id": "_0XUOPWfGm59"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yidJ5_IcGm59"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Exercise 3 here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "yidJ5_IcGm59"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvTTU6QeGm5-"
      },
      "source": [
        "Let’s recreate our data set we used for the iterative fitting so we can\n",
        "visualise the optimisation process."
      ],
      "id": "mvTTU6QeGm5-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBt5XBMBGm5-"
      },
      "outputs": [],
      "source": [
        "m_true  = 1.4\n",
        "c_true = -3.1\n",
        "np.random.seed(42)\n",
        "x = np.random.normal(size=(4, 1))\n",
        "noise = np.random.normal(scale=0.5, size=(4, 1)) # standard deviation of the noise is 0.5\n",
        "y = m_true*x + c_true + noise"
      ],
      "id": "gBt5XBMBGm5-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeAnOGAoGm5-"
      },
      "source": [
        "## Contour Plot of Error Function\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-contour-plot.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-contour-plot.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "To understand how the least squares algorithm works, it’s helpful to\n",
        "visualize the error function as a surface in parameter space. Since we\n",
        "have two parameters ($m$ and $c$), our error function $E(m, c)$ defines\n",
        "a surface in 3D space where the height at any point represents the error\n",
        "for that combination of parameters.\n",
        "\n",
        "The global minimum of this surface is given by the optimal parameter\n",
        "values that best fit our data according to the least squares objective.\n",
        "By visualising this surface through contour plots, we can gain intuition\n",
        "about the optimization landscape and understand why gradient-based\n",
        "methods work effectively for this problem.\n",
        "\n",
        "First, we create vectors of parameter values to explore around the true\n",
        "values we used to generate the data. We sample points in a range around\n",
        "the true parameters to see how the error function behaves in the local\n",
        "neighborhood."
      ],
      "id": "ZeAnOGAoGm5-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzpVnaWIGm5_"
      },
      "outputs": [],
      "source": [
        "# create an array of linearly separated values around m_true\n",
        "m_vals = np.linspace(m_true-3, m_true+3, 100)\n",
        "# create an array of linearly separated values around c_true\n",
        "c_vals = np.linspace(c_true-3, c_true+3, 100)"
      ],
      "id": "AzpVnaWIGm5_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C42fRLi_Gm5_"
      },
      "source": [
        "Next, we create a 2D grid from these parameter vectors. This grid allows\n",
        "us to evaluate the error function at every combination of $m$ and $c$\n",
        "values, giving us a complete picture of the error surface over the\n",
        "parameter space we’re exploring."
      ],
      "id": "C42fRLi_Gm5_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG6krzu1Gm5_"
      },
      "outputs": [],
      "source": [
        "m_grid, c_grid = np.meshgrid(m_vals, c_vals)"
      ],
      "id": "BG6krzu1Gm5_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrP8K6oSGm5_"
      },
      "source": [
        "Now we compute the error function at each combination of parameters. For\n",
        "each point in our grid, we:\n",
        "\n",
        "1.  Use the parameter values to make predictions:\n",
        "    $\\hat{y}_i = m \\cdot x_i + c$  \n",
        "2.  Calculate the squared errors: $(y_i - \\hat{y}_i)^2$\n",
        "3.  Sum these squared errors to get the total error for that parameter\n",
        "    combination\n",
        "\n",
        "This gives us the complete error surface that we can then visualize. The\n",
        "nested loop structure evaluates the sum of squared errors formula at\n",
        "each $(m, c)$ coordinate in our grid."
      ],
      "id": "CrP8K6oSGm5_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q21J1TM0Gm6A"
      },
      "outputs": [],
      "source": [
        "E_grid = np.zeros((100, 100))\n",
        "for i in range(100):\n",
        "    for j in range(100):\n",
        "        E_grid[i, j] = ((y - m_grid[i, j]*x - c_grid[i, j])**2).sum()"
      ],
      "id": "q21J1TM0Gm6A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywUSXBvzGm6B"
      },
      "source": [
        "With our error surface computed, we can now create a contour plot to\n",
        "visualize the optimization landscape. A contour plot shows lines of\n",
        "equal error value, similar to elevation contours on a topographic map.\n",
        "\n",
        "Insights from this visualisation include:\n",
        "\n",
        "-   *Bowl-shaped surface*: For linear regression with least squares, the\n",
        "    error surface is a smooth, convex bowl with a unique global minimum\n",
        "-   *Contour lines*: Each contour represents parameter combinations that\n",
        "    yield the same error value\n",
        "-   *Minimum location*: The centre of the concentric ellipses shows\n",
        "    where the error is minimized - this should be close to our true\n",
        "    parameter values\n",
        "\n",
        "This visualisation helps explain why least squares regression has nice\n",
        "mathematical properties and why optimisation algorithms converge\n",
        "reliably to the solution."
      ],
      "id": "ywUSXBvzGm6B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyqNmyjgGm6B"
      },
      "outputs": [],
      "source": [
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "cyqNmyjgGm6B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFn-EgnxGm6C"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(5,5))\n",
        "plot.regression_contour(f, ax, m_vals, c_vals, E_grid)\n",
        "mlai.write_figure(filename='regression_contour.svg', directory='./ml')"
      ],
      "id": "DFn-EgnxGm6C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKN-XpyYGm6D"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/regression_contour.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Contours of the objective function for linear regression by\n",
        "minimizing least squares.</i>\n",
        "\n",
        "The contour plot reveals the characteristic elliptical shape of the\n",
        "least squares error surface. The concentric ellipses represent\n",
        "increasing levels of error as we move away from the optimal parameters.\n",
        "\n",
        "Key observations from this visualization:\n",
        "\n",
        "-   *Convex optimisation*: The smooth, bowl-shaped surface guarantees\n",
        "    that any local minimum is also the global minimum\n",
        "-   *Parameter sensitivity*: The shape of the ellipses tells us how\n",
        "    sensitive the error is to changes in each parameter\n",
        "-   *Optimization efficiency*: The regular, predictable shape means we\n",
        "    can develop optimisation methods that will converge quickly and\n",
        "    reliably\n",
        "-   *True parameter location*: The minimum should occur very close to\n",
        "    our known true values ($m_{true} = 1.4$, $c_{true} = -3.1$)\n",
        "\n",
        "*Warning:* This visualisation is great for giving some intuition, but\n",
        "can be quite misleading about how these objective funcitons look in very\n",
        "high dimensions. Unfortunately high dimensions are much harder to\n",
        "visualise."
      ],
      "id": "vKN-XpyYGm6D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnSHfXaKGm6E"
      },
      "source": [
        "## Regression Coordiate Fit\n",
        "\n",
        "Now we’ll run Each frame shows:\n",
        "\n",
        "-   *Current position*: The green star indicating our current parameter\n",
        "    estimates\n",
        "-   *Error contours*: The background showing the error landscape\n",
        "-   *Path*: The trajectory we’ve taken from the starting point to the\n",
        "    current position\n",
        "\n",
        "Watch how the algorithm follows a “staircase” path that converges\n",
        "towards the minimum. Note that the more correlated $m$ and $c$ are\n",
        "(principle axes of the ellipse contours is diagonal) the slower the\n",
        "convergence towards the minimum demonstrating the potential limitation\n",
        "of coordinate descent."
      ],
      "id": "gnSHfXaKGm6E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MuZ6ZZdGm6E"
      },
      "outputs": [],
      "source": [
        "import mlai.plot as plot"
      ],
      "id": "8MuZ6ZZdGm6E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5XaJ2CBGm6F"
      },
      "outputs": [],
      "source": [
        "num_plots = plot.regression_contour_coordinate_descent(x, y, max_iters=10, diagrams='./ml')"
      ],
      "id": "E5XaJ2CBGm6F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taxiIbihGm6F"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "taxiIbihGm6F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz5yN1Y_Gm6F"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('regression_coordinate_descent_contour_fit{num:0>3}.svg', directory='./ml', num=IntSlider(0, 0, num_plots, 1))"
      ],
      "id": "Vz5yN1Y_Gm6F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7pPbMCUGm6G"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/regression_coordinate_descent_contour_fit020.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Coordinate descent for linear regression showing the path\n",
        "after 20 updates between $c$ and $m$.</i>"
      ],
      "id": "b7pPbMCUGm6G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omjl8d3-Gm6G"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Why does the coordiate descent converge so quickly for the toy data\n",
        "example, but take so long for the Marathon data?"
      ],
      "id": "omjl8d3-Gm6G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVr3WvedGm6G"
      },
      "source": [
        "### Exercise 4 Answer\n",
        "\n",
        "Write your answer to Exercise 4 here"
      ],
      "id": "eVr3WvedGm6G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j53IT0nGm6G"
      },
      "source": [
        "## Important Concepts Not Covered\n",
        "\n",
        "-   Other optimization methods:\n",
        "    -   Second order methods, conjugate gradient, quasi-Newton and\n",
        "        Newton.\n",
        "-   Effective heuristics such as momentum.\n",
        "-   Local vs global solutions."
      ],
      "id": "4j53IT0nGm6G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hiqe5rPGm6H"
      },
      "source": [
        "# Multivariate Regression\n",
        "\n",
        "What if we’re faced with a multivariate regression. For example, we\n",
        "might try and predict height given weight and gender."
      ],
      "id": "9Hiqe5rPGm6H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvrXIYysGm6H"
      },
      "source": [
        "## Log Likelihood for Multivariate Regression\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-multivariate-log-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-multivariate-log-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
      ],
      "id": "fvrXIYysGm6H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24mvmSEDGm6H"
      },
      "source": [
        "## Quadratic Loss\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-direct-solution.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-direct-solution.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Now we’ve identified the empirical risk with the loss, we’ll use\n",
        "$E(\\mathbf{ w})$ to represent our objective function. $$\n",
        "E(\\mathbf{ w}) = \\sum_{i=1}^n\\left(y_i - f(\\mathbf{ x}_i, \\mathbf{ w})\\right)^2\n",
        "$$ gives us our objective.\n",
        "\n",
        "In the case of the linear prediction function, we can substitute\n",
        "$f(\\mathbf{ x}_i, \\mathbf{ w}) = \\mathbf{ w}^\\top \\mathbf{ x}_i$. $$\n",
        "E(\\mathbf{ w}) = \\sum_{i=1}^n\\left(y_i - \\mathbf{ w}^\\top \\mathbf{ x}_i\\right)^2\n",
        "$$ To compute the gradient of the objective, we first expand the\n",
        "brackets."
      ],
      "id": "24mvmSEDGm6H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh-YysGmGm6H"
      },
      "source": [
        "## Bracket Expansion\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  E(\\mathbf{ w},\\sigma^2)  = &\n",
        "\\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum\n",
        "_{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\\sum\n",
        "_{i=1}^{n}y_i\\mathbf{ w}^{\\top}\\mathbf{ x}_i\\\\&+\\frac{1}{2\\sigma^2}\\sum\n",
        "_{i=1}^{n}\\mathbf{ w}^{\\top}\\mathbf{ x}_i\\mathbf{ x}_i^{\\top}\\mathbf{ w}\n",
        "+\\text{const}.\\\\\n",
        "    = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum\n",
        "_{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n",
        "\\mathbf{ w}^\\top\\sum_{i=1}^{n}\\mathbf{ x}_iy_i\\\\&+\\frac{1}{2\\sigma^2}\n",
        "\\mathbf{ w}^{\\top}\\left[\\sum\n",
        "_{i=1}^{n}\\mathbf{ x}_i\\mathbf{ x}_i^{\\top}\\right]\\mathbf{ w}+\\text{const}.\n",
        "\\end{align*}\n",
        "$$"
      ],
      "id": "hh-YysGmGm6H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isjiIOxzGm6H"
      },
      "source": [
        "# Solution with Linear Algebra\n",
        "\n",
        "In this section we’re going compute the minimum of the quadratic loss\n",
        "with respect to the parameters. When we do this, we’ll also review\n",
        "*linear algebra*. We will represent all our errors and functions in the\n",
        "form of matrices and vectors.\n",
        "\n",
        "Linear algebra is just a shorthand for performing lots of\n",
        "multiplications and additions simultaneously. What does it have to do\n",
        "with our system then? Well, the first thing to note is that the classic\n",
        "linear function we fit for a one-dimensional regression has the form: $$\n",
        "f(x) = mx + c\n",
        "$$ the classical form for a straight line. From a linear algebraic\n",
        "perspective, we are looking for multiplications and additions. We are\n",
        "also looking to separate our parameters from our data. The data is the\n",
        "*givens*. In French the word is données literally translated means\n",
        "*givens* that’s great, because we don’t need to change the data, what we\n",
        "need to change are the parameters (or variables) of the model. In this\n",
        "function the data comes in through $x$, and the parameters are $m$ and\n",
        "$c$.\n",
        "\n",
        "What we’d like to create is a vector of parameters and a vector of data.\n",
        "Then we could represent the system with vectors that represent the data,\n",
        "and vectors that represent the parameters.\n",
        "\n",
        "We look to turn the multiplications and additions into a linear\n",
        "algebraic form, we have one multiplication ($m\\times c$) and one\n",
        "addition ($mx + c$). But we can turn this into an inner product by\n",
        "writing it in the following way, $$\n",
        "f(x) = m \\times x +\n",
        "c \\times 1,\n",
        "$$ in other words, we’ve extracted the unit value from the offset, $c$.\n",
        "We can think of this unit value like an extra item of data, because it\n",
        "is always given to us, and it is always set to 1 (unlike regular data,\n",
        "which is likely to vary!). We can therefore write each input data\n",
        "location, $\\mathbf{ x}$, as a vector $$\n",
        "\\mathbf{ x}= \\begin{bmatrix} 1\\\\ x\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Now we choose to also turn our parameters into a vector. The parameter\n",
        "vector will be defined to contain $$\n",
        "\\mathbf{ w}= \\begin{bmatrix} c \\\\ m\\end{bmatrix}\n",
        "$$ because if we now take the inner product between these two vectors we\n",
        "recover $$\n",
        "\\mathbf{ x}\\cdot\\mathbf{ w}= 1 \\times c + x \\times m = mx + c\n",
        "$$ In `numpy` we can define this vector as follows"
      ],
      "id": "isjiIOxzGm6H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY_HmKvVGm6I"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "HY_HmKvVGm6I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvFvQcmxGm6I"
      },
      "outputs": [],
      "source": [
        "# define the vector w\n",
        "w = np.zeros(shape=(2, 1))\n",
        "w[0] = m\n",
        "w[1] = c"
      ],
      "id": "AvFvQcmxGm6I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI1hXbuTGm6I"
      },
      "source": [
        "This gives us the equivalence between original operation and an\n",
        "operation in vector space. Whilst the notation here isn’t a lot shorter,\n",
        "the beauty is that we will be able to add as many features as we like\n",
        "and keep the same representation. In general, we are now moving to a\n",
        "system where each of our predictions is given by an inner product. When\n",
        "we want to represent a linear product in linear algebra, we tend to do\n",
        "it with the transpose operation, so since we have\n",
        "$\\mathbf{a}\\cdot\\mathbf{b} = \\mathbf{a}^\\top\\mathbf{b}$ we can write $$\n",
        "f(\\mathbf{ x}_i) = \\mathbf{ x}_i^\\top\\mathbf{ w}.\n",
        "$$ Where we’ve assumed that each data point, $\\mathbf{ x}_i$, is now\n",
        "written by appending a 1 onto the original vector $$\n",
        "\\mathbf{ x}_i = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "x_i\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "id": "bI1hXbuTGm6I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34MpKe9IGm6J"
      },
      "source": [
        "# Design Matrix\n",
        "\n",
        "We can do this for the entire data set to form a [*design\n",
        "matrix*](http://en.wikipedia.org/wiki/Design_matrix) $\\mathbf{X}$, $$\n",
        "\\mathbf{X}\n",
        "= \\begin{bmatrix}\n",
        "\\mathbf{ x}_1^\\top \\\\\\\n",
        "\\mathbf{ x}_2^\\top \\\\\\\n",
        "\\vdots \\\\\\\n",
        "\\mathbf{ x}_n^\\top\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "1 & x_1 \\\\\\\n",
        "1 & x_2 \\\\\\\n",
        "\\vdots\n",
        "& \\vdots \\\\\\\n",
        "1 & x_n\n",
        "\\end{bmatrix},\n",
        "$$ which in `numpy` can be done with the following commands:"
      ],
      "id": "34MpKe9IGm6J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0csHfw6FGm6J"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "0csHfw6FGm6J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr5vPHFwGm6J"
      },
      "outputs": [],
      "source": [
        "X = np.hstack((np.ones_like(x), x))\n",
        "print(X)"
      ],
      "id": "Cr5vPHFwGm6J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX_mmliOGm6J"
      },
      "source": [
        "## Writing the Objective with Linear Algebra\n",
        "\n",
        "When we think of the objective function, we can think of it as the\n",
        "errors where the error is defined in a similar way to what it was in\n",
        "Legendre’s day $y_i - f(\\mathbf{ x}_i)$, in statistics these errors are\n",
        "also sometimes called\n",
        "[*residuals*](http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics).\n",
        "So, we can think as the objective and the prediction function as two\n",
        "separate parts, first we have, $$\n",
        "E(\\mathbf{ w}) = \\sum_{i=1}^n(y_i - f(\\mathbf{ x}_i; \\mathbf{ w}))^2,\n",
        "$$ where we’ve made the function $f(\\cdot)$’s dependence on the\n",
        "parameters $\\mathbf{ w}$ explicit in this equation. Then we have the\n",
        "definition of the function itself, $$\n",
        "f(\\mathbf{ x}_i; \\mathbf{ w}) = \\mathbf{ x}_i^\\top \\mathbf{ w}.\n",
        "$$ Let’s look again at these two equations and see if we can identify\n",
        "any inner products. The first equation is a sum of squares, which is\n",
        "promising. Any sum of squares can be represented by an inner product, $$\n",
        "a = \\sum_{i=1}^{k} b^2_i = \\mathbf{b}^\\top\\mathbf{b}.\n",
        "$$ If we wish to represent $E(\\mathbf{ w})$ in this way, all we need to\n",
        "do is convert the sum operator to an inner product. We can get a vector\n",
        "from that sum operator by placing both $y_i$ and\n",
        "$f(\\mathbf{ x}_i; \\mathbf{ w})$ into vectors, which we do by defining $$\n",
        "\\mathbf{ y}= \\begin{bmatrix}y_1\\\\ y_2\\\\ \\vdots \\\\ y_n\\end{bmatrix}\n",
        "$$ and defining $$\n",
        "\\mathbf{ f}(\\mathbf{ x}_1; \\mathbf{ w}) = \\begin{bmatrix}f(\\mathbf{ x}_1; \\mathbf{ w})\\\\ f(\\mathbf{ x}_2; \\mathbf{ w})\\\\ \\vdots \\\\ f(\\mathbf{ x}_n; \\mathbf{ w})\\end{bmatrix}.\n",
        "$$ The second of these is a vector-valued function. This term may appear\n",
        "intimidating, but the idea is straightforward. A vector valued function\n",
        "is simply a vector whose elements are themselves defined as *functions*,\n",
        "i.e., it is a vector of functions, rather than a vector of scalars. The\n",
        "idea is so straightforward, that we are going to ignore it for the\n",
        "moment, and barely use it in the derivation. But it will reappear later\n",
        "when we introduce *basis functions*. So, we will for the moment ignore\n",
        "the dependence of $\\mathbf{ f}$ on $\\mathbf{ w}$ and $\\mathbf{X}$ and\n",
        "simply summarise it by a vector of numbers $$\n",
        "\\mathbf{ f}= \\begin{bmatrix}f_1\\\\f_2\\\\\n",
        "\\vdots \\\\ f_n\\end{bmatrix}.\n",
        "$$ This allows us to write our objective in the folowing, linear\n",
        "algebraic form, $$\n",
        "E(\\mathbf{ w}) = (\\mathbf{ y}- \\mathbf{ f})^\\top(\\mathbf{ y}- \\mathbf{ f})\n",
        "$$ from the rules of inner products. But what of our matrix $\\mathbf{X}$\n",
        "of input data? At this point, we need to dust off [*matrix-vector\n",
        "multiplication*](http://en.wikipedia.org/wiki/Matrix_multiplication).\n",
        "Matrix multiplication is simply a convenient way of performing many\n",
        "inner products together, and it’s exactly what we need to summarize the\n",
        "operation $$\n",
        "f_i = \\mathbf{ x}_i^\\top\\mathbf{ w}.\n",
        "$$ This operation tells us that each element of the vector $\\mathbf{ f}$\n",
        "(our vector valued function) is given by an inner product between\n",
        "$\\mathbf{ x}_i$ and $\\mathbf{ w}$. In other words, it is a series of\n",
        "inner products. Let’s look at the definition of matrix multiplication,\n",
        "it takes the form $$\n",
        "\\mathbf{c} = \\mathbf{B}\\mathbf{a},\n",
        "$$ where $\\mathbf{c}$ might be a $k$ dimensional vector (which we can\n",
        "interpret as a $k\\times 1$ dimensional matrix), and $\\mathbf{B}$ is a\n",
        "$k\\times k$ dimensional matrix and $\\mathbf{a}$ is a $k$ dimensional\n",
        "vector ($k\\times 1$ dimensional matrix).\n",
        "\n",
        "The result of this multiplication is of the form $$\n",
        "\\begin{bmatrix}c_1\\\\c_2 \\\\ \\vdots \\\\\n",
        "a_k\\end{bmatrix} =\n",
        "\\begin{bmatrix} b_{1,1} & b_{1, 2} & \\dots & b_{1, k} \\\\\n",
        "b_{2, 1} & b_{2, 2} & \\dots & b_{2, k} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "b_{k, 1} & b_{k, 2} & \\dots & b_{k, k} \\end{bmatrix} \\begin{bmatrix}a_1\\\\a_2 \\\\\n",
        "\\vdots\\\\ c_k\\end{bmatrix} = \\begin{bmatrix} b_{1, 1}a_1 + b_{1, 2}a_2 + \\dots +\n",
        "b_{1, k}a_k\\\\\n",
        "b_{2, 1}a_1 + b_{2, 2}a_2 + \\dots + b_{2, k}a_k \\\\\n",
        "\\vdots\\\\\n",
        "b_{k, 1}a_1 + b_{k, 2}a_2 + \\dots + b_{k, k}a_k\\end{bmatrix}.\n",
        "$$ We see that each element of the result, $\\mathbf{a}$ is simply the\n",
        "inner product between each *row* of $\\mathbf{B}$ and the vector\n",
        "$\\mathbf{c}$. Because we have defined each element of $\\mathbf{ f}$ to\n",
        "be given by the inner product between each *row* of the design matrix\n",
        "and the vector $\\mathbf{ w}$ we now can write the full operation in one\n",
        "matrix multiplication,\n",
        "\n",
        "$$\n",
        "\\mathbf{ f}= \\mathbf{X}\\mathbf{ w}.\n",
        "$$"
      ],
      "id": "vX_mmliOGm6J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPhBFi5JGm6K"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "nPhBFi5JGm6K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F-utsVMGm6K"
      },
      "outputs": [],
      "source": [
        "f = X@w # The @ sign performs matrix multiplication"
      ],
      "id": "2F-utsVMGm6K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yqJmYyLGm6K"
      },
      "source": [
        "Combining this result with our objective function, $$\n",
        "E(\\mathbf{ w}) = (\\mathbf{ y}- \\mathbf{ f})^\\top(\\mathbf{ y}- \\mathbf{ f})\n",
        "$$ we find we have defined the *model* with two equations. One equation\n",
        "tells us the form of our predictive function and how it depends on its\n",
        "parameters, the other tells us the form of our objective function."
      ],
      "id": "9yqJmYyLGm6K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYy5JDGCGm6L"
      },
      "outputs": [],
      "source": [
        "resid = (y-f)\n",
        "E = np.dot(resid.T, resid) # matrix multiplication on a single vector is equivalent to a dot product.\n",
        "print(\"Error function is:\", E)"
      ],
      "id": "dYy5JDGCGm6L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i--o4Pk0Gm6L"
      },
      "source": [
        "# Objective Optimization\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-objective-optimisation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-objective-optimisation.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Our *model* has now been defined with two equations: the prediction\n",
        "function and the objective function. Now we will use multivariate\n",
        "calculus to define an *algorithm* to fit the model. The separation\n",
        "between model and algorithm is important and is often overlooked. Our\n",
        "model contains a function that shows how it will be used for prediction,\n",
        "and a function that describes the objective function we need to optimize\n",
        "to obtain a good set of parameters.\n",
        "\n",
        "The model linear regression model we have described is still the same as\n",
        "the one we fitted above with a coordinate ascent algorithm. We have only\n",
        "played with the notation to obtain the same model in a matrix and vector\n",
        "notation. However, we will now fit this model with a different\n",
        "algorithm, one that is much faster. It is such a widely used algorithm\n",
        "that from the end user’s perspective it doesn’t even look like an\n",
        "algorithm, it just appears to be a single operation (or function).\n",
        "However, underneath the computer calls an algorithm to find the\n",
        "solution. Further, the algorithm we obtain is very widely used, and\n",
        "because of this it turns out to be highly optimized.\n",
        "\n",
        "Once again, we are going to try and find the stationary points of our\n",
        "objective by finding the *stationary points*. However, the stationary\n",
        "points of a multivariate function, are a little bit more complex to\n",
        "find. As before we need to find the point at which the gradient is zero,\n",
        "but now we need to use *multivariate calculus* to find it. This involves\n",
        "learning a few additional rules of differentiation (that allow you to do\n",
        "the derivatives of a function with respect to vector), but in the end it\n",
        "makes things quite a bit easier. We define vectorial derivatives as\n",
        "follows, $$\n",
        "\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}\\mathbf{ w}} =\n",
        "\\begin{bmatrix}\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}w_1}\\\\\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}w_2}\\end{bmatrix}.\n",
        "$$ where $\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}w_1}$ is the [partial\n",
        "derivative](http://en.wikipedia.org/wiki/Partial_derivative) of the\n",
        "error function with respect to $w_1$.\n",
        "\n",
        "Differentiation through multiplications and additions is relatively\n",
        "straightforward, and since linear algebra is just multiplication and\n",
        "addition, then its rules of differentiation are quite straightforward\n",
        "too, but slightly more complex than regular derivatives."
      ],
      "id": "i--o4Pk0Gm6L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytkwHGwzGm6M"
      },
      "source": [
        "## Multivariate Derivatives\n",
        "\n",
        "We will need two rules of multivariate or *matrix* differentiation. The\n",
        "first is differentiation of an inner product. By remembering that the\n",
        "inner product is made up of multiplication and addition, we can hope\n",
        "that its derivative is quite straightforward, and so it proves to be. We\n",
        "can start by thinking about the definition of the inner product, $$\n",
        "\\mathbf{a}^\\top\\mathbf{z} = \\sum_{i} a_i\n",
        "z_i,\n",
        "$$ which if we were to take the derivative with respect to $z_k$ would\n",
        "simply return the gradient of the one term in the sum for which the\n",
        "derivative was non-zero, that of $a_k$, so we know that $$\n",
        "\\frac{\\text{d}}{\\text{d}z_k} \\mathbf{a}^\\top \\mathbf{z} = a_k\n",
        "$$ and by our definition for multivariate derivatives, we can simply\n",
        "stack all the partial derivatives of this form in a vector to obtain the\n",
        "result that $$\n",
        "\\frac{\\text{d}}{\\text{d}\\mathbf{z}}\n",
        "\\mathbf{a}^\\top \\mathbf{z} = \\mathbf{a}.\n",
        "$$ The second rule that’s required is differentiation of a ‘matrix\n",
        "quadratic.’ A scalar quadratic in $z$ with coefficient $c$ has the form\n",
        "$cz^2$. If $\\mathbf{z}$ is a $k\\times 1$ vector and $\\mathbf{C}$ is a\n",
        "$k \\times k$ *matrix* of coefficients then the matrix quadratic form is\n",
        "written as $\\mathbf{z}^\\top \\mathbf{C}\\mathbf{z}$, which is itself a\n",
        "*scalar* quantity, but it is a function of a *vector*."
      ],
      "id": "ytkwHGwzGm6M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6T2-iU4Gm6M"
      },
      "source": [
        "### Matching Dimensions in Matrix Multiplications\n",
        "\n",
        "There’s a trick for telling a multiplication leads to a scalar result.\n",
        "When you are doing mathematics with matrices, it’s always worth pausing\n",
        "to perform a quick sanity check on the dimensions. Matrix multplication\n",
        "only works when the dimensions match. To be precise, the ‘inner’\n",
        "dimension of the matrix must match. What is the inner dimension? If we\n",
        "multiply two matrices $\\mathbf{A}$ and $\\mathbf{B}$, the first of which\n",
        "has $k$ rows and $\\ell$ columns and the second of which has $p$ rows and\n",
        "$q$ columns, then we can check whether the multiplication works by\n",
        "writing the dimensionalities next to each other, $$\n",
        "\\mathbf{A} \\mathbf{B} \\rightarrow (k \\times\n",
        "\\underbrace{\\ell)(p}_\\text{inner dimensions} \\times q) \\rightarrow (k\\times q).\n",
        "$$ The inner dimensions are the two inside dimensions, $\\ell$ and $p$.\n",
        "The multiplication will only work if $\\ell=p$. The result of the\n",
        "multiplication will then be a $k\\times q$ matrix: this dimensionality\n",
        "comes from the ‘outer dimensions.’ Note that matrix multiplication is\n",
        "not [*commutative*](http://en.wikipedia.org/wiki/Commutative_property).\n",
        "And if you change the order of the multiplication, $$\n",
        "\\mathbf{B} \\mathbf{A} \\rightarrow (\\ell \\times \\underbrace{k)(q}_\\text{inner dimensions} \\times p) \\rightarrow (\\ell \\times p).\n",
        "$$ Firstly, it may no longer even work, because now the condition is\n",
        "that $k=q$, and secondly the result could be of a different\n",
        "dimensionality. An exception is if the matrices are square matrices\n",
        "(e.g., same number of rows as columns) and they are both *symmetric*. A\n",
        "symmetric matrix is one for which $\\mathbf{A}=\\mathbf{A}^\\top$, or\n",
        "equivalently, $a_{i,j} = a_{j,i}$ for all $i$ and $j$.\n",
        "\n",
        "For applying and developing machine learning algorithms you should get\n",
        "familiar with working with matrices and vectors. You should have come\n",
        "across them before, but you may not have used them as extensively as we\n",
        "are doing now. It’s worth getting used to using this trick to check your\n",
        "work and ensure you know what the dimension of an output matrix should\n",
        "be. For our matrix quadratic form, it turns out that we can see it as a\n",
        "special type of inner product. $$\n",
        "\\mathbf{z}^\\top\\mathbf{C}\\mathbf{z} \\rightarrow (1\\times\n",
        "\\underbrace{k) (k}_\\text{inner dimensions}\\times k) (k\\times 1) \\rightarrow\n",
        "\\mathbf{b}^\\top\\mathbf{z}\n",
        "$$ where $\\mathbf{b} = \\mathbf{C}\\mathbf{z}$ so therefore the result is\n",
        "a scalar, $$\n",
        "\\mathbf{b}^\\top\\mathbf{z} \\rightarrow\n",
        "(1\\times \\underbrace{k) (k}_\\text{inner dimensions}\\times 1) \\rightarrow\n",
        "(1\\times 1)\n",
        "$$ where a $(1\\times 1)$ matrix is recognised as a scalar.\n",
        "\n",
        "This implies that we should be able to differentiate this form, and\n",
        "indeed the rule for its differentiation is slightly more complex than\n",
        "the inner product, but still quite simple, $$\n",
        "\\frac{\\text{d}}{\\text{d}\\mathbf{z}}\n",
        "\\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}= \\mathbf{C}\\mathbf{z} + \\mathbf{C}^\\top\n",
        "\\mathbf{z}.\n",
        "$$ Note that in the special case where $\\mathbf{C}$ is symmetric then we\n",
        "have $\\mathbf{C} = \\mathbf{C}^\\top$ and the derivative simplifies to $$\n",
        "\\frac{\\text{d}}{\\text{d}\\mathbf{z}} \\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}=\n",
        "2\\mathbf{C}\\mathbf{z}.\n",
        "$$"
      ],
      "id": "p6T2-iU4Gm6M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bECZk-9Gm6M"
      },
      "source": [
        "## Differentiate the Objective\n",
        "\n",
        "First, we need to compute the full objective by substituting our\n",
        "prediction function into the objective function to obtain the objective\n",
        "in terms of $\\mathbf{ w}$. Doing this we obtain $$\n",
        "E(\\mathbf{ w})= (\\mathbf{ y}- \\mathbf{X}\\mathbf{ w})^\\top (\\mathbf{ y}- \\mathbf{X}\\mathbf{ w}).\n",
        "$$ We now need to differentiate this *quadratic form* to find the\n",
        "minimum. We differentiate with respect to the *vector* $\\mathbf{ w}$.\n",
        "But before we do that, we’ll expand the brackets in the quadratic form\n",
        "to obtain a series of scalar terms. The rules for bracket expansion\n",
        "across the vectors are similar to those for the scalar system giving, $$\n",
        "(\\mathbf{a} - \\mathbf{b})^\\top\n",
        "(\\mathbf{c} - \\mathbf{d}) = \\mathbf{a}^\\top \\mathbf{c} - \\mathbf{a}^\\top\n",
        "\\mathbf{d} - \\mathbf{b}^\\top \\mathbf{c} + \\mathbf{b}^\\top \\mathbf{d}\n",
        "$$ which substituting for $\\mathbf{a} = \\mathbf{c} = \\mathbf{ y}$ and\n",
        "$\\mathbf{b}=\\mathbf{d} = \\mathbf{X}\\mathbf{ w}$ gives $$\n",
        "E(\\mathbf{ w})=\n",
        "\\mathbf{ y}^\\top\\mathbf{ y}- 2\\mathbf{ y}^\\top\\mathbf{X}\\mathbf{ w}+\n",
        "\\mathbf{ w}^\\top\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w}\n",
        "$$ where we used the fact that\n",
        "$\\mathbf{ y}^\\top\\mathbf{X}\\mathbf{ w}=\\mathbf{ w}^\\top\\mathbf{X}^\\top\\mathbf{ y}$.\n",
        "\n",
        "Now we can use our rules of differentiation to compute the derivative of\n",
        "this form, which is, $$\n",
        "\\frac{\\text{d}}{\\text{d}\\mathbf{ w}}E(\\mathbf{ w})=- 2\\mathbf{X}^\\top \\mathbf{ y}+\n",
        "2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w},\n",
        "$$ where we have exploited the fact that $\\mathbf{X}^\\top\\mathbf{X}$ is\n",
        "symmetric to obtain this result."
      ],
      "id": "0bECZk-9Gm6M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0RLQGeKGm6N"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "Use the equivalence between our vector and our matrix formulations of\n",
        "linear regression, alongside our definition of vector derivates, to\n",
        "match the gradients we’ve computed directly for\n",
        "$\\frac{\\text{d}E(c, m)}{\\text{d}c}$ and\n",
        "$\\frac{\\text{d}E(c, m)}{\\text{d}m}$ to those for\n",
        "$\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}\\mathbf{ w}}$."
      ],
      "id": "u0RLQGeKGm6N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp2O49mdGm6N"
      },
      "source": [
        "### Exercise 5 Answer\n",
        "\n",
        "Write your answer to Exercise 5 here"
      ],
      "id": "gp2O49mdGm6N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-93ol75Gm6N"
      },
      "source": [
        "# Update Equation for Global Optimum\n",
        "\n",
        "We need to find the minimum of our objective function. Using our\n",
        "objective function, we can minimize for our parameter vector\n",
        "$\\mathbf{ w}$. Firstly, we seek stationary points by find parameter\n",
        "vectors that solve for when the gradients are zero, $$\n",
        "\\mathbf{0}=- 2\\mathbf{X}^\\top\n",
        "\\mathbf{ y}+ 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w},\n",
        "$$ where $\\mathbf{0}$ is a *vector* of zeros. Rearranging this equation,\n",
        "we find the solution to be $$\n",
        "\\mathbf{X}^\\top \\mathbf{X}\\mathbf{ w}= \\mathbf{X}^\\top\n",
        "\\mathbf{ y}\n",
        "$$ which is a matrix equation of the familiar form\n",
        "$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$."
      ],
      "id": "T-93ol75Gm6N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKn5dYGKGm6N"
      },
      "source": [
        "## Solving the Multivariate System\n",
        "\n",
        "The solution for $\\mathbf{ w}$ can be written mathematically in terms of\n",
        "a matrix inverse of $\\mathbf{X}^\\top\\mathbf{X}$, but computation of a\n",
        "matrix inverse requires an algorithm to resolve it. You’ll know this if\n",
        "you had to invert, by hand, a $3\\times 3$ matrix in high school. From a\n",
        "numerical stability perspective, it is also best not to compute the\n",
        "matrix inverse directly, but rather to ask the computer to *solve* the\n",
        "system of linear equations given by $$\n",
        "\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w}= \\mathbf{X}^\\top\\mathbf{ y}\n",
        "$$ for $\\mathbf{ w}$."
      ],
      "id": "iKn5dYGKGm6N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Z_Wxv9Gm6O"
      },
      "source": [
        "## Multivariate Linear Regression\n",
        "\n",
        "A major advantage of the new system is that we can build a linear\n",
        "regression on a multivariate system. The matrix calculus didn’t specify\n",
        "what the length of the vector $\\mathbf{ x}$ should be, or equivalently\n",
        "the size of the design matrix."
      ],
      "id": "f5Z_Wxv9Gm6O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnD_0GWHGm6O"
      },
      "source": [
        "## Movie Violence Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/movie-body-count-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/movie-body-count-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "This is a data set created by Simon Garnier and Rany Olson for exploring\n",
        "the differences between R and Python for data science. The data contains\n",
        "information about different movies augmented by estimates about how many\n",
        "on-screen deaths are contained in the movie. The data is craped from\n",
        "<http://www.moviebodycounts.com>. The data contains the following\n",
        "featuers for each movie: `Year`, `Body_Count`, `MPAA_Rating`, `Genre`,\n",
        "`Director`, `Actors`, `Length_Minutes`, `IMDB_Rating`."
      ],
      "id": "EnD_0GWHGm6O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isHXoPKLGm6O"
      },
      "outputs": [],
      "source": [
        "import pods"
      ],
      "id": "isHXoPKLGm6O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVPSWxyvGm6O"
      },
      "outputs": [],
      "source": [
        "data = pods.datasets.movie_body_count()\n",
        "movies = data['Y']"
      ],
      "id": "zVPSWxyvGm6O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO5wIZ9XGm6P"
      },
      "source": [
        "The data is provided to us in the form of a pandas data frame, we can\n",
        "see the features we’re provided with by inspecting the columns of the\n",
        "data frame."
      ],
      "id": "nO5wIZ9XGm6P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7cD944nGm6P"
      },
      "outputs": [],
      "source": [
        "print(', '.join(movies.columns))"
      ],
      "id": "d7cD944nGm6P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09k4-U7PGm6P"
      },
      "source": [
        "Once it is loaded in the data can be summarized using the `describe`\n",
        "method in pandas."
      ],
      "id": "09k4-U7PGm6P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwoUCn8aGm6Q"
      },
      "source": [
        "## Multivariate Regression on Movie Violence Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/movie-body-count-linear-regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/movie-body-count-linear-regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Now we will build a design matrix based on the numeric features: year,\n",
        "Body_Count, Length_Minutes in an effort to predict the rating. We build\n",
        "the design matrix as follows:\n",
        "\n",
        "Bias as an additional feature."
      ],
      "id": "YwoUCn8aGm6Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNCBofxbGm6Q"
      },
      "outputs": [],
      "source": [
        "select_features = ['Year', 'Body_Count', 'Length_Minutes']\n",
        "X = movies[select_features].copy()\n",
        "X['Eins'] = 1 # add a column for the offset\n",
        "y = movies[['IMDB_Rating']]"
      ],
      "id": "GNCBofxbGm6Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxdS4QNYGm6R"
      },
      "source": [
        "Now let’s perform a linear regression. But this time, we will create a\n",
        "pandas data frame for the result so we can store it in a form that we\n",
        "can visualise easily."
      ],
      "id": "hxdS4QNYGm6R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdyKIxUuGm6R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "id": "GdyKIxUuGm6R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ttr2aFbnGm6S"
      },
      "outputs": [],
      "source": [
        "solution = np.linalg.solve(X.T@X, X.T@y) # solve linear regression here\n",
        "# Place the solution in the data frame\n",
        "w = pd.DataFrame(data=solution,\n",
        "                 index = X.columns,  # columns of X become rows of w\n",
        "                 columns=['regression_coefficient']) # the column of X is the value of regression coefficient"
      ],
      "id": "Ttr2aFbnGm6S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05FUsUIRGm6T"
      },
      "source": [
        "We can check the residuals to see how good our estimates are. First we\n",
        "create a pandas data frame containing the predictions and use it to\n",
        "compute the residuals."
      ],
      "id": "05FUsUIRGm6T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WusKenKGm6T"
      },
      "outputs": [],
      "source": [
        "ypred = pd.DataFrame(data=(X@w).values, columns=['IMDB_Rating'])\n",
        "resid = y-ypred"
      ],
      "id": "9WusKenKGm6T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gdQEOHAGm6T"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "-gdQEOHAGm6T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_aRDoPsGm6U"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "resid.hist(ax=ax)\n",
        "mlai.write_figure(filename='movie-body-count-rating-residuals.svg',\n",
        "                  directory='./ml')"
      ],
      "id": "y_aRDoPsGm6U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOgVl4VZGm6U"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/movie-body-count-rating-residuals.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Residual values for the ratings from the prediction of the\n",
        "movie rating given the data from the film.</i>\n",
        "\n",
        "Which shows our model *hasn’t* yet done a great job of representation,\n",
        "because the spread of values is large. We can check what the rating is\n",
        "dominated by in terms of regression coefficients."
      ],
      "id": "FOgVl4VZGm6U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6qo87vBGm6U"
      },
      "outputs": [],
      "source": [
        "w"
      ],
      "id": "u6qo87vBGm6U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSE4tnK5Gm6U"
      },
      "source": [
        "Although we have to be a little careful about interpretation because our\n",
        "input values live on different scales, however it looks like we are\n",
        "dominated by the bias, with a small negative effect for later films (but\n",
        "bear in mind the years are large, so this effect is probably larger than\n",
        "it looks) and a positive effect for length. So it looks like long\n",
        "earlier films generally do better, but the residuals are so high that we\n",
        "probably haven’t modelled the system very well."
      ],
      "id": "vSE4tnK5Gm6U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kao7dNhBGm6V"
      },
      "outputs": [],
      "source": [
        "from IPython.lib.display import YouTubeVideo\n",
        "YouTubeVideo('ui-uNlFHoms')"
      ],
      "id": "kao7dNhBGm6V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rDpRpXgGm6V"
      },
      "source": [
        "Figure: <i>MLAI Lecture 15 from 2014 on Multivariate Regression.</i>"
      ],
      "id": "4rDpRpXgGm6V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uYyDDVwGm6V"
      },
      "outputs": [],
      "source": [
        "from IPython.lib.display import YouTubeVideo\n",
        "YouTubeVideo('78YNphT90-k')"
      ],
      "id": "7uYyDDVwGm6V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oICqqw6rGm6W"
      },
      "source": [
        "Figure: <i>MLAI Lecture 3 from 2012 on Maximum Likelihood</i>"
      ],
      "id": "oICqqw6rGm6W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL4I-Zm3Gm6W"
      },
      "source": [
        "## Solution with QR Decomposition\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/qr-decomposition-regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/qr-decomposition-regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Performing a solve instead of a matrix inverse is the more numerically\n",
        "stable approach, but we can do even better.\n",
        "\n",
        "The problems can arise with very large data sets, where computing\n",
        "$\\mathbf{X}^\\top \\mathbf{X}$ can lead to a very poorly conditioned\n",
        "matrix. Instead, we can compute the solution without having to compute\n",
        "the matrix. The trick is to use a QR decomposition.\n",
        "\n",
        "A [QR-decomposition](http://en.wikipedia.org/wiki/QR_decomposition) of a\n",
        "matrix factorizes it into a matrix which is an orthogonal matrix\n",
        "$\\mathbf{Q}$, so that $\\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{I}$. And a\n",
        "matrix which is upper triangular, $\\mathbf{R}$. $$\n",
        "\\mathbf{X}^\\top \\mathbf{X}\\boldsymbol{\\beta} =\n",
        "\\mathbf{X}^\\top \\mathbf{ y}\n",
        "$$ and we substitute $\\mathbf{X}= \\mathbf{Q}\\mathbf{R}$ so we have $$\n",
        "(\\mathbf{Q}\\mathbf{R})^\\top\n",
        "(\\mathbf{Q}\\mathbf{R})\\boldsymbol{\\beta} = (\\mathbf{Q}\\mathbf{R})^\\top\n",
        "\\mathbf{ y}\n",
        "$$ $$\n",
        "\\mathbf{R}^\\top (\\mathbf{Q}^\\top \\mathbf{Q}) \\mathbf{R}\n",
        "\\boldsymbol{\\beta} = \\mathbf{R}^\\top \\mathbf{Q}^\\top \\mathbf{ y}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{R}^\\top \\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{R}^\\top \\mathbf{Q}^\\top\n",
        "\\mathbf{ y}\n",
        "$$ $$\n",
        "\\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{Q}^\\top \\mathbf{ y}\n",
        "$$ which leaves us with a lower triangular system to solve.\n",
        "\n",
        "This is a more numerically stable solution because it removes the need\n",
        "to compute $\\mathbf{X}^\\top\\mathbf{X}$ as an intermediate. Computing\n",
        "$\\mathbf{X}^\\top\\mathbf{X}$ is a bad idea because it involves squaring\n",
        "all the elements of $\\mathbf{X}$ and thereby potentially reducing the\n",
        "numerical precision with which we can represent the solution. Operating\n",
        "on $\\mathbf{X}$ directly preserves the numerical precision of the model.\n",
        "\n",
        "This can be more particularly seen when we begin to work with *basis\n",
        "functions* in the next session. Some systems that can be resolved with\n",
        "the QR decomposition cannot be resolved by using solve directly."
      ],
      "id": "xL4I-Zm3Gm6W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNwCIT90Gm6W"
      },
      "outputs": [],
      "source": [
        "import scipy as sp"
      ],
      "id": "YNwCIT90Gm6W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ0s-F1oGm6W"
      },
      "outputs": [],
      "source": [
        "Q, R = np.linalg.qr(X)\n",
        "w = sp.linalg.solve_triangular(R, Q.T@y)\n",
        "w = pd.DataFrame(w, index=X.columns)\n",
        "w"
      ],
      "id": "ZZ0s-F1oGm6W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbf57L9lGm6X"
      },
      "source": [
        "### Exercise 6\n",
        "\n",
        "Ca you see any difference between the values for the coefficients you\n",
        "got using QR decomposition than for the system where you computed\n",
        "$\\mathbf{X}^\\top \\mathbf{X}$? Why is this?"
      ],
      "id": "Nbf57L9lGm6X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7-3IXvoGm6X"
      },
      "source": [
        "### Exercise 6 Answer\n",
        "\n",
        "Write your answer to Exercise 6 here"
      ],
      "id": "W7-3IXvoGm6X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3wo6iTNGm6X"
      },
      "source": [
        "# Nonlinear Regression with Linear Models"
      ],
      "id": "U3wo6iTNGm6X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6xjEDMiGm6X"
      },
      "source": [
        "## Nonlinear Regression\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/non-linear-regression-intro.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/non-linear-regression-intro.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We’ve now seen how we may perform linear regression. Now, we are going\n",
        "to consider how we can perform *non-linear* regression. However, before\n",
        "we get into the details of how to do that we first need to consider in\n",
        "what ways the regression can be non-linear. Multivariate linear\n",
        "regression allows us to build models that take many features into\n",
        "account when making our prediction. In this session we are going to\n",
        "introduce *basis functions*. The term seems complicted, but they are\n",
        "actually based on rather a simple idea. If we are doing a multivariate\n",
        "linear regression, we get extra features that *might* help us predict\n",
        "our required response varible (or target value), $y$. But what if we\n",
        "only have one input value? We can actually artificially generate more\n",
        "input values with basis functions."
      ],
      "id": "p6xjEDMiGm6X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2sKteyqGm6Y"
      },
      "source": [
        "## Non-linear in the Inputs\n",
        "\n",
        "When we refer to non-linear regression, we are normally referring to\n",
        "whether the regression is non-linear in the input space, or non-linear\n",
        "in the *covariates*. The covariates are the observations that move with\n",
        "the target (or *response*) variable. In our notation we have been using\n",
        "$\\mathbf{ x}_i$ to represent a vector of the covariates associated with\n",
        "the $i$th observation. The coresponding response variable is $y_i$. If a\n",
        "model is non-linear in the inputs, it means that there is a non-linear\n",
        "function between the inputs and the response variable. Linear functions\n",
        "are functions that only involve multiplication and addition, in other\n",
        "words they can be represented through *linear algebra*. Linear\n",
        "regression involves assuming that a function takes the form $$\n",
        "f(\\mathbf{ x}) = \\mathbf{ w}^\\top \\mathbf{ x}\n",
        "$$ where $\\mathbf{ w}$ are our regression weights. A very easy way to\n",
        "make the linear regression non-linear is to introduce non-linear\n",
        "functions. When we are introducing non-linear regression these functions\n",
        "are known as *basis functions*."
      ],
      "id": "P2sKteyqGm6Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dewC7ciwGm6Y"
      },
      "source": [
        "# Basis Functions"
      ],
      "id": "dewC7ciwGm6Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HNJ2tsyGm6Y"
      },
      "source": [
        "## Basis Functions\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-intro.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-intro.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Here’s the idea, instead of working directly on the original input\n",
        "space, $\\mathbf{ x}$, we build models in a new space,\n",
        "$\\boldsymbol{ \\phi}(\\mathbf{ x})$ where $\\boldsymbol{ \\phi}(\\cdot)$ is a\n",
        "*vector-valued* function that is defined on the space $\\mathbf{ x}$."
      ],
      "id": "-HNJ2tsyGm6Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTXfPf4SGm6Y"
      },
      "source": [
        "## Quadratic Basis\n",
        "\n",
        "Remember, that a *vector-valued function* is just a vector that contains\n",
        "functions instead of values. Here’s an example for a one dimensional\n",
        "input space, $x$, being projected to a *quadratic* basis. First we\n",
        "consider each basis function in turn, we can think of the elements of\n",
        "our vector as being indexed so that we have $$\n",
        "\\begin{align*}\n",
        "\\phi_1(x) & = 1, \\\\\n",
        "\\phi_2(x) & = x, \\\\\n",
        "\\phi_3(x) & = x^2.\n",
        "\\end{align*}\n",
        "$$ Now we can consider them together by placing them in a vector, $$\n",
        "\\boldsymbol{ \\phi}(x) = \\begin{bmatrix} 1\\\\ x\\\\ x^2\\end{bmatrix}.\n",
        "$$ For the vector-valued function, we have simply collected the\n",
        "different functions together in the same vector making them notationally\n",
        "easier to deal with in our mathematics.\n",
        "\n",
        "When we consider the vector-valued function for each data point, then we\n",
        "place all the data into a matrix. The result is a matrix valued\n",
        "function, $$\n",
        "\\boldsymbol{ \\Phi}(\\mathbf{ x}) =\n",
        "\\begin{bmatrix} 1 & x_1 &\n",
        "x_1^2 \\\\\n",
        "1 & x_2 & x_2^2\\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "1 & x_n& x_n^2\n",
        "\\end{bmatrix}\n",
        "$$ where we are still in the one dimensional input setting so\n",
        "$\\mathbf{ x}$ here represents a vector of our inputs with $n$ elements.\n",
        "\n",
        "Let’s try constructing such a matrix for a set of inputs. First of all,\n",
        "we create a function that returns the matrix valued function."
      ],
      "id": "MTXfPf4SGm6Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vxt_DE1Gm6Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "4Vxt_DE1Gm6Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH59KWl_Gm6Z"
      },
      "outputs": [],
      "source": [
        "def quadratic(x, **kwargs):\n",
        "    \"\"\"Take in a vector of input values and return the design matrix associated\n",
        "    with the basis functions.\"\"\"\n",
        "    return np.hstack([np.ones((x.shape[0], 1)), x, x**2])"
      ],
      "id": "MH59KWl_Gm6Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsriQIlCGm6Z"
      },
      "source": [
        "## Functions Derived from Quadratic Basis\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/quadratic-basis.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/quadratic-basis.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "$$\n",
        "f(x) = {\\color{red}{w_0}} + {\\color{magenta}{w_1 x}} + {\\color{blue}{w_2 x^2}}\n",
        "$$"
      ],
      "id": "gsriQIlCGm6Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZYKFXRJGm6a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot"
      ],
      "id": "YZYKFXRJGm6a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn3d929RGm6a"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "loc =[[0, 1.4,],\n",
        "      [0, -0.7],\n",
        "      [0.75, -0.2]]\n",
        "text =[r'$\\phi(x) = 1$',\n",
        "       r'$\\phi(x) = x$',\n",
        "       r'$\\phi(x) = x^2$']\n",
        "\n",
        "plot.basis(quadratic, x_min=-1.3, x_max=1.3,\n",
        "           fig=f, ax=ax, loc=loc, text=text,\n",
        "           diagrams='./ml')\n"
      ],
      "id": "Mn3d929RGm6a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyNWTPP4Gm6b"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/quadratic_basis002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The set of functions which are combined to form a *quadratic*\n",
        "basis.</i>"
      ],
      "id": "oyNWTPP4Gm6b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A14HdiyHGm6b"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "A14HdiyHGm6b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5SpcTqUGm6c"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "R5SpcTqUGm6c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_CpAgN5Gm6c"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('quadratic_basis{num_basis:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            num_basis=IntSlider(0,0,2,1))"
      ],
      "id": "T_CpAgN5Gm6c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTVi9fg_Gm6c"
      },
      "source": [
        "This function takes in an $n\\times 1$ dimensional vector and returns an\n",
        "$n\\times 3$ dimensional *design matrix* containing the basis functions.\n",
        "We can plot those basis functions against there input as follows."
      ],
      "id": "bTVi9fg_Gm6c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNhx8k9pGm6d"
      },
      "outputs": [],
      "source": [
        "# first let's generate some inputs\n",
        "n = 100\n",
        "x = np.zeros((n, 1))  # create a data set of zeros\n",
        "x[:, 0] = np.linspace(-1, 1, n) # fill it with values between -1 and 1\n",
        "\n",
        "Phi = quadratic(x)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "ax.set_ylim([-1.2, 1.2]) # set y limits to ensure basis functions show.\n",
        "ax.plot(x[:,0], Phi[:, 0], 'r-', label = r'$\\phi=1$', linewidth=3)\n",
        "ax.plot(x[:,0], Phi[:, 1], 'g-', label = r'$\\phi=x$', linewidth=3)\n",
        "ax.plot(x[:,0], Phi[:, 2], 'b-', label = r'$\\phi=x^2$', linewidth=3)\n",
        "ax.legend(loc='lower right')\n",
        "_ = ax.set_title('Quadratic Basis Functions')"
      ],
      "id": "CNhx8k9pGm6d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuXhR3-hGm6d"
      },
      "source": [
        "The actual function we observe is then made up of a sum of these\n",
        "functions. This is the reason for the name basis. The term *basis* means\n",
        "‘the underlying support or foundation for an idea, argument, or\n",
        "process,’ and in this context they form the underlying support for our\n",
        "prediction function. Our prediction function can only be composed of a\n",
        "weighted linear sum of our basis functions."
      ],
      "id": "HuXhR3-hGm6d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpYZPrz1Gm6e"
      },
      "source": [
        "## Quadratic Functions\n",
        "\n",
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/quadratic_function002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Functions constructed by weighted sum of the components of a\n",
        "quadratic basis.</i>"
      ],
      "id": "jpYZPrz1Gm6e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XYxwUByGm6e"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "0XYxwUByGm6e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zFlIaL1Gm6e"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "7zFlIaL1Gm6e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ5bC09dGm6f"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('quadratic_function{num_function:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            num_function=IntSlider(0,0,2,1))"
      ],
      "id": "KZ5bC09dGm6f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbetvzdcGm6f"
      },
      "source": [
        "## Different Bases\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-different-bases.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-different-bases.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Our choice of basis can be made based on what our beliefs about what is\n",
        "appropriate for the data. For example, the polynomial basis extends the\n",
        "quadratic basis to aribrary degree, so we might define the $j$th basis\n",
        "function associated with the model as $$\n",
        "\\phi_j(x_i) = x_i^j\n",
        "$$ which is known as the *polynomial basis*."
      ],
      "id": "gbetvzdcGm6f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ppIoGz1Gm6f"
      },
      "source": [
        "## Polynomial Basis\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/polynomial-basis.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/polynomial-basis.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "The polynomial basis combines higher order polynomials together to\n",
        "create the function. For example, the fourth order polynomial has five\n",
        "components to its basis function. $$\n",
        "\\phi_j(x) = x^j\n",
        "$$"
      ],
      "id": "0ppIoGz1Gm6f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b19X71oqGm6g"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "b19X71oqGm6g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icFIb7xRGm6g"
      },
      "outputs": [],
      "source": [
        "import mlai"
      ],
      "id": "icFIb7xRGm6g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtxeCSU4Gm6h"
      },
      "outputs": [],
      "source": [
        "%load -n mlai.polynomial"
      ],
      "id": "vtxeCSU4Gm6h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73WGBCoqGm6h"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "73WGBCoqGm6h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtpGNX8wGm6h"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "loc =[[0, 1.4,],\n",
        "      [0, -0.7],\n",
        "      [0.75, -0.2],\n",
        "     [-0.75, -0.2],\n",
        "     [-0.75, 2]]\n",
        "text =[r'$\\phi(x) = 1$',\n",
        "       r'$\\phi(x) = x$',\n",
        "       r'$\\phi(x) = x^2$',\n",
        "       r'$\\phi(x) = x^3$',\n",
        "       r'$\\phi(x) = x^4$']\n",
        "\n",
        "plot.basis(mlai.polynomial, x_min=-1.3, x_max=1.3,\n",
        "           fig=f, ax=ax, loc=loc, text=text, num_basis=5,\n",
        "           diagrams='./ml')"
      ],
      "id": "rtpGNX8wGm6h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOYuWEa_Gm6i"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/polynomial_basis004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The set of functions which are combined to form a\n",
        "*polynomial* basis.</i>"
      ],
      "id": "NOYuWEa_Gm6i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTTSZePAGm6i"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider\n",
        "import notutils as nu"
      ],
      "id": "xTTSZePAGm6i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt5_IUM9Gm6j"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('polynomial_basis{num_basis:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            num_basis=IntSlider(0,0,4,1))"
      ],
      "id": "pt5_IUM9Gm6j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJOnmpNxGm6j"
      },
      "outputs": [],
      "source": [
        "nu.display_prediction(basis=mlai.polynomial, num_basis=5)"
      ],
      "id": "vJOnmpNxGm6j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMaRRLF0Gm6k"
      },
      "source": [
        "## Functions Derived from Polynomial Basis\n",
        "\n",
        "$$\n",
        "f(x) = {\\color{red}{w_0}} + {\\color{magenta}{w_1 x}} + {\\color{blue}{w_2 x^2}} + {\\color{green}{w_3 x^3}} + {\\color{cyan}{w_4 x^4}}\n",
        "$$\n",
        "\n",
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/polynomial_function002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>A random combination of functions from the polynomial\n",
        "basis.</i>"
      ],
      "id": "XMaRRLF0Gm6k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4i2uR46Gm6l"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider\n",
        "import notutils as nu"
      ],
      "id": "W4i2uR46Gm6l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr93v4RaGm6l"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('polynomial_function{func_num:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            func_num=IntSlider(0,0,2,1))"
      ],
      "id": "Qr93v4RaGm6l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyOS2jakGm6l"
      },
      "source": [
        "To aid in understanding how a basis works, we’ve provided you with a\n",
        "small interactive tool for exploring this polynomial basis. The tool can\n",
        "be summoned with the following command."
      ],
      "id": "PyOS2jakGm6l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI_dsipIGm6m"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "yI_dsipIGm6m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8XuqnfsGm6m"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "c8XuqnfsGm6m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOmjzCLyGm6m"
      },
      "outputs": [],
      "source": [
        "nu.display_prediction(basis=mlai.polynomial, num_basis=5)"
      ],
      "id": "DOmjzCLyGm6m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGMl5dcSGm6m"
      },
      "source": [
        "Try moving the sliders around to change the weight of each basis\n",
        "function. Click the control box `display_basis` to show the underlying\n",
        "basis functions (in red). The prediction function is shown in a thick\n",
        "blue line. *Warning* the sliders aren’t presented quite in the correct\n",
        "order. `w_0` is associated with the bias, `w_1` is the linear term,\n",
        "`w_2` the quadratic and here (because we have four basis functions) we\n",
        "have `w_3` for the *cubic* term. So the subscript of the weight\n",
        "parameter is always associated with the corresponding polynomial’s\n",
        "degree."
      ],
      "id": "mGMl5dcSGm6m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z55vqvJkGm6n"
      },
      "source": [
        "### Exercise 7\n",
        "\n",
        "Try increasing the number of basis functions (thereby increasing the\n",
        "*degree* of the resulting polynomial). Describe what you see as you\n",
        "increase number of basis up to 10. Is it easy to change the function in\n",
        "intiutive ways?"
      ],
      "id": "Z55vqvJkGm6n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-stu9tBGm6n"
      },
      "source": [
        "### Exercise 7 Answer\n",
        "\n",
        "Write your answer to Exercise 7 here"
      ],
      "id": "K-stu9tBGm6n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6FEAgoyGm6n"
      },
      "outputs": [],
      "source": [
        "# Use this box for any code you need\n",
        "\n"
      ],
      "id": "V6FEAgoyGm6n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD53V6LRGm6n"
      },
      "source": [
        "## Different Basis\n",
        "\n",
        "The polynomial basis is widely used in Engineering and graphics, but it\n",
        "has some drawbacks in machine learning: outside the input region between\n",
        "-1 and 1, the values of the polynomial basis rise very quickly.\n",
        "\n",
        "Now we look at basis functions that have been used as the *activation*\n",
        "functions in neural network model."
      ],
      "id": "BD53V6LRGm6n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIxFIsOgGm6o"
      },
      "source": [
        "## Radial Basis Functions\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/radial-basis.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/radial-basis.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Another type of basis is sometimes known as a ‘radial basis’ because the\n",
        "effect basis functions are constructed on ‘centres’ and the effect of\n",
        "each basis function decreases as the radial distance from each centre\n",
        "increases.\n",
        "\n",
        "$$\n",
        "\\phi_j(x) = \\exp\\left(-\\frac{(x-\\mu_j)^2}{\\ell^2}\\right)\n",
        "$$"
      ],
      "id": "PIxFIsOgGm6o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lajpLQlbGm6o"
      },
      "outputs": [],
      "source": [
        "import mlai"
      ],
      "id": "lajpLQlbGm6o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sR0kzLVGm6o"
      },
      "outputs": [],
      "source": [
        "%load -n mlai.radial"
      ],
      "id": "4sR0kzLVGm6o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_TvcUdtGm6p"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "K_TvcUdtGm6p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aBNsIg6Gm6v"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "\n",
        "loc = [[-1.25, -0.4],\n",
        "       [0., 1.25],\n",
        "       [1.25, -0.4]]\n",
        "text = [r'$\\phi_1(x) = e^{-(x + 1)^2}$',\n",
        "        r'$\\phi_2(x) = e^{-2x^2}$',\n",
        "        r'$\\phi_3(x) = e^{-2(x-1)^2}$']\n",
        "plot.basis(mlai.radial, x_min=-2, x_max=2,\n",
        "           fig=f, ax=ax, loc=loc, text=text,\n",
        "           diagrams='./ml')"
      ],
      "id": "_aBNsIg6Gm6v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1eRqXURGm6w"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/radial_basis002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The set of functions which are combined to form the radial\n",
        "basis.</i>"
      ],
      "id": "l1eRqXURGm6w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYLGjk3nGm6x"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "AYLGjk3nGm6x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDv_woV8Gm6y"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('radial_basis{num_basis:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            num_basis=IntSlider(0,0,2,1))"
      ],
      "id": "EDv_woV8Gm6y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJEQZKf6Gm6z"
      },
      "outputs": [],
      "source": [
        "nu.display_prediction(basis=mlai.radial, num_basis=3)"
      ],
      "id": "HJEQZKf6Gm6z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_yPG3k0Gm60"
      },
      "source": [
        "## Functions Derived from Radial Basis\n",
        "\n",
        "$$\n",
        "f(x) = \\color{red}{w_1 e^{-2(x+1)^2}}  + \\color{magenta}{w_2e^{-2x^2}} + \\color{blue}{w_3 e^{-2(x-1)^2}}\n",
        "$$\n",
        "\n",
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/radial_function002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>A radial basis is made up of different locally effective\n",
        "functions centered at different points.</i>"
      ],
      "id": "p_yPG3k0Gm60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwubqSCYGm60"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "HwubqSCYGm60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hsBye8WGm60"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('radial_function{func_num:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            func_num=IntSlider(0,0,2,1))"
      ],
      "id": "0hsBye8WGm60"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxYnF6P7Gm61"
      },
      "source": [
        "## Rectified Linear Units\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/relu-basis.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/relu-basis.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "The rectified linear unit is a basis function that emerged out of the\n",
        "deep learning community. Rectified linear units are popular in the\n",
        "current generation of multilayer perceptron models, or deep networks.\n",
        "These basis functions start flat, and then become linear functions at a\n",
        "certain threshold. $$\n",
        "\\phi_j(x) = xH(v_j x+ v_0)\n",
        "$$"
      ],
      "id": "DxYnF6P7Gm61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7zlTvcIGm61"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "-7zlTvcIGm61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24mnUtbhGm61"
      },
      "outputs": [],
      "source": [
        "import mlai"
      ],
      "id": "24mnUtbhGm61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LMuwdkfGm61"
      },
      "outputs": [],
      "source": [
        "%load -n mlai.relu"
      ],
      "id": "6LMuwdkfGm61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyhOSQuQGm62"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "fyhOSQuQGm62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyf04eW_Gm62"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "loc =[[0, 1.4,],\n",
        "      [-1, -0.5],\n",
        "      [-0.33, 0.2],\n",
        "      [0.33, -0.5],\n",
        "      [1, 0.2]]\n",
        "text =[r'$\\phi(x) = 1$',\n",
        "       r'$\\phi(x) = xH(x+1.0)$',\n",
        "       r'$\\phi(x) = xH(x+0.33)$',\n",
        "       r'$\\phi(x) = xH(x-0.33)$',\n",
        "       r'$\\phi(x) = xH(x-1.0)$']\n",
        "plot.basis(mlai.relu, x_min=-2.0, x_max=2.0,\n",
        "           fig=f, ax=ax, loc=loc, text=text,\n",
        "           diagrams='./ml',\n",
        "           num_basis=5)"
      ],
      "id": "oyf04eW_Gm62"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mLEeKZbGm62"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/relu_basis004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The set of functions which are combined to form a rectified\n",
        "linear unit basis.</i>"
      ],
      "id": "4mLEeKZbGm62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkzYPt1lGm62"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "IkzYPt1lGm62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t62ZX7xGm63"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "1t62ZX7xGm63"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-zF2KlmGm63"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('relu_basis{num_basis:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            num_basis=IntSlider(0,0,4,1))"
      ],
      "id": "9-zF2KlmGm63"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxNA9VaaGm63"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "AxNA9VaaGm63"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaXlZta0Gm63"
      },
      "outputs": [],
      "source": [
        "nu.display_prediction(basis=mlai.relu, num_basis=5)"
      ],
      "id": "LaXlZta0Gm63"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr7V_G1oGm64"
      },
      "source": [
        "## Functions Derived from Relu Basis\n",
        "\n",
        "$$\n",
        "f(x) = \\color{red}{w_0}   + \\color{magenta}{w_1 xH(x+1.0) } + \\color{blue}{w_2 xH(x+0.33) } + \\color{green}{w_3 xH(x-0.33)} +  \\color{cyan}{w_4 xH(x-1.0)}\n",
        "$$\n",
        "\n",
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/relu_function002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>A rectified linear unit basis is made up of different\n",
        "rectified linear unit functions centered at different points.</i>"
      ],
      "id": "nr7V_G1oGm64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRqR05AaGm64"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "IRqR05AaGm64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdjeapLzGm64"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "YdjeapLzGm64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW2cTsURGm64"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('relu_function{func_num:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            func_num=IntSlider(0,0,2,1))"
      ],
      "id": "lW2cTsURGm64"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoEK1FnEGm65"
      },
      "source": [
        "## Hyperbolic Tangent Basis\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/hyperbolic-tangent-basis.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/hyperbolic-tangent-basis.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "The rectified linear unit is a basis function that used to be used a lot\n",
        "for neural network models. It’s related to the sigmoid function by a\n",
        "scaling. $$\n",
        "\\phi_j(x) = \\tanh(v_j x+ v_0)\n",
        "$$"
      ],
      "id": "CoEK1FnEGm65"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wefFxkRjGm65"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "wefFxkRjGm65"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlNMh78KGm65"
      },
      "outputs": [],
      "source": [
        "import mlai"
      ],
      "id": "jlNMh78KGm65"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCFVQwGjGm65"
      },
      "outputs": [],
      "source": [
        "%load -n mlai.hyperbolic_tangent"
      ],
      "id": "hCFVQwGjGm65"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybft-EKuGm66"
      },
      "source": [
        "Sigmoid or hyperbolic tangent basis was popular in the original\n",
        "generation of multilayer perceptron models, or deep networks. These\n",
        "basis functions start flat, rise and then saturate."
      ],
      "id": "Ybft-EKuGm66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGsyduogGm66"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "ZGsyduogGm66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzRJdtGUGm66"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "loc =[[0, 1.4,],\n",
        "      [-1, -0.7],\n",
        "      [-0.33, 0],\n",
        "      [0.33, -0.7],\n",
        "      [1, 0]]\n",
        "text =[r'$\\phi(x) = 1$',\n",
        "       r'$\\phi(x) = \\tanh(x+1.0)$',\n",
        "       r'$\\phi(x) = \\tanh(x+0.33)$',\n",
        "       r'$\\phi(x) = \\tanh(x-0.33)$',\n",
        "       r'$\\phi(x) = \\tanh(x-1.0)$']\n",
        "plot.basis(mlai.hyperbolic_tangent, x_min=-2.0, x_max=2.0,\n",
        "           fig=f, ax=ax, loc=loc, text=text,\n",
        "           diagrams='./ml',\n",
        "           num_basis=5)"
      ],
      "id": "WzRJdtGUGm66"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XGviGGQGm66"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/hyperbolic_tangent_basis004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The set of functions which are combined to form a *hyberbolic\n",
        "tangent* basis.</i>"
      ],
      "id": "3XGviGGQGm66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VUpnjWMGm67"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "3VUpnjWMGm67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdlnaOVQGm67"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "wdlnaOVQGm67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdjMXQVeGm67"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('hyperbolic_tangent_basis{num_basis:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            num_basis=IntSlider(0,0,4,1))"
      ],
      "id": "HdjMXQVeGm67"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhHuB1CXGm67"
      },
      "source": [
        "## Functions Derived from Tanh Basis\n",
        "\n",
        "$$\n",
        "f(x) = {\\color{red}{w_0}}   + {\\color{magenta}{w_1 \\text{tanh}\\left(x+1\\right)}}  + {\\color{blue}{w_2 \\text{tanh}\\left(x+0.33\\right)}}  + {\\color{green}{w_3 \\text{tanh}\\left(x-0.33\\right)}} + {\\color{cyan}{w_4 \\text{tanh}\\left(x-1\\right)}}\n",
        "$$\n",
        "\n",
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/hyperbolic_tangent_function002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>A hyperbolic tangent basis is made up of s-shaped basis\n",
        "functions centered at different points.</i>"
      ],
      "id": "RhHuB1CXGm67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HO9JY5bGm68"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "6HO9JY5bGm68"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaZG2-RvGm68"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "vaZG2-RvGm68"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79kd7atbGm68"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('hyperbolic_tangent_function{func_num:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            func_num=IntSlider(0,0,2,1))"
      ],
      "id": "79kd7atbGm68"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyomFL5aGm68"
      },
      "source": [
        "## Fourier Basis\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/fourier-basis.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/fourier-basis.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "[Joseph Fourier](https://en.wikipedia.org/wiki/Joseph_Fourier) suggested\n",
        "that functions could be converted to a sum of sines and cosines. A\n",
        "Fourier basis is a linear weighted sum of these functions. $$\n",
        "f(x) = w_0  + w_1 \\sin(x) + w_2 \\cos(x) + w_3 \\sin(2x) + w_4 \\cos(2x)\n",
        "$$"
      ],
      "id": "PyomFL5aGm68"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_RRD0cSGm68"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "B_RRD0cSGm68"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzJA_xGpGm69"
      },
      "outputs": [],
      "source": [
        "import mlai"
      ],
      "id": "GzJA_xGpGm69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Txww9yzNGm69"
      },
      "outputs": [],
      "source": [
        "%load -n mlai.fourier"
      ],
      "id": "Txww9yzNGm69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kryiy1Y_Gm69"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai\n",
        "import mlai.plot as plot"
      ],
      "id": "kryiy1Y_Gm69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l29EVBqvGm69"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "loc =[[0., 0.4,],\n",
        "      [0.5, 0.4],\n",
        "      [1, -0.4],\n",
        "      [1.25, 0.4],\n",
        "      [1.5, -0.4]]\n",
        "text =[r'$\\phi(x) = 1$',\n",
        "       r'$\\phi(x) = \\sin(x)$',\n",
        "       r'$\\phi(x) = \\cos(x)$',\n",
        "       r'$\\phi(x) = \\sin(2x)$',\n",
        "       r'$\\phi(x) = \\cos(2x)$']\n",
        "plot.basis(mlai.fourier, x_min=0, x_max=2,\n",
        "           fig=f, ax=ax, loc=loc, text=text,\n",
        "           diagrams='./ml',\n",
        "           num_basis=5)"
      ],
      "id": "l29EVBqvGm69"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mJcNR_UGm6-"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/fourier_basis004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The set of functions which are combined to form a *Fourier*\n",
        "basis.</i>"
      ],
      "id": "7mJcNR_UGm6-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctv5npaOGm6-"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "ctv5npaOGm6-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC20t9GCGm6-"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "qC20t9GCGm6-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bcxkM1FGm6-"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('fourier_basis{num_basis:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            num_basis=IntSlider(0,0,4,1))"
      ],
      "id": "6bcxkM1FGm6-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeRPZV7QGm6_"
      },
      "source": [
        "In this code, basis functions with an *odd* index are sine and basis\n",
        "functions with an *even* index are cosine. The first basis function\n",
        "(index 0, so cosine) has a frequency of 0 and then frequencies increase\n",
        "every time a sine and cosine are included."
      ],
      "id": "BeRPZV7QGm6_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdvosK_mGm6_"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "kdvosK_mGm6_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEGBVdAsGm6_"
      },
      "outputs": [],
      "source": [
        "nu.display_prediction(basis=mlai.fourier, num_basis=5)"
      ],
      "id": "KEGBVdAsGm6_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNjZWw3kGm6_"
      },
      "source": [
        "## Functions Derived from Fourier Basis\n",
        "\n",
        "$$\n",
        "f(x) = {\\color{red}{w_0}}  + {\\color{magenta}{w_1 \\sin(x)}} + {\\color{blue}{w_2 \\cos(x)}} + {\\color{green}{w_3 \\sin(2x)}} + {\\color{cyan}{w_4 \\cos(2x)}}\n",
        "$$\n",
        "\n",
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/fourier_function002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>A random combination of functions from the Fourier basis.\n",
        "Fourier basis is made up of sine and cosine functions with different\n",
        "frequencies.</i>"
      ],
      "id": "mNjZWw3kGm6_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PtJxv36Gm7A"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "1PtJxv36Gm7A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPrBvI50Gm7A"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "HPrBvI50Gm7A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zy01oz-Gm7A"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('fourier_function{func_num:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            func_num=IntSlider(0,0,2,1))"
      ],
      "id": "7zy01oz-Gm7A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHg7WET4Gm7B"
      },
      "source": [
        "# Fitting Basis Function Models"
      ],
      "id": "OHg7WET4Gm7B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mmu29cFGm7B"
      },
      "source": [
        "## Fitting to Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-fitting-to-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-fitting-to-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Now we are going to consider how these basis functions can be adjusted\n",
        "to fit to a particular data set. We will return to the olympic marathon\n",
        "data from last time. First we will scale the output of the data to be\n",
        "zero mean and variance 1."
      ],
      "id": "0mmu29cFGm7B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pISMHmcYGm7B"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import notutils as nu"
      ],
      "id": "pISMHmcYGm7B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oldWHij2Gm7C"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "yhat = (y - offset)/scale\n",
        "\n",
        "_ = ax.plot(x, yhat, 'r.',markersize=10)\n",
        "nu.display_prediction(basis=dict(radial=mlai.radial,\n",
        "                                            polynomial=mlai.polynomial,\n",
        "                                            tanh=mlai.hyperbolic_tangent,\n",
        "                                            fourier=mlai.fourier,\n",
        "                                            relu=mlai.relu),\n",
        "                                 data_limits=(1888, 2020),\n",
        "                                 fig=fig, ax=ax,\n",
        "                                 offset=0.,\n",
        "                                 wlim = (-4., 4.),\n",
        "                                 num_basis=4)"
      ],
      "id": "oldWHij2Gm7C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z2NTavNGm7C"
      },
      "source": [
        "### Exercise 8\n",
        "\n",
        "Use the tool provided above to try and find the best fit you can to the\n",
        "data. Explore the parameter space and give the weight values you used\n",
        "for the\n",
        "\n",
        "1.  polynomial basis\n",
        "2.  Radial basis\n",
        "3.  Fourier basis\n",
        "\n",
        "Write your answers in the code box below creating a new vector of\n",
        "parameters (in the correct order!) for each basis."
      ],
      "id": "1Z2NTavNGm7C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke70_Fr3Gm7D"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Exercise 8 here\n",
        "\n",
        "\n",
        "\n",
        "# (a) polynomial\n",
        "###### Edit these lines #####\n",
        "# w_0 =\n",
        "# w_1 =\n",
        "# w_2 =\n",
        "# w_3 =\n",
        "##############################\n",
        "# w_polynomial = np.asarray([[w_0], [w_1], [w_2], [w_3]])\n",
        "\n",
        "# (b) radial\n",
        "###### Edit these lines #####\n",
        "# w_0 =\n",
        "# w_1 =\n",
        "# w_2 =\n",
        "# w_3 =\n",
        "##############################\n",
        "# w_rbf = np.asarray([[w_0], [w_1], [w_2], [w_3]])\n",
        "\n",
        "# (c) fourier\n",
        "###### Edit these lines #####\n",
        "# w_0 =\n",
        "# w_1 =\n",
        "# w_2 =\n",
        "# w_3 =\n",
        "##############################\n",
        "# w_fourier = np.asarray([[w_0], [w_1], [w_2], [w_3]])\n",
        "\n"
      ],
      "id": "ke70_Fr3Gm7D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMfADjQ2Gm7D"
      },
      "outputs": [],
      "source": [
        "np.asarray([[1, 2, 3, 4]]).shape"
      ],
      "id": "PMfADjQ2Gm7D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rTtGKVqGm7E"
      },
      "source": [
        "## Basis Function Models\n",
        "\n",
        "$$\n",
        "  f(\\mathbf{ x}_i) = \\sum_{j=1}^mw_j \\phi_{i, j}\n",
        "  $$\n",
        "\n",
        "$$\n",
        "  f(\\mathbf{ x}_i) = \\mathbf{ w}^\\top \\boldsymbol{ \\phi}_i\n",
        "  $$"
      ],
      "id": "_rTtGKVqGm7E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaGNVDHZGm7E"
      },
      "source": [
        "## Log Likelihood for Basis Function Model\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-log-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-log-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "The likelihood of a single data point given the model parameters is\n",
        "given by $$\n",
        "  p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\left(y_i-\\mathbf{ w}^{\\top}\\boldsymbol{ \\phi}_i\\right)^{2}}{2\\sigma^2}\\right).\n",
        "  $$ and making an assumption of *conditional independence* given the\n",
        "parameters we can write\n",
        "\n",
        "to give us the likelihood for the whole data set."
      ],
      "id": "ZaGNVDHZGm7E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISXKlD3UGm7E"
      },
      "source": [
        "## Objective Function\n",
        "\n",
        "Traditionally in optimization, we choose to minmize an object function\n",
        "(or loss function, or cost function) rather than maximizing a\n",
        "likelihood. For these models we *minimize the negative log likelihood*.\n",
        "This function takes the form,\n",
        "\n",
        "$$\n",
        "E(\\mathbf{ w},\\sigma^2)= \\frac{n}{2}\\log\\sigma^2 + \\frac{\\sum_{i=1}^{n}\\left(y_i-\\mathbf{ w}^{\\top}\\boldsymbol{ \\phi}_i\\right)^{2}}{2\\sigma^2}.\n",
        "$$\n",
        "\n",
        "To minimize this objective, we first expand the brackets as follows,\n",
        "\n",
        "Now we pull out the vectors, $\\mathbf{ w}$, to highlight that what we\n",
        "have is a multivariate quadratic form in $\\mathbf{ w}$."
      ],
      "id": "ISXKlD3UGm7E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfZh-iPuGm7F"
      },
      "source": [
        "## Design Matrices\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-optimisation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-optimisation.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We like to make use of *design* matrices for our data. Design matrices\n",
        "involve placing the data points into rows of the matrix and data\n",
        "features into the columns of the matrix. By convention, we are\n",
        "referincing a vector with a bold lower case letter, and a matrix with a\n",
        "bold upper case letter. The design matrix is therefore given by $$\n",
        "  \\boldsymbol{ \\Phi}= \\begin{bmatrix} \\mathbf{1} & \\mathbf{ x}& \\mathbf{ x}^2\\end{bmatrix}\n",
        "  $$ so that $$\n",
        "  \\boldsymbol{ \\Phi}\\in \\Re^{n\\times p}.\n",
        "  $$"
      ],
      "id": "JfZh-iPuGm7F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxgEhSHvGm7F"
      },
      "source": [
        "### Multivariate Derivatives Reminder\n",
        "\n",
        "To find the minimum of the objective function, we need to make use of\n",
        "multivariate calculus. The two results we need from multivariate\n",
        "calculus have the following form.\n",
        "\n",
        "$$\\frac{\\text{d}\\mathbf{a}^{\\top}\\mathbf{ w}}{\\text{d}\\mathbf{ w}}=\\mathbf{a}$$\n",
        "and\n",
        "$$\\frac{\\text{d}\\mathbf{ w}^{\\top}\\mathbf{A}\\mathbf{ w}}{\\text{d}\\mathbf{ w}}=\\left(\\mathbf{A}+\\mathbf{A}^{\\top}\\right)\\mathbf{ w}$$\n",
        "or if $\\mathbf{A}$ is symmetric (*i.e.* $\\mathbf{A}=\\mathbf{A}^{\\top}$)\n",
        "$$\\frac{\\text{d}\\mathbf{ w}^{\\top}\\mathbf{A}\\mathbf{ w}}{\\text{d}\\mathbf{ w}}=2\\mathbf{A}\\mathbf{ w}.$$"
      ],
      "id": "yxgEhSHvGm7F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agt0FOYpGm7G"
      },
      "source": [
        "## Differentiate\n",
        "\n",
        "Differentiating with respect to the vector $\\mathbf{ w}$ we obtain\n",
        "$$\\frac{\\text{d} E\\left(\\mathbf{ w},\\sigma^2 \\right)}{\\text{d}\\mathbf{ w}}=-\\frac{1}{\\sigma^2} \\sum_{i=1}^{n}\\boldsymbol{ \\phi}_iy_i+\\frac{1}{\\sigma^2} \\left[\\sum_{i=1}^{n}\\boldsymbol{ \\phi}_i\\boldsymbol{ \\phi}_i^{\\top}\\right]\\mathbf{ w}$$\n",
        "Leading to\n",
        "$$\\mathbf{ w}^{*}=\\left[\\sum_{i=1}^{n}\\boldsymbol{ \\phi}_i\\boldsymbol{ \\phi}_i^{\\top}\\right]^{-1}\\sum_{i=1}^{n}\\boldsymbol{ \\phi}_iy_i,$$\n",
        "\n",
        "Rewriting this result in matrix notation we obtain: $$\n",
        "\\sum_{i=1}^{n}\\boldsymbol{ \\phi}_i\\boldsymbol{ \\phi}_i^\\top = \\boldsymbol{ \\Phi}^\\top \\boldsymbol{ \\Phi}$$\n",
        "$$\\sum _{i=1}^{n}\\boldsymbol{ \\phi}_iy_i = \\boldsymbol{ \\Phi}^\\top \\mathbf{ y}\n",
        "$$\n",
        "\n",
        "Setting the derivative to zero we obtain update equations for the\n",
        "parameter vector and the noise variance.\n",
        "\n",
        "$$\n",
        "  \\mathbf{ w}^{*} = \\left(\\boldsymbol{ \\Phi}^\\top \\boldsymbol{ \\Phi}\\right)^{-1} \\boldsymbol{ \\Phi}^\\top \\mathbf{ y}\n",
        "  $$\n",
        "\n",
        "$$\n",
        "  \\left.\\sigma^2\\right.^{*}=\\frac{\\sum_{i=1}^{n}\\left(y_i-\\left.\\mathbf{ w}^{*}\\right.^{\\top}\\boldsymbol{ \\phi}_i\\right)^{2}}{n}.\n",
        "  $$\n",
        "\n",
        "In practice we should avoid solving these equations through direct use\n",
        "of the inverse. Instead we solve for $\\mathbf{ w}$ in the following\n",
        "linear system.\n",
        "\n",
        "$$\n",
        "  \\left(\\boldsymbol{ \\Phi}^\\top \\boldsymbol{ \\Phi}\\right)\\mathbf{ w}= \\boldsymbol{ \\Phi}^\\top \\mathbf{ y}\n",
        "$$ Compare this system with *solve for* $\\mathbf{x}$ in $$\n",
        "\\mathbf{A}\\mathbf{x} = \\mathbf{b}.\n",
        "$$ For example see the `numpy.linalg.solve` or `torch.linalg.solve`.\n",
        "\n",
        "But the correct and more stable approach is to make use of the QR\n",
        "decomposition.\n",
        "\n",
        "Don’t forget, the multivariate solution should be computed using QR\n",
        "decomposition."
      ],
      "id": "agt0FOYpGm7G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA8zArqzGm7H"
      },
      "source": [
        "## Polynomial Fits to Olympic Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-all-polynomial.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/olympic-marathon-all-polynomial.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
      ],
      "id": "NA8zArqzGm7H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiCJ29-wGm7H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pods"
      ],
      "id": "YiCJ29-wGm7H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3foqmbfgGm7H"
      },
      "outputs": [],
      "source": [
        "basis = mlai.polynomial\n",
        "\n",
        "data = pods.datasets.olympic_marathon_men()\n",
        "\n",
        "x = data['X']\n",
        "y = data['Y']\n",
        "\n",
        "xlim = [1892, 2020]\n",
        "\n",
        "basis=mlai.Basis(mlai.polynomial, number=1, data_limits=xlim)"
      ],
      "id": "3foqmbfgGm7H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Tpv1dodGm7H"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "7Tpv1dodGm7H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-BgQwc_Gm7I"
      },
      "outputs": [],
      "source": [
        "plot.rmse_fit(x, y, param_name='number', param_range=(1, 30),\n",
        "              model=mlai.LM,\n",
        "              basis=basis,\n",
        "              xlim=xlim, objective_ylim=[0, 0.8],\n",
        "              diagrams='./ml')"
      ],
      "id": "o-BgQwc_Gm7I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EOImWiDGm7I"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import IntSlider"
      ],
      "id": "6EOImWiDGm7I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJjLrJZtGm7I"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "aJjLrJZtGm7I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy0QQ6TCGm7I"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('olympic_LM_polynomial_number{num_basis:0>3}.svg',\n",
        "                            directory='./ml',\n",
        "                            num_basis=IntSlider(1,1,29,1))"
      ],
      "id": "Cy0QQ6TCGm7I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dDeszVrGm7J"
      },
      "outputs": [],
      "source": [
        "import mlai"
      ],
      "id": "1dDeszVrGm7J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD4bCowBGm7J"
      },
      "outputs": [],
      "source": [
        "x = data['X']\n",
        "y = data['Y']\n",
        "\n",
        "xlim = [1892, 2020]\n",
        "max_basis = 30\n",
        "\n",
        "ll = np.array([np.nan]*(max_basis))\n",
        "sum_squares = np.array([np.nan]*(max_basis))\n",
        "basis=mlai.Basis(mlai.polynomial, number=1, data_limits=xlim)"
      ],
      "id": "uD4bCowBGm7J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP6dgEFaGm7J"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/olympic_LM_polynomial_number002.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Fit of a 1 degree polynomial to the olympic marathon\n",
        "data.</i>\n",
        "\n",
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/olympic_LM_polynomial_number003.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Fit of a 2 degree polynomial to the olympic marathon\n",
        "data.</i>"
      ],
      "id": "eP6dgEFaGm7J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLyW1Zu7Gm7J"
      },
      "source": [
        "## Non-linear but Linear in the Parameters\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/non-linear-but-linear-in-parameters.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/non-linear-but-linear-in-parameters.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "One rather nice aspect of our model is that whilst it is non-linear in\n",
        "the inputs, it is still linear in the parameters $\\mathbf{ w}$. This\n",
        "means that our derivations from before continue to operate to allow us\n",
        "to work with this model. In fact, although this is a non-linear\n",
        "regression it is still known as a *linear model* because it is linear in\n",
        "the parameters,\n",
        "\n",
        "$$\n",
        "f(\\mathbf{ x}) = \\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x})\n",
        "$$ where the vector $\\mathbf{ x}$ appears inside the basis functions,\n",
        "making our result, $f(\\mathbf{ x})$ non-linear in the inputs, but\n",
        "$\\mathbf{ w}$ appears outside our basis function, making our result\n",
        "*linear* in the parameters. In practice, our basis function itself may\n",
        "contain its own set of parameters, $$\n",
        "f(\\mathbf{ x}) = \\mathbf{ w}^\\top \\boldsymbol{ \\phi}(\\mathbf{ x};\n",
        "\\boldsymbol{\\theta}),\n",
        "$$ that we’ve denoted here as $\\boldsymbol{\\theta}$. If these parameters\n",
        "appear inside the basis function then our model is *non-linear* in these\n",
        "parameters."
      ],
      "id": "WLyW1Zu7Gm7J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_r3FSWQGm7J"
      },
      "source": [
        "### Exercise 9\n",
        "\n",
        "For the following prediction functions state whether the model is linear\n",
        "in the inputs, the parameters or both.\n",
        "\n",
        "1.  $f(x) = w_1x_1 + w_2$\n",
        "\n",
        "2.  $f(x) = w_1\\exp(x_1) + w_2x_2 + w_3$\n",
        "\n",
        "3.  $f(x) = \\log(x_1^{w_1}) + w_2x_2^2 + w_3$\n",
        "\n",
        "4.  $f(x) = \\exp(-\\sum_i(x_i - w_i)^2)$\n",
        "\n",
        "5.  $f(x) = \\exp(-\\mathbf{ w}^\\top \\mathbf{ x})$"
      ],
      "id": "O_r3FSWQGm7J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnQu4MBkGm7K"
      },
      "source": [
        "### Exercise 9 Answer\n",
        "\n",
        "Write your answer to Exercise 9 here"
      ],
      "id": "TnQu4MBkGm7K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVqK2B46Gm7K"
      },
      "source": [
        "## Fitting the Model Yourself\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-student-fitting-exercise.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/basis-functions-student-fitting-exercise.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "You now have everything you need to fit a non- linear (in the inputs)\n",
        "basis function model to the marathon data."
      ],
      "id": "SVqK2B46Gm7K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5jVRjDGGm7K"
      },
      "source": [
        "### Exercise 10\n",
        "\n",
        "Choose one of the basis functions you have explored above. Compute the\n",
        "design matrix on the covariates (or input data), `x`. Use the design\n",
        "matrix and the response variable `y` to solve the following linear\n",
        "system for the model parameters `w`. $$\n",
        "\\boldsymbol{ \\phi}^\\top\\boldsymbol{ \\phi}\\mathbf{ w}= \\boldsymbol{ \\phi}^\\top \\mathbf{ y}\n",
        "$$ Compute the corresponding error on the training data. How does it\n",
        "compare to the error you were able to achieve fitting the basis above?\n",
        "Plot the form of your prediction function from the least squares\n",
        "estimate alongside the form of you prediction function you fitted by\n",
        "hand."
      ],
      "id": "k5jVRjDGGm7K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C0gVMx_Gm7K"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Exercise 10 here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "3C0gVMx_Gm7K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npsr8fkaGm7L"
      },
      "source": [
        "## Thanks!\n",
        "\n",
        "For more information on these subjects and more you might want to check\n",
        "the following resources.\n",
        "\n",
        "-   company: [Trent AI](https://trent.ai)\n",
        "-   book: [The Atomic\n",
        "    Human](https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248)\n",
        "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
        "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
        "-   newspaper: [Guardian Profile\n",
        "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
        "-   blog:\n",
        "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
      ],
      "id": "Npsr8fkaGm7L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOPzw8szGm7L"
      },
      "source": [
        "## Further Reading\n",
        "\n",
        "-   For fitting linear models: Section 1.1-1.2 of Rogers and\n",
        "    Girolami (2011)\n",
        "\n",
        "-   Section 1.2.5 up to equation 1.65 of Bishop (2006)\n",
        "\n",
        "-   Section 1.3 for Matrix & Vector Review of Rogers and Girolami (2011)"
      ],
      "id": "tOPzw8szGm7L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPboDh3KGm7L"
      },
      "source": [
        "::: {.cell .markdown}\n",
        "\n",
        "## References\n",
        "\n",
        "Bishop, C.M., 2006. Pattern recognition and machine learning. springer.\n",
        "\n",
        "Legendre, A.-M., 1805. Nouvelles méthodes pour la détermination des\n",
        "orbites des comètes. F. Didot.\n",
        "\n",
        "Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC\n",
        "Press."
      ],
      "id": "FPboDh3KGm7L"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  }
}